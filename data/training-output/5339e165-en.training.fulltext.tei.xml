<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<head>Table of contents<lb/></head>

			<p>Executive summary.</p>

			<head>.................................................................................................................................................... 4<lb/></head>

			<p>Background .</p>

			<head>................................................................................................................................................................ 5<lb/></head>

			<p>1.<lb/> Current challenges of agricultural R&amp;D and issues for Agricultural Research Impact Assessment (ARIA) .</p>

			<head>....... 6<lb/></head>

			<p>The main societal challenges for agricultural research .</p>

			<head>...................................................................................... 6<lb/></head>

			<p>Looking back: Some shared concerns on the future ability of ARIS to address these challenges .</p>

			<head>..................... 7<lb/></head>

			<p>The increasing complexity of the Agricultural Research and Innovation System (ARIS) .</p>

			<head>................................... 8<lb/></head>

			<p>2.<lb/> Methodologies for (ex post) Research Impact Assessment (RIA) .</p>

			<head>..................................................................... 9<lb/></head>

			<p>Standard economic approaches .</p>

			<head>...................................................................................................................... 10<lb/></head>

			<p>Approaches based on case studies .</p>

			<head>................................................................................................................. 13<lb/></head>

			<p>Towards integrated approaches? .</p>

			<head>..................................................................................................................... 17<lb/></head>

			<p>3.<lb/> Practices of ARIA in some research organisations .</p>

			<head>.......................................................................................... 18<lb/></head>

			<p>An overview of selected organisations .</p>

			<head>............................................................................................................. 18<lb/> RIA in practice: A cross-cutting analysis ........................................................................................................... 20<lb/> Conclusions ...................................................................................................................................................... 23<lb/></head>

			<p>4.<lb/> Challenges of ARIA: Promoting improved practices? .</p>

			<head>...................................................................................... 29<lb/></head>

			<p>References .</p>

			<head>.............................................................................................................................................................. 31<lb/></head>

			<p>Appendix. Practices of RIA in five international public agricultural research organisations .</p>

			<head>..................................... 35<lb/></head>

			<p>USDA-ARS (and ERS) .</p>

			<head>......................................................................................................................................... 35<lb/> INRA ..................................................................................................................................................................... 38<lb/> CSIRO .................................................................................................................................................................. 41<lb/> EMBRAPA ............................................................................................................................................................ 44<lb/> CGIAR .................................................................................................................................................................. 47<lb/></head>

			<p>Tables<lb/> <ref type="table">Table 1</ref>.<lb/> Purposes of impact evaluation .</p>

			<head>.......................................................................................................... 24<lb/> <ref type="table">Table 2</ref>.<lb/> Evaluation design, methods and methodology issues .</p>

			<head>....................................................................... 25<lb/> <ref type="table">Table 3</ref>.<lb/> Implementation .</p>

			<head>.................................................................................................................................. 27<lb/> <ref type="table">Table 4</ref>.<lb/> Utilisation of evaluation&apos;s results .</p>

			<head>....................................................................................................... 28<lb/> Executive summary<lb/></head>

			<p>Agricultural research and innovation systems are expected to address a wide range of socio-economic<lb/> and environmental issues and to contribute to global challenges, while improving the efficiency with which<lb/> public funds are used. Among other changes aimed to improve the functioning of the systems, this prompted<lb/> renewed interest for Research Impact Assessment (RIA) methods and practice in the research community and<lb/> governments.<lb/> The challenge for RIA is to take into account broader impacts, beyond those on science and economic<lb/> performance and improve knowledge on impact generating mechanisms. It is difficult to link efforts and impact<lb/> as research and innovation systems are increasingly open and complex; and are changing at a quick pace.<lb/> Furthermore, in complex systems, the impact is not additive but depends on productive interactions.<lb/> Two sets of methodologies are discussed in this report — &quot; standard economic approaches &quot; and<lb/> &quot; approaches based on case studies &quot; . Standard economic approaches aim to estimate economic benefits of<lb/> research investments in order to calculate economic indicators of the impact of research such as internal rate<lb/> of return or cost-benefit ratio. They generally focus on public research as information on private efforts is<lb/> limited, and on economic impacts such as productivity growth. Case-study is a useful method to analyse how<lb/> and why a phenomenon occurs. Case studies provide richer information, through a narrative, about the<lb/> element which is evaluated and highlights the complex relationships among the various variables, events and<lb/> actors, but it is difficult to standardise results and scale them up.<lb/> These methods are complementary and it is crucial to develop approaches that match quantitative and<lb/> qualitative analysis to reinforce the credibility of RIA. For both sets of methods, adequate databases and<lb/> metrologies need to be developed, in particular to take into account non-economic impacts.<lb/> Observation of RIA practices in five selected organisations confirms the gap between academic research<lb/> and practices. However, in some organisations interactions between research and practice is organised in a<lb/> systematic way and involve economists, agronomists and social scientists in the design of approaches. RIA is<lb/> high on the agenda of all the organisations, as building credibility is expected to secure funding. However,<lb/> limited resources devoted to RIA constrain the implementation of systematic and comprehensive evaluation in<lb/> many cases. Evaluation systems vary across organisations in scope and level, depending on programming<lb/> design.<lb/></p>

			<p>The assessment systems pursue the same three objectives:<lb/></p>

			<p> Learning: enhance the know-how to produce an environment conducive to socio-economic impact.<lb/></p>

			<p> Capacity building: spread the culture of socio-economic impact to its researchers.<lb/></p>

			<p> Reporting to stakeholders: from accountability purposes to advocacy targeted to various audiences.<lb/></p>

			<p>These objectives are difficult to pursue simultaneously. The accountability objective, including for the<lb/> purposes of return on the financial investment, poses particularly complex challenges, and there are tensions<lb/> between this objective and learning and capacity building objectives. Some experts suggest it may be<lb/> necessary to make choices between these objectives, and determine whether evaluation is for internal<lb/> purpose or external control. It is argued that multidimensional evaluation to foster internal learning would be<lb/> more efficient at improving research impact than external evaluation for control. RIA would thus nurture a<lb/> culture of impact and become a central tool for strategic intelligence.<lb/> Finally, the future of RIA depends on both the improvement of methods and information, which would be<lb/> fostered by the commitment of institutions and the constitution of a community of professionals interacting<lb/> globally.<lb/></p>

			<head>Background<lb/></head>

			<p>Research Impact Assessment (RIA) is not a new issue. Since the 1950s, the economic returns to<lb/> research investment have been analysed repeatedly. Agriculture is the single economic activity that<lb/> concentrates most analysis of economic impact of R&amp;D investment, starting with the seminal contribution of<lb/> Zvi Griliches (1958). Alston (2010) contains a comprehensive survey of these analyses. In addition, major<lb/> programmes based on case studies (such as TRACES and HINDSIGHT) have focused on analysis of the<lb/> non-academic impact of research.<lb/> RIA is receiving renewed attention in light of increased expectations about the ability of research to<lb/> deliver a wider range of socio-economic impacts. The Lisbon Agenda (2000)<lb/> 1 is one of the landmarks in this<lb/> evolution, and the organisation of research towards major challenges has extended this logic. At the same<lb/> time, many countries face budget constraints which reinforce the need to account for the impact of public<lb/> funding. This context is promoting a revival of interest in RIA methodologies, and has been the motivation for a<lb/> number of projects such as: Assessments of the impacts of the Advanced Technology Program (ATP) (<ref type="biblio">Ruegg<lb/> and Feller, 2003</ref>), Public Value Mapping (<ref type="biblio">Bozeman, 2003</ref>), the Payback Framework (<ref type="biblio">Donovan and Hanney,<lb/> 2011</ref>), and the Social Impact Assessment Method (SIAMPI) (<ref type="biblio">Spaapen and Van Drooge, 2011</ref>).<lb/> Various institutions have designed, and are experimenting with, new ways to assess the impacts of their<lb/> research. Public sector Research Organisations (PROs) dedicated to agriculture are contributing to this rich<lb/> field of experimentation including the Consultative Group for International Agricultural Research (CGIAR)<lb/> (<ref type="biblio">Walker et al., 2008</ref>), the Brazilian corporation of agricultural research (<ref type="biblio">EMBRAPA, 2015</ref>), the Economic<lb/> Research Service (ERS) of the US Department of Agriculture (USDA) (<ref type="biblio">Heisey et al., 2010</ref>), and the<lb/> Commonwealth Scientific and Industrial Research Organization for Australian research (CSIRO) (Acil Tasman<lb/> Pty Ltd, 2010).<lb/> This report is based on an extensive survey of the literature of Agricultural Research Impact Assessment<lb/> (ARIA) encompassing peer reviewed articles, publications and institutional reviews. It focuses on the links<lb/> between academic research on RIA and actual implementation of approaches, an issue that is generally blind-<lb/>spotted. Hence, the goal is to identify the state of the art and the current challenges of ARIA in the perspective<lb/> of actual implementation.<lb/> Section 1 outlines the implications for ARIA of main societal challenges for agricultural R&amp;D and<lb/> developments in agricultural innovation systems. Section 2 looks at the two types of methodologies used for<lb/> ex post research impact assessment, and Section 3 presents practices of ARIA in selected countries and<lb/> research organisations. The final section proposes ways forward to improve research impact assessment and<lb/> accountability.<lb/></p>

			<head>1. Current challenges of agricultural R&amp;D and issues for Agricultural Research Impact<lb/> Assessment (ARIA)<lb/></head>

			<p>Current challenges of agricultural R&amp;D have some crucial implications for Agricultural Research Impact<lb/> Assessment (ARIA). To sum up, the new agenda for ARIA must address the three following issues:<lb/>  take into account the diversity of dimensions of impacts of research;<lb/>  contribute to the credibility as well as to the improvement of the capacity of Agricultural Research and<lb/> Innovation System (ARIS) to produce impact; and<lb/>  adapt to the growing complexity of ARIS.<lb/></p>

			<head>The main societal challenges for agricultural research<lb/></head>

			<p>As many inter-governmental and governmental organisations have pointed out, global agriculture will<lb/> face multiple challenges over the coming decades. It must produce more food to feed an increasingly affluent<lb/> and growing world population that will demand a more diverse diet. By 2050, world population is expected to<lb/> swell to 9 billion people. The United Nations&apos; Food and Agriculture Organization predicts that in that time<lb/> global food production will need to increase by 70% in order to prevent massive famine.<lb/> Global agriculture must also contribute to overall development and poverty alleviation in many<lb/> developing countries, confront increased competition for alternative uses of finite land and water resources,<lb/> adapt to climate change, and contribute to preserving biodiversity and restoring fragile ecosystems<lb/> (<ref type="biblio">Interagency Report, 2012</ref>).<lb/> Improving agricultural productivity, while conserving and enhancing natural resources, is an essential<lb/> requirement for farmers to increase global food supplies on a sustainable basis. While agricultural productivity<lb/> represents a worldwide goal for agriculture research, other research objectives addressing societal challenges<lb/> are becoming central in the agricultural research agenda. These include:<lb/></p>

			<p> Dealing with environmental issues:<lb/>  address the shortage of natural resources, from fossil fuels to water to phosphorus,<lb/>  contribute to biodiversity conservation,<lb/>  reduce greenhouse gas emissions and contribute to carbon sequestration,<lb/>  enhance ecosystem services,<lb/>  reduce soil erosion,<lb/>  reduce dependency on pesticides.<lb/></p>

			<p> Improving health: safety and healthy food provision, safety working conditions.<lb/></p>

			<p> Enhancing the social value of agriculture: poverty alleviation, maintenance of viable rural areas, and<lb/> quality of life in rural areas.<lb/></p>

			<p> Reducing food waste from spoilage to produce culled by retailers.<lb/></p>

			<p>These challenges are very high on the agenda of OECD and key partner countries, as ministers<lb/> indicated when they met on 7-8 April 2016 at the OECD in Paris.<lb/> 2 Research and innovation are considered as<lb/> the main solutions to address these challenges. For instance, the US White House Council of Advisors on<lb/> Science and Technology identified seven challenges: i) managing new pests, pathogens, and invasive plants;<lb/> ii) increasing the efficiency of water use; iii) reducing the environmental footprint of agriculture; iv) growing<lb/> food in a changing climate; v) managing the production of bioenergy; vi) producing safe and nutritious food;<lb/> vii) assisting with global food security and maintaining abundant yields (PCAST, 2012).<lb/> 2.<lb/> See Ministerial statements at: www.oecd.org/agriculture/ministerial/statements/.<lb/></p>

			<p>In order to meet these expectations, agricultural research needs sustained public and private funding but<lb/> also a focus on challenges, an impact orientation and an improved responsiveness. Scholars acknowledge<lb/> that addressing societal challenges requires innovation in research and innovation policies (Foray, <ref type="biblio">Mowery<lb/> and Nelson, 2012</ref>).<lb/> In this context, Research Impacts Assessment (RIA) must take into account a diversity of dimensions<lb/> beyond productivity gains: environmental impacts (natural resources, biodiversity, climate change, soil<lb/> conservation, and pollution); social impacts (viable rural areas, increased revenue of smallholders); and<lb/> impacts on food safety and occupational health. As for other sectors, there is a strong need of RIA<lb/> methodologies that take into account broader impacts (<ref type="biblio">Bozeman and Sarewitz, 2011</ref>).<lb/></p>

			<head>Looking back: Some shared concerns on the future ability of ARIS to address these challenges<lb/></head>

			<p>It is widely acknowledged that the achievements in agricultural innovation over the past century have<lb/> been impressive, supporting large increases in agricultural yields. Most of the increase in global agricultural<lb/> production over the past 50 years has come from raising crop and livestock yields rather than through area<lb/> expansion. This growth in productivity is attributed largely to investments in research and innovation (<ref type="biblio">Wright<lb/> and Shih, 2011</ref>). As a result, the economic impact of agricultural research since World War II (WW II) is very<lb/> high (<ref type="biblio">Alston, 2010</ref>).<lb/> The capacity of the ARIS to support transformations capable of addressing current global challenges is<lb/> hotly debated. The World Bank Development Report 2008 identified a halving of the growth rate in grain yields<lb/> in developing countries between 1970-1989 and 1990-2005 (<ref type="biblio">Burch et al., 2007</ref>). More recent studies confirm<lb/> these trends in yields, but bring more nuances between crops (<ref type="biblio">Grassini et al., 2013; Ray et al., 2012</ref>). Some<lb/> recent estimates suggest that total factor productivity (TFP), the most comprehensive measure of productivity<lb/> reflecting the efficiency to turn all inputs into outputs, grew at an average rate of around 2% per year since<lb/> 2000 across major world regions (<ref type="biblio">Fuglie, 2010</ref>).<lb/> Other studies, in particular those using partial factor productivity indicators such as land and labour<lb/> productivity, give a more pessimistic global picture, in particular when the People&apos;s Republic of China&apos;s<lb/> (hereafter &quot;China&quot;) performance is taken out of the calculation of the world average (<ref type="biblio">Alston, 2010</ref>). The most<lb/> popular indicator of land productivity is crop yield. The average global rates of growth in yield of most of the<lb/> major cereals are declining. Since the 1980s, growth in wheat and rice yields fell from 2.5-3% to around 1%.<lb/> Maize yields showed growth of slightly less than 2% over the last decade. While there is no evidence of a<lb/> productivity slowdown, there is a clear decline in global cereal yield from close to 2.84% per year in the 1960s,<lb/> to 2.31% between 1970 and 1989 and 1.35% between 1990 and 2007 (Fuglie, 2010: 85).<lb/> In addition to concerns about the recent contribution of ARIS, three elements further obscure the horizon.<lb/> First, climate change may severely affect crop yields. Some studies estimate that global warming could result<lb/> in a 6% reduction in global agricultural production by 2080 (<ref type="biblio">Wilson, 2012</ref>). According to the same estimates, a<lb/> 3°C to 4°C rise in temperatures would result in yield losses of 18% for wheat in northern Africa and 22% for<lb/> maize in southern Africa. Parts of Africa and India are projected to suffer a 30% decline in food production<lb/> under climate change.<lb/> Second, it is very likely that a growing part of the R&amp;D effort is devoted to maintenance research. As<lb/> discussed in Section 2, maintaining high yielding production based on limited biological and natural resources<lb/> requires recurrent investments (related to pest management, soil conservation, etc.). This reduces the part of<lb/> R&amp;D devoted to the challenges identified.<lb/></p>

			<p>Third, unlike the experience of the Green Revolution which relied on the wide diffusion of genetically<lb/> uniform high-yield varieties complemented by high levels of inputs, increasing agricultural productivity in<lb/> today&apos;s context will require gains among a large number of smallholders in very different agro-ecological<lb/> regions. Note that this objective does not substitute productivity increase in large farms and that the balance<lb/> between both objectives is discussed. The need to adapt to a wide diversity of situations may reduce the<lb/> economies of scale in research, thus affecting the impact of investments.<lb/> Against this background, global institutions consider that ARIS will have to be more impact oriented. This<lb/> may lead to developing outcome-based approaches, integrating research into development processes, and<lb/> identifying key interventions required to remove blockages and barriers to large-scale impact. However, such<lb/> an orientation may contribute to narrow the horizon of research (<ref type="biblio">Mowery et al., 2010</ref>). The underlying theory<lb/> of impacts should be examined more closely.<lb/> One of the key implications of these new challenges and concerns for agricultural research is that<lb/> methodologies of impact assessment should improve knowledge on impact generating mechanisms. It is not<lb/> sufficient to measure the efficiency of R&amp;D investment; it is also necessary for impact assessment to<lb/> contribute to the efficiency. Research impact assessment should improve research management at different<lb/> levels, from research projects or programmes to the governance of the global system.<lb/></p>

			<head>The increasing complexity of the Agricultural Research and Innovation System (ARIS)<lb/></head>

			<p>Concerns on the future of global agriculture and food create a new momentum for agricultural research.<lb/> The Global Conference on Agricultural Research for Development (GCARD), organised by the Global Forum<lb/> on Agricultural Research (GFAR), has developed a roadmap for agricultural research.<lb/> 3 This roadmap calls for<lb/> an increasing investment in research, strengthening of relations between agricultural research (mainly related<lb/> to developed countries) and agricultural research for development (AR4D, mainly related to developing<lb/> countries), and the development of public and private partnerships (<ref type="biblio">Beintema et al., 2012; CGARD, 2011</ref>). A<lb/> transformation of AR4D systems is needed to: i) focus collective research and knowledge sharing on key<lb/> outcome-focused themes globally; and ii) transform and strengthen agricultural innovation systems in<lb/> developing countries.<lb/></p>

			<p>As part of this new momentum, a worldwide system dedicated to Agricultural Science and Technology<lb/> Indicators (ASTI) has been set up. It reports that in 2008 global investment in agricultural R&amp;D amounted to<lb/> USD 41 billion, of which 79% public funding and 21% private, 51% from high income countries, 49% low and<lb/> middle income countries (<ref type="biblio">Beintema et al., 2012</ref>). From 2000 to 2008, R&amp;D investment increased steadily<lb/> (22% public, 26% private, inflation-adjusted). However, these changes were unevenly distributed. Most of this<lb/> growth was driven by developing countries, since growth in high-income countries stalled. Spending growth in<lb/> developing countries was largely driven by positive trends in a number of larger, more advanced middle-<lb/>income countries — such as China +38%, and India +11% — masking negative trends in numerous smaller,<lb/> poorer, and more technologically challenged countries (<ref type="biblio">Beintema et al., 2012</ref>).<lb/></p>

			<p>The problem of data quality should be considered. In many countries, statistical sources for R&amp;D<lb/> investment are very poor for public funds and in general do not exist for private research. Coverage of<lb/> agriculture in the OECD research expenditure database is also unequal (OECD, 2013).<lb/> 4 Therefore, data on<lb/> global agricultural research are estimates and they may be very different according to the sources.<lb/> 5 The poor<lb/> quality of private R&amp;D sources raises major difficulties when measuring the returns to R&amp;D (<ref type="biblio">Fuglie et al.,<lb/> 2012</ref>).<lb/></p>

			<head>6<lb/></head>

			<p>Globally the ARIS are in constant transformation. Recent analyses of European and OECD countries<lb/> have pointed out the complexity and – especially for some countries -the fragmentation of the ARIS (<ref type="biblio">EU<lb/> SCAR, 2015; OECD, 2013; Moreddu and Poppe, 2013</ref>). In most countries, there is a relative increase of<lb/> private R&amp;D which results from the strengthening of property rights and a stagnation (or even decrease) of<lb/> public R&amp;D. Governments have encouraged the development of public private partnerships (<ref type="biblio">Moreddu, 2016</ref>).<lb/> This raises some questions on the efficiency of the research and innovation system and its ability to address<lb/> global challenges that have in many cases public goods characteristics (<ref type="biblio">Fuglie et al., 2012; Wright and Shih,<lb/> 2011</ref>).<lb/> In this changing context, the assessment of research impact is increasingly difficult. Given the complexity<lb/> of ARIS, it is very difficult to achieve impact attribution to the various actors participating which may threaten<lb/> the sustained public investment in agricultural research. Alternative approaches based on contribution are<lb/> currently under consideration. They are based on contextual and procedural analysis that allow to describe the<lb/> role of the different actors of the innovation network to elicit their respective contributions. Contribution<lb/> analysis will be discussed further in Section 2.<lb/></p>

			<table>3.<lb/> For more information on GCARD, see GFAR web site at: www.gfar.net/about-gcard.<lb/> 4.<lb/> In a few cases, it is because the country does not use international standards to report the data.<lb/> 5.<lb/> For the estimates of the InSTePP R&amp;D Series (www.instepp.umn.edu/), see Pardey et al. (2014).<lb/> 6.<lb/> The European project IMPRESA (The Impact of Research on EU Agriculture) has achieved an inventory of the<lb/> statistical sources available in 19 EU countries and Switzerland. It concludes that official sources on agricultural R&amp;D<lb/> are generally poor, which constrains the development of effective evidence based policies by European governments<lb/> (Research Brief, August 2015 available on: www.impresa-project.eu/).<lb/> 2. Methodologies for (ex post) Research Impact Assessment (RIA)<lb/></table>

			<p>The literature points out key elements that identify the current strategic importance of RIA. As suggested<lb/> by <ref type="biblio">Rip (2003)</ref> the era of massive public investment in science based on a general expectation of positive<lb/> outcomes has passed. Political systems, funding agencies and public research organisations (PROs) must<lb/> demonstrate results from the public funds used. According to Rip, RIA serves not only to give an ex post<lb/> delayed account of the impact of research; it needs to be more strategic and anticipatory, to assist systematic<lb/> improvements, and to better identify the full range of outcomes from R&amp;D investment. This does not mean that<lb/> ex post RIA is out of scope. Past experience is the main source of knowledge. This means that RIA should be<lb/> designed and performed in such a way that it helps to improve impact generating mechanisms. Hence, RIA<lb/> approaches must foster their credibility and, at the same time, adapt to different purposes and address<lb/> different audiences. The objectives are not only related to accountability but also to advocacy and learning. A<lb/> major challenge is therefore to better link evaluation approaches and evaluation strategies with learning and<lb/> continuous improvement, as well as evaluation&apos;s more conventional role of justification (<ref type="biblio">Shapira and<lb/> Kuhlmann, 2003</ref>).<lb/> A wide variety of methods for RIA are available. Based on the assessment of the</p>

			<table>US Advanced<lb/> Technology Program — probably the most important RIA initiative ever — Ruegg and Feller (2003:17) present<lb/> a set of different methods:<lb/>  Analytical/conceptual methods for modelling and informing underlying programme theory<lb/>  Survey method<lb/>  Case study: Descriptive<lb/>  Case study: Economic estimation<lb/>  Sociometric and social network analysis<lb/>  Bibliometrics: counting, citing, and analysing content of documents<lb/>  Historical tracing<lb/>  Expert judgment.<lb/></table>

			<p>The list presented by <ref type="biblio">Ruegg and Jordan (2007)</ref> in their overview of evaluation methods for R&amp;D<lb/> programmes is even more comprehensive and includes: econometrics, mission/impact mapping, foresighting,<lb/> etc. Based on a survey of US public agencies, most of the organisations involved in R&amp;D perform some kind<lb/> of &apos;case studies&apos;.<lb/> A comprehensive presentation of the different methods is beyond the scope of this report. Other surveys<lb/> are available and it would be burdensome to add a new serial presentation. Furthermore, the<lb/> comprehensiveness would not help answer the questions of interest of this report: What are the gaps between<lb/> theory and practice? How to match qualitative and quantitative approaches? How to learn from ex post studies<lb/> to improve R&amp;D management? Neither would it add value to the points raised in Section 1: How to take into<lb/> account the different dimensions of impact? How to improve knowledge on impact generating mechanisms?<lb/> How to overcome the attribution problem?<lb/></p>

			<p>Therefore, for sake of efficiency, this review focuses on two sets of methods. The first set is presented<lb/> under the heading &quot;Standard economic approaches&quot; and follows <ref type="biblio">Heisey et al. (2010)</ref> who use the same label<lb/> to present econometric approaches and economic surplus techniques. These methods have been extensively<lb/> applied in agriculture where they were first used in the late 1950s. Although a recent survey undertaken for the<lb/> OECD presents the main results (<ref type="biblio">Alston, 2010</ref>), it is necessary to return to the basics of these methods and to<lb/> discuss their current limitations and challenges. The second set of methods includes what is generally referred<lb/> to as &quot; case studies methods &quot; . If they are widely used in practice, they are also very heterogeneous. This<lb/> section focuses on some approaches that aim at developing comprehensive and standardised approaches in<lb/> order to strengthen the robustness of case studies. It also discusses the limitations and challenges of these<lb/> approaches and concludes by a short discussion of the potential complementarity of standard economic<lb/> approaches and approaches based on case studies.<lb/></p>

			<head>Standard economic approaches<lb/></head>

			<p>The economic impact of agricultural research has been analysed in a large body of the agricultural<lb/> economic literature. The objective is to match the economic benefits with the research investment in order to<lb/> calculate economic indicators of the impact of research such as internal rate of return or cost-benefit ratio. The<lb/> original contribution by <ref type="biblio">Griliches (1958)</ref> matched research benefits and investment at the level for a given<lb/> innovation (hybrid corn). Such microeconomic analyses do not take spillovers correctly into account and most<lb/> of the literature matches benefits and investments at a national or state level. Particular attention is paid to the<lb/> (temporal) lag between the investments and the related benefits. This research does not address the<lb/> mechanism through which research investment leads to some economic impact. Moreover the analysis<lb/> focuses only on the economic impact of research and pays less attention to the other dimension of this impact.<lb/> Hence these standard economic approaches are complementary to the case studies analysis presented in the<lb/> next sub-section.<lb/> This sub-section describes the methodological framework used in standard economic approaches. It<lb/> presents and discusses the main synthetic results from the literature based on several surveys and meta-<lb/>analysis that have been published.<lb/></p>

			<head>The methodological framework<lb/></head>

			<p>Internal rates of return (or other economic indicators) are calculated from the comparison of research<lb/> investment with economic impact. In the direct cost-benefit analyses, the calculation is made directly,<lb/> assuming that all the observed economic impact is explained by the research investment considered. The<lb/> alternative approach is to use an econometric approach in order to control for other factors that may explain<lb/> the observed impact.<lb/></p>

			<head>Direct calculation of the internal rate of return<lb/></head>

			<p>Direct calculation is often made at micro-economic level, considering a single innovation. For example,<lb/> <ref type="biblio">Griliches (1958)</ref> examines the case of hybrid corn in the United States. The simplest way to compile the<lb/> economic benefit is to estimate the economic gain for each user that adopts the innovation and then<lb/> aggregate this gain over the set of adopters for the whole period where the innovation is used. In the hybrid<lb/> corn case — as well as for many innovations at the farm level — the economic gain of each user is the<lb/> product of the yield gain by the economic value of the production. This simple calculation of the economic gain<lb/> is however imperfect because it ignores the impact of the adoption of the innovation on the market equilibrium<lb/> within the agricultural sector. For example, an increase in yield leads to higher production, lower prices, and<lb/> possible substitution between crops. More elaborated compilation of the economic gain can be developed,<lb/> using partial equilibrium approaches to better represent the innovation impact on the market equilibrium and<lb/> the distribution of the economic benefit from the innovator to the final consumer (with farmers as intermediates<lb/> and possibly other intermediate actors).<lb/></p>

			<head>7<lb/></head>

			<p>Based on the economic gain and research investments related to the innovation, the net present value of<lb/> the research investment can be calculated as follows:<lb/></p>

			<table>í µí±í µí±í µí± = ∑<lb/> í µí°µí µí±í µí±í µí±í µí± í µí±¡ − í µí°¼í µí±í µí±£í µí±¡ í µí±¡<lb/> (1 + í µí±) í µí±¡<lb/> í µí±<lb/> í µí±¡=0<lb/></table>

			<p>The time horizon of T years covers the research period where the innovation is developed and the<lb/> diffusion period. í µí°µí µí±í µí±í µí±í µí± í µí±¡ is the annual benefit and í µí°¼í µí±í µí±£í µí±¡ í µí±¡ is the annual research investment. It is expected that<lb/> research investment occurs at the beginning of the period and benefits occur after some delay until the end of<lb/> the period. The internal rate of return is the value discount rate r such that the net present value is equal to<lb/> zero. Other economic indicators such as the recovery period or cost-benefit ratio can also be calculated on the<lb/> basis of streams of research investment and economic benefits.<lb/> This analysis at the level of an innovation can either be used ex post, ex ante or during the diffusion of<lb/> the innovation. Ex ante analysis may be used to compare and select projects.<lb/> 8 In practice, this is rarely the<lb/> case because of the lack of data and the high uncertainty of research (</p>

			<table>Section 3).<lb/> 7.<lb/> See Alston et al. (1995) for a detailed presentation of the impact of various types of innovations in a partial equilibrium<lb/> framework.<lb/> 8.<lb/> See Mutangadura and Norton (1999) for one example of application of such ex ante analysis.<lb/></table>

			<p>One difficulty and limit of this approach at the micro-level is to define the set of research that leads to the<lb/> innovation. Indeed research activities are not necessarily targeted towards a given innovation: they may be<lb/> rather basic and/or have an impact on multiple innovations. On the one hand, if only the research that has<lb/> been targeted to a given innovation is examined, then the impact of this research is over-estimated because<lb/> the innovation would probably not have been possible without more basic research. The literature uses the<lb/> concept of project fallacy to refer to this problem (<ref type="biblio">Georghiou et al., 2002</ref>).<lb/> On the other hand, if all the research activities that have been contributing directly or indirectly are<lb/> examined, then the impact of these activities is probably under-estimated because they have also contributed<lb/> to other innovations. Hence there is a methodological limit to these micro-level calculations because the<lb/> attribution of economic benefit to research investment is arbitrary. This attribution problem is related to the fact<lb/> that research leads to the production of knowledge but this knowledge impacts multiple innovations and<lb/> spillovers exist between the different sources of knowledge production.<lb/> This limit can partially be overcome by moving from a micro to a macro-level of analysis. Both economic<lb/> benefit and research investment are measured at the aggregate level such as Nations or States. Economic<lb/> benefit is measured by the economic surplus or productivity gain for the whole agricultural sector. Hence, the<lb/> benefits capture the impact of all the innovation applied in the country. All the agricultural research<lb/> investments in the country are assessed so that &quot;internal&quot; spillovers between research activities conducted are<lb/> taken into account. However, due to difficulties collecting data, most of the analyses only evaluate public<lb/> research investment and ignore private research investment. <ref type="biblio">Andersen (2015)</ref> made one of the most recent<lb/> direct calculations of rate of return (and other indicators) for all the US States. The precise approach used in<lb/> this article is quite similar to the econometric approach presented below<lb/> In conclusion, direct calculation of research impact can be made both at the micro or macro level. Micro<lb/> level analysis provides an interesting indication on the magnitude of the impact of various innovations.<lb/> However there are some limits to these compilations depending on the way the economic benefit is calculated<lb/> and also on the attribution of the benefit to some sets of research. Attribution problems are partially solved<lb/> with macro-level analysis. However, by design, such an approach can only provide an indication of<lb/> aggregated impact over large sets of innovations. One important limitation of such direct calculation (either<lb/> micro or macro) is that the methodology does not allow controlling for other factors that may explain the<lb/> economic impact.<lb/></p>

			<head>Estimations based on econometric approaches<lb/></head>

			<p>Econometric analysis has mainly been applied to analyse the impact of research at an aggregate level<lb/> (e.g. country level). One interest of econometric analysis is to introduce different factors along with past<lb/> research. One strategy is to estimate a production function that represents the agricultural sector. The annual<lb/> production level is explained by different factors such as capital, labour, weather conditions and past research.<lb/> Making a time series estimate of such a production function is problematic because of co-linearity among the<lb/> explanatory variables. For this reason, agricultural economists generally estimate total factor productivity by<lb/> weather conditions and past research.<lb/> This estimation requires a proxy that captures past research. The standard way is to define a knowledge<lb/> stock variable (í µí°¾í µí± í µí±¡ ) that is the weighted sum of the past research investments:<lb/></p>

			<figure>í µí°¾í µí± í µí±¡ = ∑ í µí± í µí±<lb/> í µí±<lb/> í µí±¡=0<lb/> • í µí°¼í µí±í µí±£í µí±¡ í µí±¡−í µí±<lb/></figure>

			<p>The knowledge stock at period t is the variable that explains the innovation level and hence productivity<lb/> at period t. í µí± í µí± is the lag between the research investment at some period and the level of the innovation k<lb/> years after. Because of data and calculation limitations, it is not possible to estimate í µí± í µí± . directly, hence the<lb/> need to test different assumptions about the lag structure in order to retain the most suitable one. This lag<lb/> structure is expected to have an inverse U shape. There is a minimum gestation period between when the<lb/> research investment is made and when it leads to innovation. However, knowledge becomes obsolete after<lb/> some time so that the research investment has no impact. Several forms of lag structure have been tested,<lb/> and the most recent analyses retain a Gamma function<lb/> 9 over 50 years (T=50). Two parameters are necessary<lb/> to define this Gamma function and the most suitable forms are generally those with a peak at 25 years.<lb/></p>

			<figure>9.<lb/> See Alston et al. (2009, chapters 8 and 9) for a recent and detailed discussion of research lag and the assumption on<lb/> the functional forms of the lag structure.<lb/></figure>

			<p>To take spillovers into account, one generally distinguishes the local knowledge stock and the external<lb/> knowledge stock. The local knowledge stock results from the research investment made in the zone (Nation or<lb/> State) being studied. The external knowledge stock results from the research investment made elsewhere and<lb/> that might impact productivity in the zone studied. In the analysis of the US case, Alston (<ref type="biblio">Alston, 2010; Alston<lb/> et al., 2011</ref>) considers State level research investment and the external stock of knowledge as the weighted<lb/> sum of the investments made by the other States. The weights reflect the similarities of the agricultural<lb/> production systems between the different States. In the analysis of the Australian case, <ref type="biblio">Sheng et al. (2011)<lb/></ref> define the external stock of knowledge as US investment in agricultural research.<lb/></p>

			<p>The estimated model explains productivity for a given year by the local stock of knowledge, the external<lb/> stock of knowledge, some proxies that reflect the weather in the year, and possibly other control variables.<lb/> The key parameter that captures the impact of research investment is the elasticity of the local stock of<lb/> knowledge. This elasticity can then be translated in various economic indicators such as Internal Rate of<lb/> Return or Cost-Benefit ratio. These economic indicators may differ from those estimated by direct calculation<lb/> for two reasons: (i) econometric methods enable control for other factors, such as spillovers, that may explain<lb/> productivity gains; (ii) econometric analysis estimates the marginal impact of the research investment while<lb/> direct calculation estimates the average impact (<ref type="biblio">Andersen, 2015</ref>).<lb/></p>

			<head>Results and discussions<lb/></head>

			<p>This section mainly focuses on results from econometric analysis at aggregate level. Many articles have<lb/> been published covering various countries and periods as well as reviews of this literature (<ref type="biblio">Alston et al., 2000;<lb/> Alston, 2010; Evenson, 2001</ref>). The main lesson from these surveys is that the rate of return to agricultural<lb/> research is high: <ref type="biblio">Alston et al. (2000)</ref> synthesised more than 1 000 estimates and find the median Internal Rate<lb/> of Return (IRR) is above 42%. The estimates are quite sensitive to the assumption on the lag structure. The<lb/> most recent estimates have generally considered longer lags and this leads to lower values of the IRR. Note<lb/> also that most of the estimates are made for northern and southern America as well as some Asian countries,<lb/> but very few estimates are available for Europe, which may be due to limited data availability.<lb/></p>

			<head>10<lb/></head>

			<p>Some of the recent literature challenges the relevance of using IRR as an indicator of the economic<lb/> impact of agricultural research. Several authors suggest using a Modified Internal Rate of Return (MIRR)<lb/> where it is assumed that only part of the economic benefit generated by the innovation is re-invested in<lb/> research. <ref type="biblio">Hurley et al. (2014)</ref> re-examined more than 2 000 estimates with a median IRR of 39% and<lb/> calculated a median MIRR of 9.8%. This remains a high rate of return which the authors claim is more<lb/> realistic.<lb/></p>

			<p>Another important factor that has been discussed in the literature concerns the type of research. Several<lb/> analyses distinguish public research and extension. It is shown that the impact of extension is generally lower<lb/> than the impact of research and that spillovers are smaller for extension. The objective of the research should<lb/> also be taken into account. In the United States, some agricultural economists make the distinction between<lb/> productivity enhancing research and maintenance research. The objective of maintenance research is to keep<lb/> production from decreasing. Research that targets pest resistance, invasive species, and adaptation to climate<lb/> change (etc.) can be considered as maintenance research. <ref type="biblio">Sparger et al. (2013)</ref> estimate that up to 40% of<lb/> agricultural research is devoted to maintenance. Ignoring the distinction between these different types of<lb/> research may lead to very different estimates of IRR or MIRR.<lb/></p>

			<p>Another important issue is related to private research, especially in industrialised countries such as<lb/> Northern America or Europe. Private research investment in these countries is higher than public research<lb/> investment. However collecting data on private research investment is very difficult. As a result, private<lb/> research is often omitted from the analysis, even for countries like the United States where the data sets are<lb/> the more comprehensive (<ref type="biblio">Fuglie et al., 2012; Fuglie and Toole, 2014</ref>).<lb/> In the literature there is little focus on the evolution of the nature and performance of agricultural<lb/> research over time. Studies generally consider very long periods of about 50 to 60 years. This assumes that<lb/> the lag structures and the elasticity of productivity with respect to the stock of knowledge are stable over time.<lb/> However, it is clear from the observation of public research institutes, such as Land Grand Universities, the<lb/> Agricultural Research Service (ARS) of the USDA, the French National Institute for Agricultural Research<lb/> (INRA), and Wageningen University and Research (WUR) in the Netherlands that their position in the national<lb/> research systems as well as their mission have been evolving over time. Since WW II, these public research<lb/></p>

			<figure>10.<lb/> As part of the European project IMPRESA (The Impact of Research on EU Agriculture), different methods have been<lb/> used to estimate econometrically the impact of research on agricultural productivity in European member states.<lb/> Preliminary results were presented in Rome in November 2016 and final results will be made available at www.impresa-<lb/>project.eu.<lb/></figure>

			<p>organisations have shifted from applied to more basic research and this evolution is related to the emergence<lb/> of private research. Also increasing priority given to the environment and nutrition leads these organisations to<lb/> devote more investment to these objectives, which correspond to the needs of society but do not necessarily<lb/> create economic impact. Such changes challenge the standard economic approaches to evaluating the impact<lb/> of agricultural research. One way to capture these broader objectives is to calculate the economic impact only<lb/> on the share of research that is devoted to increasing agricultural productivity. Another and more ambitious<lb/> strategy is to extend the analysis to the non-economic objectives, which requires at least having synthetic and<lb/> robust indicators.<lb/></p>

			<head>Approaches based on case studies<lb/></head>

			<p>The use of case studies in impact analysis has been popular since the 1960s and 1970s when some<lb/> government agencies wanted to understand the relation between R&amp;D spending and economic growth<lb/> (<ref type="biblio">Bozeman and Kingsley, 1997</ref>). Case-studies are useful to analyse how and why a phenomenon occurs. They<lb/> provide rich information, through a narrative, about the element which is evaluated (a programme, a public<lb/> research organisation, a series of projects, or an innovation) and highlight the critical relationships that exist<lb/> among the various variables, events, and actors. Case-study can apprehend the complexities inherent to the<lb/> processes of impact generation. Case studies are also widely used to explore topics for which no strong<lb/> theory exists. For Ruegg and Jordan (2007), another strength of case studies is their ability to put flesh on the<lb/> bones of quantitative approaches, i.e. to provide elements of contexts and qualitative explanation about<lb/> quantitative trends.<lb/></p>

			<p>Case studies are subject to several limitations. Very often case studies are criticised for their lack of<lb/> rigour, the provision of equivocal evidence, ambiguous views, and biased results rather than quantitative<lb/> indications. Another problem is the ability of the methodology to assess in robust manner causality and to<lb/> generalise results from cases studies. This critic may be addressed by multiple case analysis and cross-case<lb/> comparison that would increase the external validity. Cross-case comparison can also be strengthened by<lb/> developing an analytical framework to conduct the comparative analysis. Another recurrent critic is related to<lb/> the time and resources needed to collect and analyse the data in a valid and reliable manner.<lb/></p>

			<p>The remaining part of this section presents the more recent approaches using case studies to evaluate<lb/> the societal impact of public research and outlines their main strengths and limitations.<lb/></p>

			<head>A melting pot of approaches<lb/></head>

			<p>Case studies constitute an alternative methodology of impact evaluation and encompass at least two<lb/> main characteristics that make them depart from the standard economic approaches. First, case-study<lb/> approaches include a more broadly conceived concept of impact and consider social, cultural, environmental,<lb/> political and economic returns (<ref type="biblio">Donovan, 2007, 2008, 2011</ref>). To capture the broader societal benefits,<lb/> &quot; metrics-only approaches are behind the time, and state-of-the-art evaluations of research impact combine<lb/> narrative with relevant qualitative and quantitative indicators &quot; (ibid. 176). The aim of these approaches is to<lb/> provide a full picture of all types of societal impacts. The second characteristic of a case-study approach is to<lb/> take account of the complexity involved in the process of impact generation from a wide range of academic<lb/> research. One of their main assets is to show that impacts are generated by a network of actors that interact<lb/> to create and use research results. These networks evolve over time in terms of types and number of actors<lb/> involved, objectives, and commitments. These approaches are based on theoretical frameworks such as the<lb/> system of innovation, evolutionary economics and the Triple Helix model, that provide dynamic insights to<lb/> public research impact evaluation studies. They highlight impact generating mechanisms.<lb/></p>

			<head>Various approaches evaluate the societal impact of public research<lb/></head>

			<p>Public Value Mapping (<ref type="biblio">Bozeman, 2003; Bozeman and Sarewitz, 2011</ref>) assesses the capacity of<lb/> research to achieve social goals. It is not a classical evaluation method but a rather conceptual approach<lb/> helping to understand the contribution of science within a network of knowledge producers generating societal<lb/> impact. In this approach, scientific knowledge gains value through its use and not only through its<lb/> commodification on a market. &quot;Knowledge value collectives arise to generate, develop, and use scientific<lb/> research &quot; (<ref type="biblio">Bozeman, 2003: 13</ref>). They involve government and private funding agents, end-users, wholesalers,<lb/> equipment and other scientific resource vendors. Public Value Mapping considers outcomes such as<lb/> environmental quality and environmental sustainability, health care, and provision of basic needs, e.g. housing<lb/> and food. The following factors and mechanisms can be seen as analytical lenses and determine the social<lb/> impact of research (<ref type="biblio">Bozeman and Sarewitz, 2011</ref>): characteristics of knowledge produced by research,<lb/> institutional arrangements and management affecting knowledge production and use (user-producer<lb/> interaction, networking); and policy and political domains of knowledge production and use (political and legal<lb/> context). Bozeman and Sarewitz (2011, p.1) argue that it is vital to have a deeper understanding of these<lb/> factors to help science policy-makers in &quot; making choices among competing paths to desired social outcomes &quot; .<lb/> Public Value Mapping is a model encompassing a theoretical framework, a set of assumptions and<lb/> procedures and is based on case studies. It has been applied in various domains such as climate science,<lb/> nano-medicine and chemistry.<lb/> The Payback Framework was created to assess the outcomes of health research. It consists of two<lb/> elements (<ref type="biblio">Donovan and Hanney, 2011</ref>): a logic model consisting of stages and interfaces between the<lb/> research system and the wider user environment, and various categories of research impacts. The logic<lb/> model contributes to analysing the &quot;story&quot; of an innovation from topic identification, project specification,<lb/> research process, and primary outputs of the research, to the various dissemination steps until the final<lb/> outcomes. Various types of benefits are considered: academic benefits (publications, research reports, etc.),<lb/> benefits for future research (development of research skills), benefits of policy and product development<lb/> information, health sector benefits (improved health, improved equity in service delivery), and broader<lb/> economic benefits. The dissemination and adoption phases highlight the role played by intermediaries and<lb/> beneficiaries. <ref type="biblio">Wooding et al. (2014)</ref> underline various factors associated with high and low impacts. For<lb/> instance, researchers engaging with practitioners and patients to plan and organise their research projects are<lb/> associated with projects with high scientific and broader impacts. Research which considers the pathways of<lb/> translation and application of clinical research are associated with broader impacts. The way data is compiled<lb/> facilitates comparative cross-section analyses essentially in terms of paybacks generated.<lb/></p>

			<p>The SIAMPI (Social Impact Assessment Methods for research and funding instruments through the study<lb/> of Productive Interactions between science and society) approach considers the &quot;productive interactions&quot;<lb/> between researchers and stakeholders as central to creating research with any kind of impact (<ref type="biblio">Spaapen and<lb/> Van Drooge, 2011</ref>). SIAMPI focuses on the interaction process in order to identify the relevance of the<lb/> research, and how it is adopted, diffused, and applied, or not. Productive interactions are exchanges between<lb/> researchers and stakeholders (industry, public organisations, government, and the general public) involved in<lb/> achieving societal impacts. The interaction becomes productive because stakeholders make efforts to use and<lb/> apply the research results to generate impact. In this approach, interaction between actors is central (<ref type="biblio">de Jong<lb/> et al., 2014</ref>) and depicts the main mechanisms at stake in the impact generation process. Productive<lb/> interactions might be direct (personal links between researchers and stakeholders may accelerate research<lb/> uptake), indirect via information carriers (publications, patents) or financial (research contracts, funding). The<lb/> process of interaction is complex and takes account of the evolution of the network structure, the diversity of<lb/> actors, research fields and sectors. These interactions all influence the way societal impact is generated. The<lb/> approach does not always make &quot; a clear distinction between social impact and &apos;productive interactions&apos;<lb/> because the transition from interaction to impact is often gradual &quot; (<ref type="biblio">Spaapen and Van Drooge, 2011: 212</ref>). The<lb/> case studies are compared on a cross-sectional analytical basis. The approach has been applied to Health,<lb/> Information and Communication Technology (ICT), Nanosciences and social science and humanities (Molas-<lb/><ref type="biblio">Gallart and Tang, 2011</ref>) and more recently to engineering, artificial intelligence and biomedical science (<ref type="biblio">De<lb/> Jong et al., 2014</ref>).<lb/></p>

			<head>Methods developed to evaluate the societal impacts of public agricultural research<lb/></head>

			<p>Most of the main Agricultural Public Research Organisations have developed methodologies based on<lb/> case studies to evaluate the various types of impacts generated by their research results (see Chapter 3 for a<lb/> focus on five organisations). Their results underline that the research conducted affects a wide range of<lb/> stakeholders in terms not only of economic impact but also environmental, health, and policy impacts. In terms<lb/> of quantification of broader impacts, EMBRAPA has developed an original method to evaluate environmental<lb/> and social impacts (<ref type="biblio">Rodrigues et al., 2010</ref>). Ambitec-Agro is a multi-attribute indicators system that allows<lb/> calculating impact indexes for a given innovation. Their case studies are mainly narratives devoted to justify<lb/> the various steps of the quantification procedures.<lb/></p>

			<p>Among these various experiences, the analysis aiming at assessing the societal impact of research<lb/> conducted by the CGIAR (Consultative Group on International Agricultural Research) research centres is<lb/> interesting because it introduces the notion of &quot;impact pathway&quot; (<ref type="biblio">Douthwaite et al., 2003; Walker et al., 2008</ref>).<lb/> The Impact Pathway (IP) is a model based on identification of the different phases of impact generation, the<lb/> actors involved, the flow of resources, and the progressive transformation of knowledge into outcomes and<lb/> impacts. The model was designed as an applied assessment tool by consultants in the German Development<lb/> Agency, GTZ (<ref type="biblio">Kuby, 1999</ref>), and refined for inclusion in the international agricultural research framework to<lb/> evaluate the research impact of the CGIAR. An impact pathway captures the different stages of R&amp;D from the<lb/> basic research inputs to the final impacts, including the different research outputs and outcomes for different<lb/> types of users. Networks of stakeholders play dominant roles in the construction of research outputs and in<lb/> the diffusion and adoption at multi-scale levels. Technological change is brought about by the formation and<lb/> actions of networks of stakeholders in what essentially is a social process of communication and negotiation.<lb/> The methodology encompasses a diversity of impacts (economic productivity, social and distributional,<lb/> environmental impacts). The concept of impact pathway has been used and adapted in several methods<lb/> developed to evaluate the societal impacts of public agricultural research.<lb/> For example, INRA has developed the ASIRPA approach (Socio-economic Analysis of the diversity of<lb/> Impacts of Public Agricultural Research) (<ref type="biblio">Joly et al., 2015</ref>). It is based on an analytical framework which<lb/> identifies the factors affecting the generation of impacts. The ASIRPA approach pays attention to the process<lb/> of transformation that makes knowledge actionable and which allows to incorporate it in new products, new<lb/> processes and new ways of doing things or governing. The analysis identifies the chains of translations that<lb/> occur in the process. This vision is inspired by Actor Network Theory (<ref type="biblio">Callon, 1986</ref>) that defines translation as<lb/> a four-stage process: problematisation, interessement, enrolment and mobilisation.<lb/> 11 ASIRPA is an approach<lb/> based on standardised case studies. The analysis of each case unfolds a standardised outline and sketch<lb/> three analytical tools: (i) the chronology allows identifying the main events, resources and actors, (ii) the<lb/> impact pathway (based on an adapted version of the CGIAR model) helps understanding the role of INRA at<lb/> each step of the process within the various networks of actors, and (iii) the vector of impacts is composed of a<lb/> table and a radar exhibiting qualitative and quantitative evaluation of the various dimensions of impacts. These<lb/> three tools allow reducing the inherent complexity of cases to underline the main determinants in a<lb/> standardised schematisation. Standardisation allows systematic codification of the variables of the case<lb/> studies and to perform cross-cutting analyses.<lb/></p>

			<p>In his impact evaluation guide, the CSIRO (Commonwealth Scientific and Industrial Research<lb/> Organisation) exposes its evaluation principles and process (CSIRO, 2015). The evaluators use an impact<lb/> pathway to trace the causal relationship between inputs, outputs, outcomes and impacts. The main objective<lb/> of the case studies is to expose a narrative aiming at calculating a cost-benefit ratio. CSIRO develops a more<lb/> classical approach based on the definition of a counterfactual, an attribution coefficient and an uptake profile.<lb/> Impact Pathways have also been used in participatory analysis and labelled PIPA (Participatory Impact<lb/> Pathway Analysis). The CGIAR has developed this method which supposes that project implementers,<lb/> intermediary users, end-users and political actors meet during three days in a workshop to elaborate a<lb/> common vision about how and when a project might generate various types of impacts (<ref type="biblio">Douthwaite et al.,<lb/> 2007</ref>). This ex ante step should induce the various actors involved to be highly committed in a collective way<lb/> to reach the impacts. A PIPA workshop usually produces a series of objects such as a statement of the<lb/> problem, outputs, vision, network maps, a project timeline, a logic model and an impact narrative. PIPA is also<lb/> a method used to monitor and evaluate impacts. It has been used in an ex post evaluation study by the<lb/> CIRAD (French Agricultural Research Centre for International Development) (<ref type="biblio">Triomphe et al., 2015</ref>). In this<lb/> study, researchers used focus groups and workshops bringing together various actors involved in the<lb/> evaluated project. Actors participate to the evaluation exercise at different stages of the study and with various<lb/> intensities. Using a participatory method allows to collect rich data during various interviews, workshops and<lb/> focus groups, and to develop a common vision of how the innovation process evolved. The results are also<lb/> validated collectively during a final workshop taking place at the end of the study.<lb/></p>

			<head>What lessons can be learned?<lb/></head>

			<p>The above mentioned studies lead to the following major results. The delay between the beginning of the<lb/> research and the first societal impacts are rather long and generally estimated to an average of 15 to 20 years<lb/> (<ref type="biblio">Bornmann, 2013; Joly et al., 2015</ref>).<lb/> Impacts are produced by a network of actors and this network evolves along the impact pathway. The<lb/> network in downstream phases of the pathway is often different from the research network. Impacts are thus<lb/> difficult to attribute to one isolated agent and neither is it the sum of actions deployed by each actor. Various<lb/> approaches claim that it is necessary to shift from attribution to contribution analysis. Attribution is commonly<lb/> used both to identify causal relations and to estimate quantitatively how much of an observed impact is due to<lb/> the intervention of a given organisation (<ref type="biblio">Avila et al., 2015</ref>). Attribution supposes that the different causes that<lb/> produce a given effect are additive, which contradicts what is observed in complex ecosystems of innovation,<lb/> namely the key importance of synergistic (non-additive) interactions. Therefore, attribution may usefully be<lb/> replaced by a contribution approach (<ref type="biblio">Joly et al., 2015</ref>). Detailed analysis of the roles of actors in the process<lb/> of impact generation makes it possible to identify the contributions made. Focusing on productive interactions<lb/> (<ref type="biblio">Spaapen and Van Drooge, 2011</ref>) or configuration, actor networks, the role of intermediaries, the focus can be<lb/> shifted to the contribution of specific actors, and the exchange of knowledge and expertise by the various<lb/> stakeholders.<lb/></p>

			<p>The societal impacts are determined by a set of mechanisms specific to each method and dependent on<lb/> the theoretical framework used. SIAMPI focuses on the interaction between actors, Public Value Mapping<lb/> looks at the institutional and social arrangements and settings, while ASIRPA considers synchronic and<lb/> diachronic translation mechanisms.<lb/> The distribution of impacts is highly skewed as pointed out by several authors (<ref type="biblio">Cunningham et al., 2013;<lb/> Georghiou, 1999; Maredia and Raitzer, 2006; Molas-Gallart et al., 2002; Scherer and Harhoff, 2000</ref>: 562), who<lb/> note that &quot;researchers who seek to assess the success of government technology programmes should focus<lb/> most of their effort on measuring returns from the relatively few projects with clearly superior payoffs&quot;. This<lb/> means that ex post assessment allows concentration on a limited number of cases.<lb/></p>

			<head>What are the main methodological challenges?<lb/></head>

			<p>A number of methodologies for measuring different dimensions of impact are available, but current<lb/> approaches do not provide a universal metric for each of the main dimensions of impact, or the resources for<lb/> producing it. Indeed, universal metrics are available for scientific and economic impact, and to a lesser extent<lb/> health impact (<ref type="biblio">Hanney et al., 2007; Kamenetzky, 2013</ref>). For the other dimensions (environment, public policy,<lb/> social), ad hoc measures are to be relied on.<lb/> If all the above mentioned methods recognise a variety of impacts, it is not always easy to separate one<lb/> type of impact from another. According to (<ref type="biblio">Salter and Martin, 2001</ref>) economic and non-economic impacts<lb/> might overlap. Some impacts might need more time than others to be generated. For instance, policy impacts<lb/> might be considered as an intermediate impact that could generate other types of impacts once the policy is<lb/> implemented. There is no general model about the interconnectedness of impacts.<lb/></p>

			<p>The evaluation of societal impacts lacks an accepted and standardised framework with appropriate<lb/> datasets, criteria, indicators and methods (<ref type="biblio">Bornmann, 2013</ref>). Some techniques and methods have been<lb/> developed but they still lack validity and robustness especially in terms of measurement. For <ref type="biblio">Bozeman and<lb/> Youtie (2015)</ref>, this is mainly due to the youth of this research area as compared to economic impact<lb/> evaluation methods that started more than 60 years ago and bibliometric approaches more than 40.<lb/> Standardisation is also an issue within each of the developed methods. The above mentioned<lb/> approaches possess some degree of standardisation in the sense that they often present cross-case analysis.<lb/> However, their low degree of standardisation does not allow conducting more aggregated analysis.<lb/></p>

			<head>What are the main limitations?<lb/></head>

			<p>If case-study approaches encompass advantages linked to their ability to deal with complexity and<lb/> impact generating mechanisms, they are also subject to some critics. They often lack objectivity and<lb/> quantification, and are expensive and time-consuming. Case-study approaches often provide a set of detailed<lb/> stories, each representing a specific situation within a wider set of situations where a PRO operates.<lb/> According to <ref type="biblio">Bornmann (2013: 226</ref>), &quot; Case studies do not permit generalisations to be made but they do<lb/> provide in-depth insight into processes which resulted in societal impact and therefore lead to a better<lb/> understanding of these processes (<ref type="biblio">Rymer, 2011</ref>) &quot; . The literature recognises a general problem of aggregating<lb/> the richness of case studies. Some PROs (EMBRAPA, 2015) provide aggregated data, very often in the form<lb/> of a single figure of one type of impact (e.g. economic) based on the summation of quantified impacts with a<lb/> comparable unit. Such a figure is easy to communicate. The downside is that this communicability may hide<lb/> the knowledge characterising the complexity of the various pathways to impact. Other approaches (Payback<lb/> Framework and SIAMPI) develop indicators that allow cross-case comparisons. All these approaches hardly<lb/> consider a reduced number of impact pathways at the level of an organisation, and do not generate data to<lb/> highlight these impacts. Referring to the R&amp;D Value Mapping approach, Kingsley et al. (1996) underline that<lb/> the quantification of elements across cases can lead to generalisable data.<lb/></figure>

			<p>The ASIRPA method made a first step towards aggregating the data by developing a typology of impact<lb/> pathways. The method consists in codifying the data of each standardised case study into a database. The<lb/> data are organised around the different steps of the impact pathway. Each step is detailed by a set of<lb/> variables characterising the role of actors, intermediary products, and impacts. To build the four ideal-types<lb/> two discriminating independent variables were created, representing the roles of INRA and the stakeholders<lb/> within research networks (productive configuration) and adoption networks (the diffusion process). For each<lb/> type, a cross-cutting analysis is conducted to further characterise each ideal type impact pathway (specific<lb/> translation mechanisms, critical points, research and adoption networks, research outputs, and impacts).<lb/></p>

			<head>Towards integrated approaches?<lb/> Standard economic approaches are complementary to those based on case studies<lb/></head>

			<p>Evaluating the various impacts generated by agricultural public research requires the mobilisation of<lb/> different methodologies. Evaluation should be considered as a multifaceted exercise that should provide<lb/> relevant information to various stakeholders who are involved in different types of decision processes. There is<lb/> a need to design a system of evaluation based on different approaches that can be integrated in a<lb/> comprehensive management system and political discourse. This follows the recommendation of</p>

			<table>Irwin Feller<lb/> and colleagues:<lb/> &quot; The standard for future action is not a single flawless study that satisfies all structures, but rather a<lb/> succession of studies that critique and improve upon each other as they collectively advance toward<lb/> norms of formal evaluation methodology &quot; . <ref type="biblio">(Feller, Glasmeier and Mark, 1996: 318, quoted in (Feller,<lb/> 2003: 26).<lb/></ref> The two sets of approaches presented in previous sub-sections should thus be considered as<lb/> complementary.<lb/></p>

			<p>Quantitative methods used to evaluate economic impacts are usually based on aggregated data<lb/> (Section 2.2). Such approaches quantify the economic benefits, and are useful for justifying existing public<lb/> R&amp;D programmes at the level of a Nation, a region (or State) or an industry. However, they do not add to an<lb/> understanding of the process of generating economic benefits. Approaches based on case studies are<lb/> instrumental for understanding the impact generating mechanisms, the beneficiaries and co-innovators&apos; roles,<lb/> etc. Such approaches are useful for learning purposes and the lessons drawn from these analyses can be<lb/> used to anticipate critical points and the way to overcome them in future impact pathways. But in general, they<lb/> do not produce an overall assessment of the research economic efficiency.<lb/></p>

			<p>Both types of approaches could complement each other rather than be opposed. Some assumptions of<lb/> standard economic approaches such as the adequate lag structure could be empirically founded by case-<lb/>study analysis. Concerning economic impacts, efforts could be devoted to render measurement coherent<lb/> between the two approaches to facilitate aggregation exercises. Case studies stay ahead of quantitative<lb/> approaches in terms of the variety of impacts considered but are usually rather weak in terms of<lb/> measurement. As presented in Section 3, most of the organisations integrate cost-benefit analysis in case-<lb/>study approaches. Hence, both types of approaches are complementary and it is crucial to develop<lb/> comprehensive approaches in order to provide more robust and original measures. Such approaches must<lb/> have the ability to assess research impact at various scales: project or programme, organisation or country<lb/> level.<lb/></p>

			<head>However, there are some important differences that should not be overlooked<lb/></head>

			<p>Quantitative methods used to assess the economic impact or research can hardly escape from a simple,<lb/> reduced and rather stable representation of the research processes. Moreover, they focus mainly on the<lb/> impact on productivity and social welfare and generally neglect distributional issues or possible side effects<lb/> that may lead to negative impact. They consider knowledge, resources and projects as additive with the<lb/> objective to attribute the economic specific organisations, projects or geographic region. The main objective<lb/> of these approaches is linked to accountability issues and budget allocation.<lb/></p>

			<p>On the contrary, RIA methods recently developed to assess broader impacts hypothesise that innovation<lb/> is complex, interactive and conducted in systemic contexts, that there is an evident shift from mode 1 to<lb/> mode 2 production of knowledge (<ref type="biblio">Gibbons et al., 1994</ref>), and there is a shift from a pure competitive frame to<lb/> the need to address societal requirements. In these system-oriented approaches, actors contribute to<lb/> generate societal impacts within complex and evolving productive configurations or networks. The main aim of<lb/> these approaches is to understand the mechanisms and processes generating impact, and to support policy<lb/> learning.<lb/></p>

			<p>The design of integrated approaches should not overlook these differences. Indeed, these approaches<lb/> draw on interdisciplinarity and epistemic pluralism, which should not lead to analytical complacency or<lb/> methodological amateurism, but on the contrary to reinforced robustness through the possibility of performing<lb/> assessment from different perspectives.<lb/></p>

			<head>3. Practices of ARIA in some research organisations<lb/></head>

			<p>There is an important gap between methods of RIA published in academic journals and those actually<lb/> used in practice (<ref type="biblio">Shapira and Kuhlmann, 2003</ref>). Such an assessment, however, is based on anecdotal<lb/> evidence. So far, RIA in practice has not been systematically studied. This section contains original data on<lb/> ARIA in practice in order to grasp the current situation and to see how the gap between s is being dealt with.<lb/> It focuses on the practices of five public research organisations:<lb/></p>

			<head></head>

			<figure>The US Department of Agriculture (USDA)<lb/>  The French National Institute for Agricultural Research (INRA)<lb/>  Commonwealth Scientific and Industrial Research Organization (CSIRO) in Australia<lb/>  The Brazilian agricultural research organization, EMBRAPA<lb/>  The Consultative Group on International Agricultural Research (CGIAR)<lb/></figure>

			<p>The study focuses on the impact assessment practices of these research organisations and not on the<lb/> impact of agricultural innovation systems as such (<ref type="biblio">OECD 2012, 2013</ref>). The contribution to impact of these<lb/> organisations increasingly depends on interactions with other actors such as: universities, other public<lb/> research organisations, extension services, private companies, etc. Research impact assessment approaches<lb/> allow these organisations to identify the respective contribution of the different actors.<lb/> The selection of these organisations is based on previous information available from the literature.<lb/> CSIRO and EMBRAPA were chosen because of their long-established practices in impact assessment. USDA<lb/> and CGIAR were selected because of their focus on evaluating programmes. INRA is a newcomer in research<lb/> impact assessment. These organisations present special features and interests for monitoring and impact<lb/> evaluation. Because of the limits of this study, the focus is on these organisations although other institutions<lb/> also play an important role in agricultural research at national or international level. This study is not expected<lb/> to be representative of how other institutions function. But because these organisations are very different, it<lb/> allows wide range of diversity of practices to be observed.<lb/> This section draws on two main information sources: descriptions on the organisations&apos; web pages, and<lb/> additional information from one or two sources for each of the selected organisations. The five selected<lb/> organisations are briefly introduced and their RIA experience presented. The second section is devoted to a<lb/> cross-cutting analysis of RIA in practice.<lb/></p>

			<p>Detailed descriptions of the approaches implemented in these five organisations, along with some<lb/> insights into the challenges they are facing in terms of methodologies and implementation, are available in the<lb/> Appendix.<lb/></p>

			<head>An overview of selected organisations<lb/> US Department of Agriculture (USDA)<lb/></head>

			<p>The Agricultural Research Service (ARS) is the US Department of Agriculture&apos;s chief research in-house<lb/> agency. It employs more than 2 000 scientists in more than 90 laboratories throughout the country. ARS is<lb/> therefore the main focus of this study. The Economic Research Service (ERS), which is only marginally<lb/> concerned with research impact assessment, but provides studies on the productivity of agricultural research,<lb/> will also be looked at briefly.<lb/></p>

			<p>The main tool for evaluating all federal agencies, including research agencies, is the Government<lb/> Performance and Results Act (GPRA). USDA is also a partner in the STAR METRICS consortium (<ref type="biblio">Science<lb/></ref> and Technology for America&apos;s Reinvestment: Measuring the EffecT of Research on Innovation,<lb/> Competitiveness and Science) between US federal science agencies and research institutions to document<lb/> the return on investment, research impact, and social outcomes of federally-funded R&amp;D.<lb/> ARS is established on the basis of a five-year strategic plan (presently strategic plan fiscal year (FY)<lb/> 2012–17, revised in 2014). ARS research is organised into 17 National Programs. These programmes serve<lb/> to bring co-ordination, communication, and empowerment to approximately 750 research projects. Programme<lb/> performance against targets is monitored annually. Each National Program Team (NPT) prepares an annual<lb/> report featuring the National Program major accomplishments. At the end of each national programme&apos;s five-<lb/>year cycle, an accomplishment report is prepared by the NPT, and a retrospective Review by an external<lb/> panel is carried out.<lb/> INRA<lb/> The French National Institute for Agricultural Research (INRA) employs 8 300 people (of which<lb/> 1 800 researchers and 2 500 engineers) and is organised into 13 scientific divisions. INRA&apos;s scientific priorities<lb/> are set every ten years in a strategic guidelines document (presently 2010-20). Before 2009, while science<lb/> quality was monitored and evaluated there was no system in place to evaluate societal impact. Some reports<lb/> on innovation were commissioned by senior managers.<lb/> 12 The communication unit also maintained a database<lb/> of salient facts that gathered narratives and data regarding more than a thousand innovations since 1996.<lb/> INRA&apos;s move in developing a comprehensive system for the assessment of its socio-economic impact<lb/> dates from 2009. INRA managers funded a research project to design a methodology for the Analysis of the<lb/> Socio-economic Impacts of the Public Agricultural Research (ASIRPA) that could be tested by INRA, the<lb/> project was launched in 2011. In 2013-15, after a two-year pilot phase, half of INRA&apos;s scientific divisions were<lb/> involved and tested the approach in real assessment conditions. The resulting impact assessment system was<lb/> officially institutionalised in 2015. In parallel, in 2014, INRA&apos;s economists calculated the Internal Rate of Return<lb/> of the French agricultural research, using standard economic methods described in Section 2 of this report.<lb/> CSIRO<lb/> In Australia, the Commonwealth Scientific and Industrial Research Organization (CSIRO) employs over<lb/> 5 000 people, and is funded by the Department of Industry, Innovation and Science. CSIRO is organised into<lb/> three Lines of Business (LoB)<lb/> 13 : 1) Impact Science; 2) National Facilities and Collections; and 3) CSIRO<lb/> Services. Within its Impact Science LoB, there are nine Business Units (BU, previously known as Flagships),<lb/> focusing on the largest issues facing the nation across key sectors: Agriculture, Health and Biosecurity,<lb/> Data 61, Food and Nutrition, Land and Water, Mineral Resources, Manufacturing, Energy, and Oceans and<lb/> Atmosphere.<lb/></figure>

			<p>Since the 1990s and until 2011, the CSIRO was funded by the Federal Government through a<lb/> quadrennial funding cycle. To prepare for each new funding phase, and in order to assess the performance<lb/> achieved during the previous period, reviews were conducted by external consultants (the last review of this<lb/> type was issued in 2014). These reviews drew on representative case studies to calculate the overall<lb/> organisational economic value created by CSIRO during the period, calculating a total internal rate of return.<lb/> The quadrennial funding cycle was replaced by a four-year rolling funding agreement process, which requires<lb/> CSIRO to report yearly against key performance indicators linked to their strategic and corporate plans.<lb/></p>

			<p>The impact reviewing procedure was renewed with the objective of developing a common framework for<lb/> evaluating the different flagship programmes of CSIRO. Another objective is to link ex post assessment with<lb/> monitoring of emerging impact. In 2010, it was decided to launch a consistent, organisation-wide approach to<lb/> impact assessment and management, the Impact 2020 project. This impact-based system includes<lb/> standardised case studies for ex post societal impact assessment and a monitoring database.<lb/></p>

			<head>EMBRAPA<lb/></head>

			<p>The Brazilian Agricultural Research organization (EMBRAPA) employs 9 800 people (of which<lb/> 2 400 researchers). EMBRAPA is organised into 42 product-based, basic themes or eco-regional research<lb/> centres. EMBRAPA&apos;s strategic plan sets large missions and goals over a period of 20 years. The first<lb/> assessment of the impact of EMBRAPA&apos;s technologies started in the middle of the 1980s, as part of a national<lb/> effort for impact assessment (<ref type="biblio">Avila et al., 2015</ref>). Studies calculating economic surplus or econometric<lb/> analyses were performed ex post, either at the level of commodities, grants, programmes, regions or the<lb/> EMBRAPA as a whole. During the 1990s the majority of the econometric studies were performed at lower<lb/> levels (research centre or commodity-oriented research) and based on local initiatives, training requirements<lb/> (Ph.D. or M.Sc. thesis), or to meet the demand of large international funders (Inter-American Development<lb/> Bank and World Bank).<lb/> The impact evaluation process was renewed in 1997. It led to the publication of an annual issue of an<lb/> &quot; EMBRAPA social balance &quot; report tracking the dissemination and economic impact of 110 technologies and<lb/> 220 cultivars. The report is based on case studies compiled in a yearly updated database. The aggregation of<lb/></p>

			<table>12.<lb/> See for example Les chercheurs et l&apos;innovation: regards sur les pratiques de l&apos;INRA, 1998, Paris : Quae.<lb/> 13.<lb/> www.csiro.au/en/About/Strategy-structure/Operating-model.<lb/> data allows calculating the Internal Rate of Return of EMBRAPA every year. Starting in 2000, this assessment<lb/> and annual monitoring encompassed social and environmental impacts, calculated using the EMBRAPA&apos;s<lb/> Ambitec method (Rodrigues et al., 2010).<lb/> CGIAR<lb/> The Consultative Group on International Agricultural Research (CGIAR) was created in 1971. It counts<lb/> 15 independent research centres, with a total of 8 000 staff (researchers and technicians). In 2010, the CGIAR<lb/> moved away from centre-based programming, to cross-centres CGIAR Research Programmes (CRP).<lb/></table>

			<figure>The CGIAR Consortium co-ordinates activities across research centres and is accountable for how the<lb/> donors&apos; funds are used. Since 2010 a Strategy and Results Framework (SRF) provides common goals to be<lb/> jointly achieved by CGIAR centres through 16 CGIAR Research Programmes (CRP). Before receiving<lb/> funding, CRPs set out their expected achievements and provide verifiable targets against which progress can<lb/> be measured and monitored. The second CGIAR&apos;s 2016-30 strategic and result framework (SRF) was<lb/> approved by CGIAR&apos;s Consortium Board in May 2015. At the system level CGIAR must contribute three goals<lb/> (System Level Outcomes, SLO&apos;s) of the Sustainable Development Goals (SDGs) outlined by the United<lb/> Nations.<lb/></figure>

			<p>There is a long history of evaluation in the CGIAR, with the main responsibility residing with the former<lb/> Science Council (now the Independent Science and Partnership Council-ISPC) which organised the<lb/> independent external review of CGIAR Centres. Many of these assessments were performed by university<lb/> researchers, at the request of individual funders, which resulted in highly heterogeneous methods and quality.<lb/> In parallel, independent reviews of the CGIAR as a whole were undertaken approximately every ten years.<lb/> The independent Standard Panel for Impact Assessment (SPIA), a sub-group of the CGIAR ISPC, was<lb/> created in 1995 to advise on donors&apos; fund allocation by performing ex post meta-evaluation of the economic<lb/> impact of the CGIAR&apos;s research. Historically, SPIA performed global modelling, using IFPRI models, to<lb/> estimate Internal Rates of Return and addressed impact evaluation at a centre&apos;s level, or on broader thematic<lb/> areas. Over the years, the CGIAR&apos;s research agenda expanded to address natural resource management and<lb/> conservation issues. The Independent Evaluation Arrangement (IEA), created in 2012, has a central mandate<lb/> for external evaluation of all parts of the reformed CGIAR system. The current structure for evaluations in<lb/> CGIAR lays out a system of multi-level evaluations: CGIAR Research Program (CRP) and System (portfolio of<lb/> CRPs) levels (e.g. the CGIAR as a whole).<lb/></p>

			<head>RIA in practice: A cross-cutting analysis<lb/></head>

			<p>This cross-cutting analysis presents RIA in practice in a rather linear way. It presents the purposes of<lb/> RIA, the way RIA is designed, implemented, and the way its results are actually used. This presentation does<lb/> not do justice to the complexity of the systems of evaluation. Tables 1 to 4 and the Appendix provide details<lb/> on RIA practices in the selected organisations. This cross-cutting analysis outlines the similarities and<lb/> differences and identifies styles of RIA.<lb/></p>

			<head>Purposes of impact evaluation<lb/></head>

			<p>All five public research organisations (PROs) assign multiple purposes to their impact assessment<lb/> approach: upwards accountability (to funders, or the public at large), internal organisational learning (lessons<lb/> to improve effectiveness in producing impact) and internal culture (engaging and building capacity of<lb/> researchers). Accountability objectives usually require an external evaluation or validation, which can go<lb/> against the objectives of involving internal staff and develop internal evaluative capacities, and can even<lb/> prompt researchers to report overestimated data about their impact thereby limiting internal organisational<lb/> learning. The value of the evaluation system can therefore be characterised according to the balance between<lb/> these three — sometimes divergent — interests.<lb/></p>

			<p>The assessment approach concerns multiple levels of the organisation: national or international research<lb/> programmes (CGIAR, CSIRO, USDA), scientific divisions, centres, or business units (CSIRO, CGIAR, INRA),<lb/> the organisation as a whole (CSIRO, INRA, EMBRAPA, USDA). The evaluation of single projects is generally<lb/> not the objective of the assessment approach.<lb/></p>

			<p>In terms of timing, the approaches are designed to be inserted in an external assessment calendar, even<lb/> if internal learning is claimed to be a chief motivation. All evaluation systems (with the exception of INRA) are<lb/> linked with a monitoring system. In our sample, the evaluation of programmes is linked with the end of a<lb/> funding phase (USDA, CGIAR), and in these two cases the evaluation of the research ex post impact, which<lb/> goes beyond the timeframe of a programme, is demand-driven and depends on the availability of external<lb/> funds. Only INRA, CSIRO and EMBRAPA have a planning for reporting ex post impact. Only EMBRAPA<lb/> regularly updates studies concerning the ex post impact of their research.<lb/></p>

			<head>Evaluation designs<lb/></head>

			<p>All PROs have set guidelines to standardise the way societal impact should be assessed (CSIRO being<lb/> the most recent). In recent years they have explored new ways to evaluate their impact either by implementing<lb/> research projects (INRA, USDA) or by reviewing their monitoring or evaluation systems (CGIAR, CSIRO,<lb/> EMBRAPA). Some of these developments have not been followed through (example ERS guidelines for<lb/> USDA), or are just being implemented (CSIRO). Recent monitoring systems (CGIAR, CSIRO, USDA) are<lb/> integrated into a whole theory of change, creating a link between activities, outputs and outcomes, and trying<lb/> to track progress towards societal impact at the end of each funding phase. Ex ante assessments (USDA,<lb/> CGIAR, CSIRO and soon EMBRAPA) are part of a cycle linking monitoring and evaluation. Ex post<lb/> assessment approaches are often goal-free, with no incentive to relate impact to previously set targets,<lb/> contrary to monitoring incentives which increasingly account for the progress in achieving expected impacts.<lb/> This &quot; targeted &quot; impact assessment approach faces a challenge related to time lag since the timespan imposed<lb/> for the evaluation is often too short to observe time-distant societal impact.<lb/> All the organisations assess their economic impact, and most of them account for environmental and<lb/> other broader social impact. EMBRAPA, INRA, and to a lesser extent CGIAR, developed their own<lb/> methodologies to assess other non-economic impacts (institutional, political, sanitary, territorial).<lb/> In terms of methods, some PROs (INRA, EMBRAPA, and CGIAR) combine aggregated econometric<lb/> approaches and case-study based approaches, with some attempts to complement each approach by the<lb/> other. EMBRAPA demonstrated the consistency of this dual approach by comparing the sum of economic<lb/> surplus of its hundred technology cases to the IRR calculated with aggregated data at EMBRAPA&apos;s level.<lb/> Econometrics is also performed at the case level: EMBRAPA calculates an internal rate of return by<lb/> technology, CSIRO calculates cost benefit analysis of cases of varied sizes.<lb/> All organisations (with the exception of USDA) perform case studies although in different ways: some are<lb/> relatively short illustrative narratives (EMBRAPA) or quantification of the various steps of the impact pathway<lb/> (CSIRO) surrounding the quantification of impacts or the calculation of cost-benefit analysis, while other<lb/> case studies encompass rich qualitative narrative including network analysis, changes of context and<lb/> practices, in addition to the characterisation of impact (INRA). The approach implemented identifies the<lb/> contribution of all the actors involved in the different phases of the impact pathways. PROs attempt to quantify<lb/> impacts, often through monetisation (CSIRO, CGIAR). There are some attempts to quantify the impact with<lb/> physical indicators specific to each impact dimension (INRA, EMBRAPA, CSIRO) and to try to align the desire<lb/> for metrics and the need for meaningful analysis of achievements.<lb/></p>

			<p>The quantification of impact is often assorted with a strong focus on attributing a share to the PROs<lb/> research effort (CSIRO, EMBRAPA, CGIAR). Examples gathered from this study show that the rules to decide<lb/> on the attribution shares among a network of research and diffusion partners are unclear (CSIRO, CGIAR,<lb/> USDA-ARS). Furthermore the objective to achieve a quantitative attribution rate can lead to a bias in the case<lb/> selection. To facilitate the calculation of the attribution rate, PROs are tempted to select recent cases for which<lb/> memory is fresh and data are more easily available. They may also select smaller cases in which few external<lb/> stakeholders have taken part rather than the much larger returns from the provision of international public<lb/> goods, for lack of a credible scenario of attribution to the PRO supported-research (CSIRO, CGIAR).<lb/> The tendency towards centralised monitoring and evaluation systems (CGIAR), or standardised<lb/> guidelines for evaluation (INRA, CSIRO, EMBRAPA), answers the managers&apos; and funders&apos; desire to increase<lb/> the value of local evaluations and produce information on global impact. Building an overarching analysis of<lb/> cases (aggregation, or cross-analysis), may consist in summing up the impacts yielded by a set of cases with<lb/> the view to deliver a message accounting for the intensity (EMBRAPA, CSIRO) and diversity (INRA,<lb/> EMBRAPA) of the impact of the organisation. It may also consist in analysing the impact generation steps and<lb/> mechanisms with the view to provide organisational learning (EMBRAPA, INRA). Aggregation is often<lb/> performed at the expense of detailed meaningful information. For example, the process used by EMBRAPA to<lb/> aggregate its environmental impact relies on calculating the average of environmental impact scores<lb/> (technical local performance ranging from -15 to +15) of a hundred technologies. Yet this average does not<lb/> account for the level of adoption of the technologies. Aggregation entails a standardised methodology to study<lb/> comparable cases, which is lacking in highly decentralised implementers such as CGIAR Research<lb/> Programmes. Aggregation also requires a rationale for selecting representative cases as compared to the<lb/> objective pursued. The rationale for case selection is barely detailed in the information provided by the PROs<lb/> considered.<lb/></p>

			<head>Implementation<lb/></head>

			<p>All PROs are very concerned with the external credibility of their monitoring and or evaluation system,<lb/> and the impact they report. Measures to promote external credibility include open calls for proposals to<lb/> develop methodological supports (CGIAR), call for tender for consultants to apply standard guidelines<lb/> (CGIAR, CSIRO), external validation of the case reports (CSIRO, INRA). CSIRO organises workshops with<lb/> stakeholders in order to collect data and set its attribution shares. USDA-ARS review panels made up of<lb/> outside experts (academics, stakeholders, and government) provide feedback on the programmes. Still,<lb/> participatory approaches are not reported for setting the terms of reference of the assessment.<lb/> Examples gathered from this study show a gap between the theoretical method designed by PROs to<lb/> assess their impact, on which they communicate, and the approach they actually implement. This gap may be<lb/> explained by budget constraints (this problem was mentioned for CSIRO, CGIAR), management issues<lb/> (CGIAR), or time delays to implement recent management changes (CSIRO, CGIAR, INRA).<lb/> Despite the interest demonstrated by PROs to assess their societal impact, very limited means (in terms<lb/> of staff and budget) have been invested so far to implement the approaches designed.<lb/> Budget constraints limit the number of internal staff dedicated to perform impact case studies (INRA),<lb/> and the possibility to hire consultants to do so (CSIRO), limiting in turn the number of cases and their<lb/> representativeness at the level of the PRO. Managers from CSIRO and CGIAR also reported a problem of<lb/> timing in the allocation of budget for impact evaluation: considering average impact generation lags, the<lb/> ex post impact assessment of a programme is to be performed several years after the programme has ended,<lb/> while the programme budget is discontinued.<lb/> Skills are also lacking. CGIAR research programmes lack competent staff for establishing the baseline or<lb/> counterfactual to implement routine result-based management. CSIRO claims to be struggling with finding<lb/> skilful external consultants to perform robust cost-benefit analysis. Internal as well as external capacity<lb/> building needs time, and some institutions are developing training courses on impact evaluation (CSIRO,<lb/> INRA). Management issues also include coordination efforts, a crucial challenge for CGIAR to harmonise<lb/> methods, incentives and capabilities across its programmes, ensure quality of data and consistency of<lb/> reporting, and solve present heterogeneous assessments that are detrimental to credibility.<lb/> Producing robust information on impact is costly (EMBRAPA). In terms of management, a limited<lb/> availability of evaluation staff implies a greater involvement of non-specialist researchers. The identification<lb/> and investigation of a case study relies on their willingness to take part. Resistance based on insecurity, fear<lb/> of being monitored, or unwillingness to be evaluated increases the selection bias. This in turn limits the<lb/> number of cases studied, the potency of the case-study-based approach to account for the diversity of the<lb/> missions of the PRO (CSIRO), and therefore the effectiveness of the evaluation system to teach useful<lb/> learning for internal management improvement (CGIAR).<lb/> The implementation of evaluation models is also hampered by the lack of streamlined and workable<lb/> systems for information gathering. Most PROs relate their difficulty to implement their assessment framework<lb/> because collected data are of poor relevance (CSIRO, INRA) or not locally available. For example, the<lb/> framework designed to evaluate international CGIAR programmes can conflict with the availability of the data<lb/> produced at the level of associated national research centres. Along similar lines, confidentiality can also be a<lb/> problem (INRA and CSIRO struggle for accessing food sector private data). Again data availability orients the<lb/> selection of impact studies towards &quot; doable &quot; cases, thus increasing bias.<lb/> A last reason to explain the theory-practice gap may be related to the delays required for new knowledge<lb/> to disseminate. CGIAR for instance claims to be lacking operational tools like proxies for outcomes or<lb/> methods for attributing impact shares, while they consider that robust theories on impact pathways do exist at<lb/> CGIAR. Since the implementation of a revised assessment approach was decided only a few years ago, too<lb/> few case studies have been produced so far to allow for a scaling-up and generalisation of learning at the<lb/> CGIAR system-wide level.<lb/></p>

			<head>Use of the results of impact evaluation<lb/></head>

			<p>To what extent is evaluation used to communicate on impacts, or as support for decision-making? The<lb/> findings of this study show that research impact assessment affects PROs management practices in a variety<lb/> of ways, even if paradoxically, they make only limited uses of the results yielded so far.<lb/></p>

			<p>The main use that all the PROs effectively make is demonstration to stakeholders. While INRA, USDA<lb/> and EMBRAPA seem to build accountability on the motivation and interest of a diversity of stakeholders,<lb/> CSIRO and CGIAR prioritise reporting on the good use they make of public funds. Information is lacking on<lb/> how CSIRO, CGIAR, or USDA ground their funding allocation decision on the basis of societal impact<lb/> assessment. In some instances, experts reviewing research programmes suggest that the objectives of<lb/> influencing internal budgetary choices and priorities may be poorly served by present monitoring and<lb/> evaluation systems (CGIAR). Similarly, institutional learning based on ex-post assessment seems limited,<lb/> which is not surprising since the case-study methods used in the PROs considered do not go very far in terms<lb/> of understanding of the impact generation mechanisms (except INRA). Learning objectives seem to be chiefly<lb/> achieved through monitoring approaches, and may concern low-level tactical topics, rather than strategic<lb/> higher level issues.<lb/></p>

			<head>Conclusions<lb/></head>

			<p>The fact that all five organisations of this study have recently made serious attempts to improve the way<lb/> they evaluate their societal impact shows this has become an important consideration. Credibility is important<lb/> to PROs and the five organisations have looked for methods that would combine excellence validated by their<lb/> scientific peers and effectiveness in expressing outcomes or impacts for a specific audience. PROs have<lb/> multiple ambitions for impact assessment approach: upwards accountability (to funders, or the public at large),<lb/> internal organisational learning (lessons to improve effectiveness in producing impact) and internal culture<lb/> (engaging and researchers&apos; capacity building). However there is a gap between PROs ambitions for their RIA<lb/> systems, and what they actually apply, which can only be partly attributed to a shortage of funds or staff<lb/> capacity.<lb/> Accountability to funders (Treasury or Donors) is clearly an important driver for institutionalising impact<lb/> evaluation and monitoring systems. One area where funders exert influence is their demand for quantitative<lb/> targets and metrics for outputs, outcomes and impacts (USDA, CGIAR, CSIRO, EMBRAPA). The study<lb/> suggests that in such instances (CGIAR, EMBRAPA) this objective may be served better than institutional<lb/> learning on how to produce impact or capacity-building benefits. RIA systems build organically on the PRO<lb/> organisational structure and operate in rhythm with existing processes; they will differ if funding comes through<lb/> centres or programmes. This study suggests that organisations funding programmes tend to focus on<lb/> collecting information at the various levels of change-activities-outputs-outcomes, more than joining the links<lb/> to understand the dynamics for impact generation that are important for institutional learning.<lb/> The belief that learning and accountability goals can be mutually accommodated is widespread, but this<lb/> study shows that it is not always so evident. From the interviews it appears that learning, and<lb/> capacity building, are two important motivations for PROs&apos; staff (CGIAR, CSIRO, INRA), however it is also<lb/> evident that the accountability objective is of more universal interest for funders. Rhetoric is also widespread<lb/> about the importance for an organisation of discerning what is not working. But the question arises that, for an<lb/> organisation, exposing flaws or weaknesses could come at a cost in terms of reputation or future funding<lb/> (CGIAR, USDA). This is especially the case when there is a sense that decisions in terms of funding flows<lb/> would be immediately associated with such information, which can be a common case when funds come<lb/> through programmes. In this situation immediate operational and tactical learning will be more readily<lb/> addressed. Systems that introduce more distance between evaluation and funding decisions (EMBRAPA,<lb/> INRA) leave more opportunities for strategic learning.<lb/></p>

			<p>There is a tendency to construct RIA systems around accountability to the funders&apos; viewpoint, with the<lb/> objective of institutional and staff learning coming as an afterthought. This in turn limits the usefulness of RIA,<lb/> as evaluations may be more directed towards considering how existing programmes and strategies can be<lb/> delivered than calling this strategy into question. It is important to consider other dimensions, such as<lb/> analysing the PROs contribution to impacts within the innovation network of actors, in order to inform strategic<lb/> decisions.<lb/></p>

			<table>Table 2. Evaluation design, methods and methodology issues<lb/> Organisation<lb/> Existence of guidelines<lb/> at the level<lb/> of the organisation<lb/> Timing<lb/> of<lb/> assessment<lb/> Impact<lb/> dimensions<lb/> assessed<lb/> Theoretical framework<lb/> (Goal-free evaluation vs expected<lb/> impact, qualification of pathways<lb/> and networks, attribution of<lb/> impacts)<lb/> Methodologies : econometric,<lb/> case studies, standardisation<lb/> of cases, narrative<lb/> Measures , monetisation,<lb/> indexes, counterfactual<lb/> Aggregation<lb/> CGIAR:<lb/> CGIAR system-wide<lb/> Counselling on<lb/> methodologies by SPIA<lb/> but not unified.<lb/> Ex post.<lb/> Mostly economic, a few<lb/> environment, social and<lb/> health.<lb/> Evaluation of expected impacts, as<lb/> compared to programme objectives.<lb/> Programme theory, impact pathway,<lb/> theory of changes.<lb/> Socio-Econometric (Net Present Value<lb/> and Benefit Cost Ratio).<lb/> Case study with cost-benefit ratios for<lb/> environmental impact.<lb/> Monetised, search for counterfactual.<lb/> Cost-benefit ratios + specific indicators in<lb/> development.<lb/> Calculation of IRR of large innovations in<lb/> comparison with CGIAR funding.<lb/> CRP programmes<lb/> Evaluation guidelines by<lb/> IEA. Harmonised<lb/> monitoring and reporting<lb/> framework planned for<lb/> 2017.<lb/> Monitoring, end of<lb/> funding cycle<lb/> evaluation.<lb/> CRPs: progress towards<lb/> programmes objectives<lb/> (linked to overarching<lb/> expected impacts).<lb/> Evaluation of expected impact.<lb/> Programme result-based<lb/> management, impact pathway,<lb/> theories of change, attribution.<lb/> Depending on the proposal from the<lb/> external evaluator responsible for the<lb/> CRP assessment.<lb/> Some requisites in guidelines and terms<lb/> of reference by IEA.<lb/> Programme based quantitative targets.<lb/> Physical indicators by target.<lb/> Sum of the quantitative contribution of each<lb/> CRP to the System Level is to estimate CGIAR<lb/> Outcomes.<lb/> USDA-ARS<lb/> No public guidelines.<lb/> Some harmonisation in<lb/> the format of NPs<lb/> accomplishment reports.<lb/> Monitoring and<lb/> end of funding<lb/> cycle evaluation.<lb/> NPs: progress towards<lb/> programmes objectives and<lb/> targets (linked to USDA<lb/> overarching expected<lb/> achievements).<lb/> ARS : An Action Plan<lb/> Scorecard measures NPs<lb/> outputs and outcomes, using<lb/> narratives from the reports to<lb/> provide evidence for impact<lb/> Evaluation of expected impact.<lb/> Programme result-based<lb/> management.<lb/> Mixed review panels (academics,<lb/> stakeholders and government).<lb/> Achievement of programme-based quantitative<lb/> targets; science quality, client satisfaction,<lb/> diffusion of scientific output beyond academia.<lb/> CSIRO<lb/> Yes, guidelines for<lb/> studying cases released<lb/> in Nov 2015.<lb/> Ex post, in itinere<lb/> and monitoring.<lb/> Achieved economic,<lb/> environmental and social<lb/> impact.<lb/> Goal-free evaluation for ex post<lb/> assessment.<lb/> Progress towards impact statement<lb/> for monitoring (=pathways to<lb/> achieving future impacts).<lb/> Impact pathway, Attribution analysis,<lb/> counterfactual.<lb/></table>

			<p>Short case narrative, focused on<lb/> quantification of the steps of the impact<lb/> pathway, with quantification of impact<lb/> (allows for qualitative methods).<lb/></p>

			<p>Mostly quantitative and monetised measures<lb/> (cost-benefit analysis or non-market valuation)<lb/> Incentive to sum monetised impact of cases<lb/> but no aggregation of environment or social<lb/> impact.<lb/></p>

			<head>26</head>

			<table>– AGRICULTURAL RESEARCH IMPACT ASSESSMENT: ISSUES, METHODS AND CHALLENGES<lb/> OECD FOOD, AGRICULTURE AND FISHERIES PAPERS N°98 © OECD 2016<lb/> Table 2. Evaluation design, methods and methodology issues (cont.)<lb/> Organisation<lb/> Existence of guidelines<lb/> at the level of the<lb/> organisation<lb/> Timing<lb/> of<lb/> assessment<lb/> Impact<lb/> dimensions<lb/> assessed<lb/> Theoretical framework<lb/> (Goal-free evaluation vs expected<lb/> impact, qualification of pathways<lb/> and networks, attribution of<lb/> impacts)<lb/> Methodologies : econometric,<lb/> case studies, standardisation<lb/> of cases, narrative<lb/> Measures , monetisation, indexes,<lb/> counterfactual<lb/> Aggregation<lb/> EMBRAPA<lb/> yes<lb/> Ex post<lb/> (pending ex ante)<lb/> Economic, social,<lb/> environmental, institutional<lb/> (pending: political, food<lb/> safety)<lb/> Goal-free evaluation, but in the<lb/> process of changing toward<lb/> contribution to EMBRAPA&apos;s impact<lb/> axes.<lb/> Case-study of technologies.<lb/> Narrative, standardised through a set<lb/> template.<lb/> Calculation of attribution shares.<lb/> Quantitative assessment of economic surplus<lb/> at technology level.<lb/> Internal rate of return at technology and<lb/> EMBRAPA&apos;s level<lb/> Quantitative assessment of multidimensional<lb/> impacts: Ambitec method: impacts scored from<lb/> -15 to +15.<lb/> Aggregation of impact by type of technology:<lb/> calculation of average scores of all cases by<lb/> dimension.<lb/> INRA<lb/> ASIRPA : yes<lb/> Ex post<lb/> All innovations studied are<lb/> screened for economic,<lb/> social, environmental,<lb/> political, and sanitary impact.<lb/> Goal-free evaluation (Not dependent<lb/> on innovation objectives).<lb/> Case studies<lb/> Case studies, with contribution and<lb/> network analysis. Issue of standardised<lb/> case report: template and three<lb/> analytical tools (impact pathway<lb/> characterised with qualitative and<lb/> quantitative data, chronology, vector of<lb/> impact.<lb/> Qualification and quantification of<lb/> multidimensional impacts. Scored from 1 to<lb/> 5/5.<lb/> Economic impact: economic surplus<lb/> calculation.<lb/> Other dimensions: metric designed by expert<lb/> panels based on local descriptors expressed<lb/> by stakeholders.<lb/> Table 3. Implementation<lb/> Organisation<lb/> Selection of evaluators<lb/> (external or internal)<lb/> Internal taskforce and links<lb/> with stakeholders<lb/> (participatory evaluation)<lb/> Sources of data (panels, surveys, national data, interviews)<lb/> ways and means to access impact data<lb/> Data collection methods (interviews, experimental design)<lb/> Number of studies undertaken<lb/> Database built for impact assessment<lb/> CGIAR:<lb/> CGIAR system-wide<lb/> Ex post: external researchers<lb/> selected after a call for<lb/> proposals by SPIA.<lb/> SPIA: small secretariat with 5 non-permanent<lb/> academic researchers and consultants often<lb/> hired.<lb/> Depending on evaluator proposals.<lb/> SPIA: 30 case studies currently investigated.<lb/> CRP programmes<lb/> CRPs:<lb/> external call for tenders by IEA.<lb/> IEA supports evaluations by CRPs.<lb/> CRP&apos;s: data collected by centres in their country using their own<lb/> methods<lb/> CRPs : by end 2016, 15 CRP evaluations + a few<lb/> cross cutting evaluations related to Capacity<lb/> Strengthening, Gender and Partnerships.<lb/> USDA-ARS<lb/> NPs: External review panel<lb/> facilitated by National<lb/> programme team (NPT) report.<lb/> Stakeholders in external review panel<lb/> (selection criteria unknown).<lb/> Data for annual monitoring are provided by the research<lb/> projects.<lb/> Five-year accomplishment reports prepared by NPTs,<lb/> discussion with external review panels, who in turn prepare the<lb/> NP evaluation report<lb/> All NPs are evaluated at the end of each funding<lb/> cycle.<lb/> CSIRO<lb/> Usually: call for tenders and<lb/> self-evaluation with external<lb/> validation.<lb/> For early feedback on risky<lb/> projects: totally internal self-<lb/>assessment<lb/> Performance and Evaluation unit: 1.5 FTE<lb/> dedicated to impact assessment.<lb/> Quality assurance by internal senior economist.<lb/> Several workshops by case with stakeholders<lb/> Depending on evaluator and stakeholders&apos; proposals.<lb/> Data sets created by the stakeholders or the research team,<lb/> surveys, focus groups…<lb/> 13 case studies completed and 9 under study.<lb/> Database created to collect data related to monitoring<lb/> of future impact<lb/> EMBRAPA<lb/> Internal evaluation<lb/> Dedicated team at the headquarters and<lb/> representative in each of the 42 centres<lb/> Centres&apos; staff collects data for ex post evaluation Annual<lb/> updating (pending ex ante)<lb/> National statistics data + 10 farm surveys/case<lb/> Database of 110 technologies and 220 cultivars.<lb/> INRA<lb/> Internal evaluation with external<lb/> validation<lb/> Small unit (1 FTE and 4 non-permanent<lb/> researchers) for capacity building on impact<lb/> evaluation.<lb/> Stakeholders provide some local impact<lb/> descriptors<lb/> 6 to 10 interviews with stakeholders provide some impact<lb/> indicators, and validation of impact data.<lb/> Table 4. Utilisation of evaluation&apos;s results<lb/> Organisation<lb/> Use of results for demonstrating<lb/> impact (accountability, advocacy,<lb/> communication)<lb/> Use for organisational learning<lb/> Capacity training carried out<lb/> Use for decision-making,<lb/> CGIAR<lb/> Ex post impact evaluations and Impact<lb/> briefs on CGIAR website.<lb/> CRPs evaluation reports published on<lb/> CGIAR website.<lb/> IEA cross-analysis of Phase1 evaluation of<lb/> 17 CRPs (pending).<lb/> n.a.<lb/> The reviews of CRPs and their accomplishment during the<lb/> 2010-15 strategic plan is made available to funders.<lb/> USDA-ARS<lb/> All NPs post their Action Plans, Annual<lb/> Reports, and Five Year<lb/> Accomplishment Reports, and the<lb/> Executive Summaries of the reviews by<lb/> External Assessment Panels.<lb/> ARS post impact briefs on their web.<lb/> Through the NPs next Action Plan<lb/> n.a.<lb/> The reviews by external panels provide insight as to the<lb/> future direction of the research, programme areas or focus,<lb/> serving management purposes.<lb/> CSIRO<lb/> Plan to use the case studies to<lb/> document the annual report of CSIRO.<lb/> Use by units for self-assessment report<lb/> to international panel.<lb/> Not enough case studies yet.<lb/></table>

			<p>Plan to use monitoring for identifying what are the<lb/> keys to generate impacts and tracking opportunities<lb/> for business and partnership development.<lb/></p>

			<p>Some tools being designed: on line and face-<lb/>to-face course for senior researchers to plan or<lb/> monitor their project impact (MOOC).<lb/></p>

			<figure>EMBRAPA<lb/> Annual report featuring internal rate of<lb/> return posted on the web.<lb/> n.a.<lb/> n.a.<lb/> No (planned in 2016)<lb/> INRA<lb/> Used for the evaluation of Research<lb/> thematic divisions.<lb/> Used for the evaluation of institute.<lb/> Some insights provided by the cross analysis of<lb/> cases, and a typology of impact generating<lb/> mechanisms.<lb/> Used by 50% of INRA&apos;s scientific divisions.<lb/> Training sessions planned in 2016 (in-house<lb/> and external).<lb/> No<lb/> n.a.: not available.<lb/></figure>

			<head>4. Challenges of ARIA: Promoting improved practices?<lb/></head>

			<p>Since 2000, there has been a new momentum for agricultural research and accordingly a renewed<lb/> interest for Research Impact Assessment (RIA). This is visible both when surveying the academic literature<lb/> and when studying RIA in practice in some of the major Public Research Organisations (PROs).<lb/> Some of the main lessons from the survey of existing methods:<lb/></p>

			<p> In a context where research and innovation has to address big societal challenges, it is necessary for RIA<lb/> to deal with two main issues: (i) take into account broader impacts, beyond impacts on science and<lb/> economic impacts; and (ii) improve knowledge on impact generating mechanisms.<lb/>  RIA methods face increasing difficulties due to key characteristics of research and innovation systems:<lb/></p>

			<p>(i) research and innovation systems are increasingly open and complex; and (ii) they are changing at a<lb/> quick pace. Complexity and instability do not enable implementation of quantitative methods that allow<lb/> solving the attribution problem in an appropriate way. Furthermore, in complex systems, the impact is not<lb/> additive but depends on productive interactions. To face this situation, some scholars suggest shifting<lb/> from attribution to the analysis of contribution of a variety of actors.<lb/></p>

			<p> The two sets of methods presented here ( &quot; standard economic approaches &quot; and &quot; approaches based on<lb/> case studies &quot; ) are complementary. It is crucial to develop approaches that match quantitative and<lb/> qualitative analysis. This will reinforce the credibility of RIA.<lb/></p>

			<p> These two sets of methods still need improvement. For standard economic approaches, the two<lb/> limitations are: (i) the weak ability to take into account private research and (ii) the (quasi-exclusive) focus<lb/> on economic impacts. For case studies, the main limitation is related to the (still) low level of<lb/> standardisation. For both sets of methods, it is crucial to develop adequate databases and metrologies to<lb/> take into account non-economic impacts.<lb/> Concerning RIA in practice in the five organisations selected:<lb/></p>

			<p> The findings of this study confirm the gap between academic research and practices of RIA. However, it<lb/> also shows that in some organisations interactions between research and practice is organised in a<lb/> systematic way. In these cases social scientists are involved in the design of approaches.<lb/>  RIA is high on the agenda of all the organisations. They are all keen to build credibility of RIA for securing<lb/> funding. However, evidence shows a gap between external communication on RIA and practices. This is<lb/> partly related to budget constraints that restrain the resources devoted to RIA in almost all the<lb/> organisations.<lb/>  Evaluation systems vary across organisations. One of the main differences is related to the scope of<lb/> assessment that depends on the internal organisation. Organisations that have adopted a programme (or<lb/> flagship) structure assess the programmes (and sometime the projects related to the programmes).<lb/> Organisations that do not have this programme structure have designed approaches based on case<lb/> studies related to technologies and aggregate these technologies according to different organisational<lb/> scales (divisions, centres, the whole organisation).<lb/> The improvement of methods will be fostered by the engagement of institutional and the constitution of a<lb/> community of professionals interacting globally. This is crucial for the future of RIA.<lb/> Overall, the analysis contained in this report allows identifying tensions between different styles of<lb/> evaluation. The assessment systems pursue the same three objectives:<lb/></p>

			<p> Learning: enhance the know-how to produce an environment conducive to socio-economic impact;<lb/></p>

			<p> Capacity building: spread the culture of socio-economic impact to its researchers;<lb/></p>

			<p> Reporting to stakeholders: from accountability purposes to advocacy targeted to various audiences.<lb/> Yet none of the five systems manages to adequately meet these three objectives. The accountability<lb/> objective, including for the purposes of return on the financial investment, poses particularly complex<lb/> challenges, and conflicts with learning and capacity building objectives.<lb/> It may be necessary to make choices between these objectives, and adopt the corresponding type of<lb/> evaluation. Power (1994) identifies two contrasted ideal-types of evaluation (type 1/type 2) characterised by a<lb/> set of dichotomies. According to him, evaluation may be oriented toward external control or internal learning;<lb/> unidimensional or multidimensional; evaluation process may assume low trust or high trust between<lb/> evaluators and evaluated; evaluation may be performed by external experts or selected insiders; etc.<lb/> According to the characteristics of research and to the importance of learning and capacity building in<lb/> research organisations, one should pay attention to foster styles of evaluation of type 2. Research<lb/> organisations are learning organisations based on distributed intelligence; they are more networks than<lb/> hierarchies. Hence, as suggested by <ref type="biblio">Kuhlmann (2003)</ref>, it is necessary to consider RIA as a central tool for<lb/> strategic intelligence. RIA methods should not be implemented in a command and control logic but give a<lb/> sense of ownership to the members of the organisation, in order to nurture a culture of impact.<lb/></p>

			<head>Appendix<lb/></head>

			<figure>Practices of RIA<lb/> in Five International Public Agricultural Research Organisations<lb/> USDA-ARS (and ERS)<lb/> In the United States in 2013, federal, state, and private institutions funded and performed roughly<lb/> USD 16.3 billion worth of R&amp;D for food and agriculture. Of this total, the majority was funded and performed by<lb/> the private sector. The Federal government funded approximately USD 2.8 billion of R&amp;D, of which the US<lb/> Department of Agriculture (USDA) accounts for about 20-25%.<lb/></figure>

			<p>The main tool for evaluating all federal agencies, including research agencies, is the Government<lb/> Performance and Results Act (GPRA).<lb/> 14 USDA is also a partner in the STAR METRICS consortium<lb/> 15 (Science<lb/> and Technology for America&apos;s Reinvestment: Measuring the EffecT of Research on Innovation,<lb/> Competitiveness and Science) between US federal science agencies and research institutions to document<lb/> the return on investment, research impact, and social outcomes of federally-funded R&amp;D.<lb/> Research performed at USDA is heavily oriented toward agriculture, but also includes research on<lb/> natural resources, food and nutrition, economics and statistics, and rural development. The Agricultural<lb/> Research Service (ARS) is the USDA&apos;s chief research in-house agency. It operates 17 National Programmes<lb/> and employs more than 2 000 scientists in more than 90 laboratories throughout the country. This appendix<lb/> will focus on USDA&apos;s methods for the evaluation of agricultural research programmes at ARS, as well as the<lb/> Economic Research Service (ERS), which is only marginally concerned with research, but provides insights<lb/> on agricultural research productivity.<lb/></p>

			<head>ERS<lb/></head>

			<p>The ERS&apos;s chief mission is to inform and enhance public and private decision making on economic and<lb/> policy issues related to agriculture, food, the environment, and rural development. Only a few reports concern<lb/> research.<lb/> ERS reports address agricultural productivity and investigate the direction and efficiency of the public<lb/> and private sectors in enhancing the stock of agricultural knowledge and in developing new technologies with<lb/> the objective to inform key decision makers in USDA, federal, state and local government agencies, and all<lb/> groups interested in public policy issues.<lb/> ERS has published 38 studies regarding agricultural science policy since 1996,<lb/> 16 concerning agricultural<lb/> productivity or the performance of the public and private sectors in the US agricultural research system,<lb/> including but not restricted to federal funding.<lb/> ERS has prepared a recent summary of the studies of the rates of return to public agricultural R&amp;D in the<lb/> United States (OECD, 2016). There are very few studies that attempt to incorporate economic analysis of both<lb/> public and private R&amp;D impacts in the same analytical framework, and those that do are still limited in their<lb/> analysis. Data do not allow distinguishing easily neither the private and public research economic impacts, nor<lb/> the federal and state research. Furthermore agricultural research impact is not always separated from the<lb/> influence of other factors. Therefore the calculation of the overall rate of return of research is mostly<lb/> convincing for broad issues.<lb/></p>

			<table>14.<lb/> www.whitehouse.gov/omb/performance/gprm-act.<lb/> 15.<lb/> www.starmetrics.nih.gov/.<lb/> 16.<lb/> www.ers.usda.gov/publications.aspx?sortExpression=date&amp;sortDirection<lb/> =DESC&amp;topicId=1793, consulted on 15 March 2016.<lb/></table>

			<p>Besides, ERS is not charged with the evaluation responsibility of ARS research programmes. In 2010<lb/> ERS published a report on methodological issues in the economic impact assessment of agricultural R&amp;D,<lb/> with illustrations taken from three ARS programmes (<ref type="biblio">Heisey et al., 2010</ref>).<lb/> 17 ERS research also addresses the<lb/> complementary roles of public and private R&amp;D (see for example Fuglie and <ref type="biblio">Toole, 2014</ref>).<lb/></p>

			<head>ARS<lb/></head>

			<p>The ARS annual budget is established on the basis of a five-year strategic plan (presently strategic plan<lb/> FY 2012–2017 revised in 2014). ARS research is organised into 17 National Programmes (NPs) which serve<lb/> to bring coordination, communication, and empowerment to approximately 750 research projects carried out<lb/> by ARS.<lb/> ARS NPs are divided into four major broad categories:<lb/>  Nutrition, Food Safety, and Quality<lb/></p>

			<head> Animal Production and Protection<lb/>  Natural Resources and Sustainable Agricultural Systems<lb/>  Crop Production and Protection<lb/></head>

			<p>The primary tool of research evaluation for ARS is the five-year programme planning and evaluation<lb/> cycle. There is neither a specific budget line nor a unit dedicated to impact evaluation. Fourteen performance<lb/> measures describe specific measurable achievements, which indicate progress toward reaching USDA<lb/> strategic goals and priorities. Baseline and performance targets to be reached by 2017 are established for<lb/> NPs and projects within the programmes, to align them with performance measures and ARS vision for<lb/> agricultural research. Most targets established for programmes are on providing research outputs, but a few<lb/> targets concern research outcomes or impacts. Funds for the monitoring and evaluation of targets are planned<lb/> within the programme budget.<lb/> Programme performance against targets is monitored annually. Each NP Team (NPT) prepares an<lb/> annual report featuring the NP&apos;s major accomplishments. At the end of each NP&apos;s five-year cycle, an<lb/> accomplishment report is prepared by the NPT, and a retrospective Review by an external panel is convened.<lb/> The NPT puts together an accomplishment report, selecting research outputs to illustrate accomplishments in<lb/> the impact areas identified at the outset of the five-year period. There is no unified procedure for the terms of<lb/> reference or content of retrospective reviews, as they are dependent on the national programme objectives.<lb/> An outside group of experts (made up of academics, stakeholders, and government) give their feedback on<lb/> the programme. Criteria used by the review panels include achievement of goals, client satisfaction, and<lb/> impact of scientific outputs. While experts can observe outputs or scientific accomplishments, and discuss<lb/> research relevance, they sometimes state that they lack data to observe the impacts of recent funding on<lb/> society.<lb/></p>

			<head>18<lb/></head>

			<p>An Action Plan Scorecard measures NPs outputs and outcomes, using narratives from the reports to<lb/> provide evidence for impact.<lb/></p>

			<p>There are, occasionally, simple economic evaluations driven by individual grants and interests, at the<lb/> level of an entire research structure. For example the Dairy forage centre (one centre of ARS) made a simple<lb/> calculation exercise of financial impacts of some of its research programmes, seemingly with the assistance of<lb/> an outside consulting firm (USDA-ARS, 2015a).<lb/> The results of ARS evaluations are used for communication, accountability and management purposes.<lb/> ARS impact Report (USDA-ARS, 2015b) presents narratives of recent achievements of ARS research in<lb/> crop and animal production, disease and pest protection, bioenergy, natural resources, food safety, and<lb/> human nutrition. All NPs post their Action Plans, Annual Reports, Five Year Accomplishment Reports, and the<lb/></p>

			<table>Executive Summaries of the reviews by External Assessment Panels.<lb/> 19<lb/> 17.<lb/> This report applies qualitative economic reasoning to the evaluation of three case studies of ARS research offerings<lb/> (Bovine Quantitative Genetics and Genomics, Water Quality and Watersheds, Nutrient Data Laboratory).<lb/> 18.<lb/> See for example National Program 304 Crop Protection and Quarantine External Review Assessment, December 2013,<lb/> National Program 216 or: Agricultural System Competitiveness and Sustainability Executive Summary, 22 December<lb/> 2011<lb/> 19.<lb/> www.ars.usda.gov/research/programs.htm.<lb/></table>

			<p>The purpose of external reviews is two-fold: they ensure that the research is being conducted as<lb/> indicated in the Action Plan (summative); and they provide insight as to the future direction of the research,<lb/> programme areas or focus, serving management purposes (formative).<lb/></p>

			<head>Key references<lb/></head>

			<p>In addition to an interview with an ERS researcher, some key bibliographic documents were studied:<lb/> Fuglie, K.O., and A.A. Toole (2014), &quot;The evolving institutional structure of public and private agricultural<lb/> research&quot;, American Journal of Agricultural Economics Vol. 96, pp. 862–883.<lb/> DOI: http://dx.doi.org/10.1093/ajae/aat107.<lb/> Heisey, P., J.L. King, K. Rubenstein, D.A. Bucks, and R.</p>

			<head>Welsh (2010), Assessing the Benefits of Public<lb/> Research Within an Economic Framework. The Case of USDA&apos;s Agricultural Research Service,<lb/></head>

			<table>Economic Research Report No. 95, USDA Economic Research Service. May.<lb/> www.ers.usda.gov/webdocs/publications/err95/7547_err95_1_.pdf.<lb/> OECD (2016), Innovation, Agricultural Productivity and Sustainability in the United States, OECD<lb/> Publishing, Paris. DOI: http://dx.doi.org/10.1787/9789264264120-en.<lb/> USDA-ARS (2015a), Improving the Economic and Environmental Sustainability of Dairy Forage Farm<lb/> Systems. 8 examples showing the $1.5 billion impact of research at the U.S. Dairy Forage<lb/> Research Center.<lb/> www.ars.usda.gov/ARSUserFiles/50901500/pdf&apos;s/impact%20statement%20(8)%20v1.pdf.<lb/> USDA-ARS (2015b), Impacts—Selected accomplishments under research, education, and economics<lb/> mission area goals, USDA-ARS Office of Communication.<lb/> www.ars.usda.gov/is/np/ARSImpacts/ARSImpactsIntro.htm.<lb/> USDA-ARS (2012), ARS Strategic Plan for FY 2012-2017, The Service, Washington, D.C.<lb/> www.ars.usda.gov/ARSUserFiles/00000000/StrategicPlan/USDAARSFY2012-<lb/>2017StrategicPlan.pdf.<lb/> Link to ARS 17 national programmes (NPs): www.ars.usda.gov/research/programmes.htm<lb/>  NP 211, Water Availability and Watershed Management (action plan, accomplishment report,<lb/> assessment executive summary)<lb/>  NP 216, Agricultural System Competitiveness and Sustainability (action plan, accomplishment report,<lb/> assessment executive summary)<lb/>  NP 101, Food Animal Production (action plan, accomplishment report, assessment executive<lb/> summary)<lb/>  NP 107, Human Nutrition (action plan, accomplishment report, assessment executive summary).<lb/> INRA<lb/> The French National Institute for Agricultural Research (INRA) employs 8 300 people (of which<lb/> 1 800 researchers and 2 500 engineers), for an annual budget of EUR 880 million (in 2014). INRA is<lb/> organised into 13 scientific divisions. INRA&apos;s missions have been set by the laws on research in 1982 and<lb/> 2006, recently revised in 2015, and its scientific priorities are set every ten years in a strategic guidelines<lb/> document (presently 2010-20).<lb/></table>

			<head>History of impact assessment<lb/></head>

			<p>Before 2009, while science quality was monitored and evaluated, there was no system in place to<lb/> evaluate societal impact in INRA. Some reports were commissioned, for example in 1995 two external<lb/> research teams from École des Mines Paris reported on 12 ex-post case studies of innovation successes and<lb/> failures of INRA.<lb/> 20 Since 1996 the communication unit has maintained a database of salient facts, the &quot; Zoom &quot;<lb/> database, to illustrate the annual report and other communication needs. This database gathers narratives<lb/> and data regarding more than a thousand innovations.<lb/> Since 2009, INRA, like other French Public Research Organisations, is assessed every five years by an<lb/> external agency called HCERES (formerly the AERES). The first external assessment report in 2009<lb/> recommended INRA to go beyond the evaluation of the scientific quality, and address the socio-economic<lb/> impact of its research. As there was no universal standard for doing so, it was suggested that a<lb/> comprehensive system for impact assessment should be developed. The AERES experts argued that it would<lb/> lead to a greater legitimacy and increase INRA&apos;s role in mediating science-society debate on growing scientific<lb/> issues.<lb/> Following these recommendations, a team of seven INRA researchers in economy and sociology was<lb/> appointed to design a methodology that could be implemented to perform an Analysis of the Socio-economic<lb/> Impacts of the Public Agricultural Research, and be tested on INRA: the ASIRPA project was launched in<lb/> 2011. After a pilot phase in 2011-13 (<ref type="biblio">Colinet et al 2014</ref>), several scientific divisions of INRA tested the<lb/> approach in real assessment conditions in 2013-15, and a resulting impact assessment system was formally<lb/> institutionalised in INRA in 2015. In parallel, in 2013, INRA&apos;s economists were asked to calculate the Internal<lb/> Rate of Return of the French agricultural research (<ref type="biblio">Butault et al., 2015a, 2015b</ref>). They used standard methods<lb/> described in Section 2 of this report.<lb/></p>

			<head>Purpose of impact evaluation and evaluation design<lb/></head>

			<p>The ASIRPA approach is based on the use of standardised ex post case studies (<ref type="biblio">Colinet et al., 2013;<lb/> Joly et al., 2015</ref>). It chiefly serves three purposes: to facilitate a culture of impact that will improve research<lb/> management practices; to stimulate the understanding of the context, processes and the mechanisms through<lb/> which impact is generated (i.e. the chains of operations which &apos;translate&apos; research knowledge into a format<lb/> which can be put into practice by users beyond the academic sphere); to demonstrate the value of the<lb/> Institute&apos;s research to stakeholders and the public at large, taking into consideration that these stakeholders<lb/> often have different, even contradictory, values and priorities.<lb/></p>

			<p>Based on the information gathered in the Zoom database of salient facts, research outputs, beneficiaries,<lb/> and impacts, the information on a thousand INRA innovations (gathered from 1995 to 2012) has been<lb/> codified. The analysis produced a limited number of impact patterns (<ref type="biblio">Gaunand et al., 2015)</ref>, which facilitates<lb/> the selection of a representative sample of cases. The selection of cases is completed by interviews with<lb/> heads of scientific divisions.<lb/> ASIRPA is an ex post and backwards approach, which means that the analysis starts with identifying the<lb/> impact observed and working back the impact pathway to qualify the contribution made by research and that<lb/> of other partners. The impact for all cases is investigated, characterised and quantified along five dimensions:<lb/> economic, political, social-territorial, environmental, and health. The ASIRPA standard for case studies<lb/> comprises a report and three analytical and visual tools: a chronology which defines the beginning and end of<lb/> each case, along with the principal events taking place between these dates; an impact pathway, which drafts<lb/> the different phases and actors involved in the impact generation chain. It enables identification of the specific<lb/> contribution of INRA in the innovation network, analysing the role of contextual factors and identifying the<lb/> critical mechanisms which underpin impact-generating actions; an impact vector summarised in a table and<lb/> illustrated with a radar chart rates the intensity for each of the five dimensions of impact.<lb/></p>

			<table>20.<lb/> Les chercheurs et l&apos;innovation: regards sur les pratiques de l&apos;INRA, 1998, Paris : Quae.<lb/></table>

			<p>In terms of metrics, impact is assessed using local descriptors collected in interviews with stakeholders<lb/> and beneficiaries, for each &quot;professional adhocracy field&quot; related to a case study, and for each dimension of<lb/> impact (economic, environmental, political, human health, social). To compare impact across cases, and<lb/> above all to give an overall picture of the diversity of INRA&apos;s impact, the impact intensity is quantified on an<lb/> ordinal scale from 1 (weak impacts) to 5 (strong impacts). A scoring metric template is established for each<lb/> impact dimension based on an inter-case comparison. The translation of quantified values into rankings from 1<lb/> to 5 is based on a scoring range built by ASIRPA using expert panels. The work with expert panels, required<lb/> to rate impacts of all standardised cases in a comparable manner, has been completed for economic, political,<lb/> and environmental impact; a similar approach is planned for health, social and territorial impact. Stakeholders&apos;<lb/> opinions are taken into consideration to validate the impact pathway and characterise the impacts. In this<lb/> regard, the approach developed by the French Agricultural Research Centre for International Development<lb/> (CIRAD),<lb/> 21 which is comparable to ASIRPA, uses a more participatory method by organising workshops<lb/> where stakeholders also have a say in the terms of reference for the assessment.<lb/> Regarding the aggregation of cases, the process rests on the design of a database, built by encoding the<lb/> qualitative information collected in the standardised case studies. The variables are related to each step of the<lb/> impact pathway, and were designed during the project based on the first pilot cases studied.<lb/></p>

			<head>Implementation<lb/></head>

			<p>The whole impact assessment system of INRA is co-ordinated and methodologically supported by the<lb/> ASIRPA team, comprising of one full-time equivalent (FTE) engineer at the Delegation for Evaluation, with the<lb/> support of five researchers.<lb/> The ASIRPA project started with the study of 14 cases designed to build and standardise the<lb/> methodological tools. The ASIRPA research team selected the cases for their representativeness of INRA and<lb/> the methodological issues they raised. The team conducted the interviews and drafted the reports.<lb/> Once the standard was stabilised, INRA top-level management prompted the scientific divisions to study<lb/> cases and document their impact in the self-assessment report that is submitted every five years for review by<lb/> an international panel. Seven scientific divisions requested the support of the ASIRPA team and produced<lb/> 27 cases. Cases were selected by the head of division&apos;s team, who further co-ordinated the instruction and<lb/> drafting of case studies by the principal investigators of each case, in close relation with the ASIRPA team,<lb/> who was responsible for the enforcement of the standard methodology. Forty-one case studies have been<lb/> produced so far.<lb/></p>

			<p>The principal investigator of each division team identifies the relevant stakeholders and conducts semi-<lb/>directed interviews to collect data related to each step of the impact pathway. Impact data must originate from<lb/> sources that are external to INRA. Desk research, database analysis (contracts, patents, etc.) also complete<lb/> the information.<lb/></p>

			<p>To scale-up from the sample of single cases, the first 33 case studies were encoded along a hundred of<lb/> variables identified for performing a cross-cutting analysis. Some descriptive statistics were also performed to<lb/> build a typology of four categories of pathways, with corresponding impact pathways, and the main<lb/> mechanisms and critical factors which underpin impact-generating actions.<lb/> The next objective for INRA is to increase the number of cases produced in order to deepen the<lb/> representativeness of cases, increase the robustness of the typology and better characterise the societal<lb/> impact of INRA&apos;s scientific divisions. Given the limited resources of the ASIRPA team, generalising the<lb/> implementation of the ASIRPA methodology requires a system to build internal capacity and increase the<lb/> delegation of support delivery to principal investigators and head of divisions&apos; teams. Each research division<lb/> will designate an &quot; impact champion &quot; to assist the researchers who want to develop a case study. An important<lb/> objective of ASIPRA&apos;s next phase will also be to investigate ways to extract rich information and knowledge<lb/> from the typology and ex post studies, in order to improve INRA&apos;s methods of producing impact. Another<lb/> challenge is the necessity to update the cases, with new data on their impacts, changes in the networks<lb/> involved and the external context.<lb/></p>

			<head>Utilisation of evaluation results<lb/></head>

			<p>The results yielded by the implementation of the evaluation approach enabled capacity building,<lb/> knowledge and accounting, and may assist managers in decision-making processes.<lb/> 21.<lb/> See impact evaluation methodology toolbox, ImpresS, developed by CIRAD at: http://impress-impact-<lb/>recherche.cirad.fr/impress/a-five-step-method.<lb/></p>

			<head>CSIRO<lb/></head>

			<p>In Australia, the Commonwealth Scientific and Industrial Research Organisation (CSIRO) was<lb/> established in 1916. CSIRO employs over 5 000 people, and is funded by the Department of Industry,<lb/> Innovation and Science with a 2014-15 budget of AUD 1.2 billion (CSIRO Operational Plan 2014–15,<lb/> &quot;Achieving positive impact together&quot;) (<ref type="biblio">CSIRO, 2014</ref>). CSIRO is organised into three Lines of Business (LoB):<lb/> 24<lb/> 1) Impact Science; 2) National Facilities and Collections; and 3) CSIRO Services. Within its Impact Science<lb/> LoB, there are nine Business Units (BU, previously known as Flagships), that focus on the biggest challenges<lb/> facing the nation across key sectors: Agriculture, Food and Nutrition, Health and Biosecurity, Data 61, Land<lb/> and Water, Mineral Resources, Manufacturing, Energy, and Oceans and Atmosphere.<lb/></p>

			<head>History of impact assessment<lb/></head>

			<p>Since the 1990s and until 2011, CSIRO was funded by the Federal Government through a quadrennial<lb/> funding cycle. To prepare for each new funding phase, and in order to assess the performance achieved<lb/> during the previous period, reviews were conducted by external consultants (the last review of this type was<lb/> issued in 2014 by ACIL Allen Consulting, 2014). These reviews drew on representative case studies to<lb/> calculate the overall organisational economic value of CSIRO during the period, calculating a total Internal<lb/> Rate of Return. The quadrennial funding cycle was replaced with a four-year rolling funding agreement<lb/> process, which requires CSIRO to provide ongoing insight into business units&apos; performance on an annual<lb/> basis by reporting on key performance indicators linked to their strategic and corporate plans.<lb/></p>

			<head>Purpose of impact evaluation and evaluation design<lb/></head>

			<p>Two key drivers recently led to the renewal of the impact reviewing procedure. First, findings from a<lb/> Deloitte assessment report called for a common framework and procedure for evaluating the different flagship<lb/> programmes of CSIRO. Second, CSIRO was increasingly interested in linking ex post assessment to<lb/> monitoring of impact. In 2010, this combination of factors led to the development of a consistent, organisation-<lb/>wide approach to impact assessment and management. This decision launched the Impact 2020 project,<lb/> which investigated and designed an impact based system that replaced the temporary external assessment<lb/> process. The 2013 Public Governance, Performance and Accountability Act<lb/> 25 further supports that shift, by<lb/> imposing that government agencies use more mixed methods (than internal rate of return only) to assess their<lb/> broader impact, consistent with the diversity of CSIRO&apos;s missions, and that they focus on impacts instead of<lb/> outputs. In addition, the new CSIRO Strategy 2020,<lb/> 26 released in 2015, continues to maintain CSIRO&apos;s core<lb/> mission which is to deliver triple-bottom-line impact to the nation through its research.<lb/></p>

			<head>27<lb/></head>

			<p>The main goals of the Impact 2020 project,<lb/> 28 as well as the Strategy 2020, were to underline CSIRO&apos;s<lb/> commitment to achieving societal impacts in compliance with the new act, to account for defensible and robust<lb/> evidence of impact, to develop guidelines for assessing impacts across the different lines of business, to<lb/> internally increase the culture of impact, and to evaluate impact outcomes. The key audience of these<lb/> documents was the government, industry, other R&amp;D organisations, and universities. Through its impact<lb/> assessment approach, CSIRO aims to improve: advocacy (increased capacity to articulate future and<lb/> delivered impact); accountability (the ability to provide defensible, robust evidence of impact); analysis (the<lb/> opportunity to better understand and maximise research impact through continuous improvement; and funding<lb/> allocation (better informed decision making). The Impact 2020 initiative asserts that impact assessment not<lb/> only informs investment decisions, but also inspires improved research management practices, and<lb/> particularly collaborating practices. This organisational learning is particularly targeted through monitoring of<lb/> impact. The Impact 2020 initiative also aims at changing the attitude of scientists to partners.<lb/> An important output of the Impact 2020 project is the development of a common framework for all impact<lb/> assessments within CSIRO, to be assessed using a case study approach. This framework is described in the<lb/> Impact Evaluation Guide (<ref type="biblio">CSIRO, 2015</ref>) and is to be implemented by whoever is performing ex post, ex ante<lb/> or in itinere impact assessments of CSIRO. These guidelines notably develop a programme logic based on<lb/> input-outcome-impact model. It is suggested to broaden the dimensions of impacts considered, including<lb/></p>

			<figure>24.<lb/> www.csiro.au/en/About/Strategy-structure/Operating-model.<lb/> 25.<lb/> www.comlaw.gov.au/Details/C2013A00123.<lb/> 26.<lb/> www.csiro.au/strategy/.<lb/> 27.<lb/> Triple bottom line (or otherwise noted as TBL or 3BL) is an accounting framework with three parts: social,<lb/> environmental (or ecological) and financial<lb/> 28.<lb/> www.csiro.au/en/About/Our-impact/Our-impact-model.<lb/></figure>

			<p>economy, environment and society, and to use a mix of methods for investigating the impact, including<lb/> qualitative, cost-benefit analysis and option values (to account for the externalities). Great efforts have been<lb/> provided in an attempt to define each of the three impact categories. Measurement is to be done on a case-<lb/>by-case basis, using quantitative or qualitative methods. However, few tools are provided to assess non-<lb/>economic impact without monetisation.<lb/> Regarding monitoring, within CSIRO the future intended impact is investigated at the BU programme<lb/> level. These future impacts are described in terms of impact pathways, captured in &quot; Impact Statements &quot;<lb/> (pathways to achieve future intended impacts),<lb/> 29 which are aggregated to &quot; Impact Areas &quot; and linked to each<lb/> BU&apos;s goal.<lb/></p>

			<head>30<lb/></head>

			<p>In terms of method, given the diversity of sectors tackled by CSIRO, it is a challenge to identify relevant<lb/> indicators of impacts for each of the sectors. Nevertheless, CSIRO has clearly defined the subcategories of<lb/> environmental, social and economic impacts their research tends to achieve (see <ref type="table">Table 2</ref> of the Impact<lb/> Evaluation Guide<lb/> 31 ).<lb/></p>

			<head>Implementation<lb/></head>

			<p>The Performance and Evaluation team of CSIRO, allocates 1.5 FTE in order to implement the impact<lb/> assessment approach. The team identifies potential case studies, leads the case selection process, assists<lb/> researchers in drafting reports (notably for analysing economic impact), ensures the consistency of the cases<lb/> by promoting the implementation of the guidelines, and mediates the case studies&apos; participants (consultants,<lb/> stakeholders, etc.). The team also assists in the development and maintenance of the impact statements, and<lb/> provides capability building courses. Three different ways to study ex post impact are used (whatever the<lb/> model, a case requires around 8 weeks to be completed):<lb/></p>

			<figure><lb/> External consultants are hired for 90% of the cases and draft reports following CSIRO&apos;s guidelines. This<lb/> model costs AUD 20 000 to AUD 45 000 per case.<lb/></figure>

			<head> CSIRO&apos;s Performance and Innovation team performs the economic analysis and drafts the report which is<lb/></head>

			<p>validated by an external third party. This model costs AUD 5 000 to AUD 8 000 per case.<lb/></p>

			<head> CSIRO&apos;s Performance and Innovation team carries</head>

			<p>out the study totally internally. This model is only used<lb/> to provide very early feedback on risky research, since independent evaluation process is a core value of<lb/> the approach.<lb/></p>

			<p>Two internal calendars incite CSIRO to regularly perform case studies: its annual report and the external<lb/> assessment of its BUs. CSIRO Impact Science BUs are evaluated every four years by an international<lb/> evaluation panel to evaluate their impacts. Previously, BUs presented case studies but these were<lb/> inconsistent, assessing either intended or delivered impacts, and using different methodologies. Guidelines<lb/> are now implemented to study at least one case, each year across each BU, requiring support from the BUs<lb/> as well as the CSIRO&apos;s reporting effort. Still, the total number of cases studied per year is dictated by budget<lb/> and internal resources.<lb/></p>

			<figure>In terms of results yielded by the implementation of the evaluation approach, between 2011 and 2015,<lb/> thirteen case studies have been undertaken in the different research areas of CSIRO. These case studies<lb/> were used to pilot test the Impact Evaluation Guide and refine the assessment process. An additional nine<lb/> cases are being conducted in the 2015-16 period. All twenty-two cases followed the guidelines, which were<lb/> publically released in November 2015.<lb/></figure>

			<p>Cases are selected according to a specific rationale: for each case, a strong counterfactual and a high<lb/> share of attribution back to CSIRO must be ensured. Also, maintaining relationships with the stakeholders<lb/> involved eases the access to evidence and impact data. A cross section of cases should be ensured between<lb/> long investments of CSIRO in traditional research areas involving fundamental science and more recent<lb/> innovative research but no balance of case number is sought between programmes or BUs. The cases must<lb/> cover the three lines of business of CSIRO (Science, National Facilities and Collections and Services). A<lb/> CSIRO case study can be variable in size (either one project, or one field of research comprising over<lb/> 30 projects, or one patent or one license). BUs are actively involved in the case selection process through<lb/> proposing a shortlist of three case studies each to the</p>

			<table>Performance and Evaluation Team. Each case is<lb/> 29.<lb/> See the Energy BUs Impact Statements: www.csiro.au/en/Research/EF/Areas/Our-impact-strategy.<lb/> 30.<lb/> www.csiro.au/en/About/Our-impact/Planning-and-monitoring-our-impact.<lb/> 31 .<lb/> www.csiro.au/en/About/Our-impact/Evaluating-our-impact.<lb/></table>

			<p>described in a template, including the timespan, the stakeholders involved, the counterfactual and the<lb/> contribution.<lb/></p>

			<p>In order to collect data for ex post assessment, two to three workshops are organised with stakeholders<lb/> (partner universities, industries or spin-offs) for each case study. The first workshop provides an inventory of<lb/> the data available, decides on the data to be collected and the measurement method to be used, raises<lb/> analytical questions possibly amending the terms of reference of the study, and makes hypotheses regarding<lb/> the potential benefits. The next workshops are mainly dedicated to co-creating the data sets, and discussing<lb/> and deciding on impact attribution to be assigned to CSIRO. The Performance and Evaluation Team ends up<lb/> processing these data to characterise the economic, environmental and social impact of each case, using<lb/> either cost-benefit analysis, or non-market valuation, or non-monetary quantification or narrative.<lb/> The data collection for monitoring relies on the involvement of programme managers. CSIRO&apos;s BU<lb/> programmes have defined between one and five impact statements each (for instance, one impact statement<lb/> of the Energy BU is &quot;National transition pathways to decentralised, low carbon electricity systems&quot;).<lb/></p>

			<head>32<lb/></head>

			<p>Programmes are responsible for developing and managing the impact pathways (or statements) for the<lb/> portfolio of projects they manage. Monitoring data related to Impact Statements (i.e. future impacts) are stored<lb/> in a central database capable of producing simple reports and graphics to assist assessors. This monitoring<lb/> system, already in place, enables identification of opportunities for business development and partnership<lb/> development, and of the keys to generating impacts. This system also shows what data are already available<lb/> in order to decide on studying a case, and provides an archive of research activities at CSIRO.<lb/> In terms of capacity building, CSIRO&apos;s Performance and Evaluation team is designing two courses<lb/> (partly online and partly face to face) aimed at enabling all staff to plan and monitor ongoing impact pathways<lb/> of their projects or assess their impacts ex post.<lb/></p>

			<figure>Ex post case studies&apos; aggregation relies on calculating a total benefit-cost ratio indicator derived from the<lb/> cost-benefit analysis. For cases for which no cost-benefit analysis has been performed, it is suggested to<lb/> provide the full range of relevant and measurable – monetary and non-monetary – costs and benefits of the<lb/> work programme.<lb/> Regarding the monitoring database in place, it ensures consistency of tracking of impacts and enables<lb/> identification of the key targets when it comes to impact at the level of the whole enterprise. The information in<lb/> that database allows for the creation of CSIRO&apos;s Impact Map,<lb/> 33 a communication tool which describes the<lb/> major impacts being progressed and delivered by the organisation.<lb/></figure>

			<p>The shift toward assessing societal impact, described in Strategy 2020, is still recent at CSIRO and<lb/> several implementation issues remain to be addressed, particularly regarding the selection of cases, the way<lb/> their investigation is conducted and the impact assessed, and the aggregation at the level of CSIRO.<lb/> Regarding the identification of case studies, so far most of the cases proposed for impact assessment by<lb/> BUs are related to commercially-based outputs from industry-oriented research, and few cases are related to<lb/> policies, land use, or climate change (i.e. public good or environmentally based research). That bias prevents<lb/> methodological improvements on the related impact dimensions. Another cultural challenge that may influence<lb/> number and types of cases proposed for investigation is related to the understanding of impact pathways. The<lb/> lack of impact planning and monitoring is affecting CSIRO&apos;s ability to conduct assessments but this is a key<lb/> area under focus for capacity development and support. Besides, the methodological choices related to the<lb/> attribution issue leads to case selection bias. Indeed, accounting for the pre-existing stock of knowledge while<lb/> trying to attribute a share of the impact being assessed is a tricky task. Thus, CSIRO tends to select recent<lb/> cases or cases with specific achievements (as opposed to climate change or land change impacts), where<lb/> attribution share is easier.<lb/></p>

			<p>Implementation issues are also a concern in the case investigation steps, since keeping the study within<lb/> eight weeks is difficult while accounting for other commitments of the stakeholders and researchers involved<lb/> and issues with data collection. Besides, dealing with the competing objectives of producing credible reports<lb/> resulting from an external validation, while developing an internal culture of impact, remains a &quot; golden egg &quot;<lb/> perspective for CSIRO.<lb/></p>

			<figure>32.<lb/> See example impact statements for the Energy BU: www.csiro.au/en/Research/EF/Areas/Our-impact-strategy.<lb/> 33.<lb/> Figure 3 at www.csiro.au/en/About/Our-impact/Our-impact-model/Ensuring-we-deliver-impact.<lb/></figure>

			<p>In terms of impact characterisation, access to data may be hindered by lack of data history for long-term<lb/> cases or by confidentiality matters when competitive advantage or commercial confidences are at stake.<lb/> Regarding quantification, CSIRO is struggling finding skilful external consultants, with experience in impact<lb/> assessment accounting for triple bottom line benefits, which extends beyond performing standard high-quality<lb/> cost-benefit analysis.<lb/> A large number of case studies are required in order to deliver a relevant impact message on the<lb/> different sectors where CSIRO is involved. Increasing the number of cases studied annually is thus an<lb/> important pre-condition. Budget and staff capacity being limited, this increase relies on a greater involvement<lb/> and capacity building of researchers. A cultural shift is required in order to arouse more proposals for cases<lb/> from researchers, as well as a significant increase in funding to the BUs and the Performance and Evaluation<lb/> team to resource the aspired increase in the number of case studies.<lb/> Despite being mentioned in the guidelines released in November 2015, ex ante impact assessment is<lb/> not performed yet and its implementation at CSIRO remains to be planned.<lb/></p>

			<head>Utilisation of evaluation results<lb/></head>

			<p>Impact case studies are used for communication on impacts and reporting. They intake the form of 12-15<lb/> page summaries which feed into other performance reports such as the CSIRO&apos;s annual report<lb/> 34 and the BUs<lb/> self-assessment reports, and published on CSIRO&apos;s website.<lb/> 35 Other infographics or communication tools are<lb/> developed based on these reports and used by each business unit for its own communication.<lb/></p>

			<head>Key references<lb/></head>

			<p>In addition to two interviews conducted with senior managers of CSIRO, some key bibliographic<lb/> documents were studied:<lb/></p>

			<table>Acil Allen Consulting (2014), CSIRO&apos;s Impact and Value. An Independent Evaluation, Melbourne.<lb/> www.acilallen.com.au/cms_files/ACILAllen_CSIROAssessment_2014.pdf.<lb/> CSIRO (2015), Impact Evaluation Guide, CSIRO, Performance &amp; Evaluation Unit.<lb/> www.csiro.au/impact.<lb/> CSIRO (2014), Operational Plan 2014-2015: Achieving positive impact together. www.csiro.au/impact.<lb/></table>

			<figure>EMBRAPA<lb/> The Brazilian Agricultural Research organization, EMBRAPA (Empresa Brasileira de Pesquisa<lb/> Agropecuária), employs 9 800 people (of which 2 400 researchers), for an annual budget of BRL 2.6 billion<lb/> (USD 670 million). EMBRAPA is organised into 42 product-based, basic theme or ecoregional research<lb/> centres. Each centre comprises research or service units. The EMBRAPA strategic plan sets large missions<lb/> and goals over a period of 20 years.<lb/></figure>

			<head>History of impact assessment<lb/></head>

			<p>The first assessment of the impact of EMBRAPA&apos;s technologies started in the middle of the 1980s, as<lb/> part of a national effort for impact assessment (<ref type="biblio">Avila et al., 2015</ref>). Studies calculating economic surplus or<lb/> econometric analyses were performed ex post, either at the level of commodities, grants, programmes,<lb/> regions or the EMBRAPA as a whole. During the 1990s the majority of the econometric studies were related to<lb/> smaller objects (research centre or commodity-oriented research) and based on local initiatives, training<lb/> requirements (Ph.D. or M.Sc. thesis), or at the demand of large international funders (Inter-American<lb/> Development Bank and World Bank).<lb/></p>

			<head>Purpose of impact evaluation and evaluation design<lb/></head>

			<p>The impact evaluation process was renewed in 1997, with the annual issue of an &quot; EMBRAPA social<lb/> balance &quot; report (adapted from a model suggested by the Brazilian Institute for Social and Economic Analysis,<lb/> Ibase). This process chiefly serves accountability purposes, demonstrating the impact of donors&apos; investments,<lb/> thus easing the negotiation of the annual budget with the Government, Congress, and access to international<lb/> loans. Internal learning regarding the impact generation mechanisms is also sought, with regular feedback to<lb/></p>

			<table>34.<lb/> www.csiro.au/en/About/Our-impact/Reporting-our-impact.<lb/> 35.<lb/> www.csiro.au/en/About/Our-impact/Our-impact-in-action.<lb/> the researchers. The main effort concerns ex post impact evaluation. Econometrics was developed in the<lb/> 2000s in collaboration with international researchers. These studies used a variety of models and approaches<lb/> (including TFP) and demonstrated the decisive roles played by investment in agricultural research in Brazil,<lb/> notably those of EMBRAPA. A large initiative was launched to monitor and assess the dissemination and<lb/> impact over time (and until they disappear from the market) of 110 technologies and 220 cultivars. In 2000, the<lb/> Impact Assessment process of EMBRAPA shifted from a one-dimensional approach (economic), to a<lb/> multidimensional approach, accounting for economic, social, and environmental. Other societal impact<lb/> dimensions have later been accounted for: institutional impact, intangible impact, and impact on consumption.<lb/> More recently, the Brazilian Office of the Comptroller General (CGU)</table>

			<p>, which commonly serves as an external<lb/> adviser, checking internal finances of EMBRAPA, and ensuring no bribery is at stake, requested EMBRAPA to<lb/> account for its results and how it intends to achieve the goals set in its strategic plan. This led EMBRAPA to<lb/> launch an initiative to assess ex ante impacts.<lb/></p>

			<p>Each impact dimension assessed relies on specific ex post metrics. Economic impact is calculated as an<lb/> economic surplus, and takes into consideration the adoption rate of the technology and the share of impact<lb/> attributed to EMBRAPA&apos;s contribution. The attribution share to EMBRAPA cannot exceed 70%. Since FTEs<lb/> are also recorded, ROI is calculated for each technology. It appears that economic impacts are highly skewed<lb/> among technologies. For the assessment of the non-economic impact of each technology, an ad hoc method<lb/> has been designed, called Ambitec. The Ambitec method was first employed by EMBRAPA&apos;s researchers in<lb/> ecology, geography, and later joined by economists. Ambitec is a &quot;multi-attribute indicators system&quot;,<lb/> comprising 24 criteria and 125 sustainability indicators, which enable environmental and social impacts of<lb/> technologies (impacts on consumption/food safety are being integrated into Ambitec) to be identified. The<lb/> indicators have been selected from prior experience and field trials (<ref type="biblio">Rodrigues et al., 2010</ref>). For each<lb/> technology, impact data are collected by centres through surveys and interviews with farmers/administrators<lb/> to obtain change coefficients related to a given technology or rural activity effects observed. Ten farmers are<lb/> surveyed for each technology, and average change coefficients are calculated. Ambitec assigns relevant<lb/> coefficients to the different impact criteria, according to its relevance toward effecting socio-environmental<lb/> impacts and its scale of occurrence. This results in an aggregated index of socio-environmental impact<lb/> ranging from -15 to +15. Ambitec accounts for specific evaluation contexts since it allows for emphasizing<lb/> relevant local aspects or evaluation objectives, or excluding non-applicable indicators. Institutional impact of<lb/> EMBRAPA is defined as the impact of research on external actors and on EMBRAPA&apos;s organisation itself. It<lb/> encompasses issues related to knowledge advancement, capacity building and use in public policies. This<lb/> impact dimension is assessed through an internal survey. The intangible impacts of technologies are related<lb/> to knowledge, training and other political and institutional impacts. They are assessed at EMBRAPA since<lb/> 2006 using the ESAC methodology that has been designed by Geopi/Unicamp (Brazil).<lb/> Regarding the characterisation of ex ante impact assessment, the EMBRAPA&apos;s 2014-34 strategic plan<lb/> (EMBRAPA, 2015) set five impact targets: sustainability, insertion in bioeconomy, contribution to public<lb/> policies, poverty reduction and positioning at the frontier of knowledge. The ex ante impact assessment<lb/> criteria of projects are derived from the ex post Ambitec method (type of impact, technologies expected and<lb/> target aimed). Ex ante impact of research projects is to be assessed on a scale running from very negative to<lb/> very positive impact, and along criteria related to the dimensions of impact affected (economic, social,<lb/> environmental, institutional, food safety) and the impact targets.<lb/> EMBRAPA&apos;s methodological choices still encompass challenges. In terms of method, the rules to decide<lb/> on attribution shares among EMBRAPA and stakeholders remain unclear. Only public research organisations<lb/> (no private research facilities) are considered for a potential contribution to EMBRAPA&apos;s impact. Another<lb/> important objective for EMBRAPA&apos;s current work is its policy impact. Policies that EMBRAPA&apos;s researchers<lb/> have investigated have been reported (the 2014 survey reported 60 policies). Two different approaches<lb/> (quantitative using aggregated data, or qualitative inspired by expert panel work by ASIRPA) are being<lb/> considered for assessing and monitoring this impact. Regarding ex ante impact assessment, the tools to<lb/> enable researchers to assess and argue on their credible expected impacts remain to be developed.<lb/></p>

			<head>Implementation<lb/></head>

			<p>The whole impact assessment and monitoring system of EMBRAPA is co-ordinated and<lb/> methodologically supported by an impact assessment team located at headquarters, under the guidance of<lb/> the Secretariat of Management and Institutional Development. In each of the 40 regional centre, appointed<lb/> dedicated socio-environmental researchers who are specifically trained for impact assessment, and are<lb/> supported by the headquarter team, are in charge of prospective and annual monitoring of three technologies.<lb/> Case studies are carried out by the centres&apos; &quot; impact team &quot; along with the principal investigator in charge of<lb/> each technology assessed. A budgetary allowance is made available through the Secretariat for Management<lb/> and Institutional Development (SGI) for this task.<lb/> For each technology, ex post economic impact data are collected through national services, rural<lb/> extension contacts and Embrapa or private surveys. Ex post non-economic impacts data are collected and<lb/> processed by centres by implementing the Ambitec method. Ambitec has been designed as a user-friendly<lb/> device, with an integrated software platform. It incorporates formalised and standardised guidelines to assist<lb/> the implementation of Ambitec, where three steps are defined: perimeter definition, field survey including<lb/> indicator&apos;s scaling checklists, and reporting in template. Ambitec is a practical, expeditious, low cost, and<lb/> reproducible socio-environmental impact assessment procedure relevant for the wide range of agricultural<lb/> technologies and rural activities concerned in Embrapa&apos;s research programme. Each technology leads to an<lb/> annual case study report that is prepared by the regional centres, and details the measurement of impacts by<lb/> the scientific board of each centre. This annual assessment is part of the performance measurement system<lb/> of the centres, implemented since 2001.<lb/> At the level of EMBRAPA, the ex post economic impact is calculated by adding all the economic surplus<lb/> of the individual technologies monitored. The productivity of research is estimated by dividing this total surplus<lb/> by the annual budget of EMBRAPA, thus producing a single figure: &quot; each BRL invested generated BRL 8.53<lb/> to Brazilian society &quot; in 2014. At the level of EMBRAPA, the environmental and social impact is aggregated by<lb/> calculating average indexes of impact by type of technology (plant varieties, animal production, software,<lb/> processing technologies). That method does not account for different adoption rates among the portfolio of<lb/> technologies.<lb/></p>

			<p>In 2016, EMBRAPA will implement a global management plan related to ex ante impact assessment.<lb/> According to the draft plan, employees will be linked to projects and actions aligned to the five impact targets<lb/> defined in the strategic plan. Ex ante impact will be assessed by each project leader before each funding<lb/> request is submitted. The Secretariat of intelligence and macro strategy of EMBRAPA will then select the<lb/> projects to be funded, notably accounting for its expected impact. The coming implementation of the strategic<lb/> plan of EMBRAPA 2014-34 is a challenge for the headquarter impact assessment team. The team is to<lb/> reorganise the existing institutional process to evaluate the ex post impacts of the EMBRAPA technologies to<lb/> align with the five impact axes newly established in the 2014-34 strategic plan and to support the R&amp;D<lb/> selection process of new projects with ex ante impact assessment.<lb/></p>

			<head>Utilisation of evaluation results<lb/></head>

			<p>So far, the evaluation&apos;s results are used for advocacy and accountability. Ex post impact assessment is<lb/> annually reported in a standardised Social Balance Report which is largely diffused to stakeholders and<lb/> funders. A website<lb/> 36 is dedicated to the Social Balance of EMBRAPA, and diffuses the databases of social<lb/> actions of EMBRAPA.<lb/> Ex ante impact assessment of technologies is expected to notably influence funding of new research<lb/> projects (with a highly selective rate, selection being done by external experts). Ex ante impact claims could<lb/> also be considered for the annual negotiations of salaries and benefit sharing.<lb/></p>

			<head>Key references<lb/></head>

			<p>In addition to an interview conducted with a senior manager of EMBRAPA&apos;s headquarters, some key<lb/> bibliographic documents were studied:<lb/> Avila, A.F., G.S. Rodrigues, G.L. Vedovoto, R. de Camargo Penteado Filho, and W. Corrêa da Fonseca<lb/> Junior (2015), EMBRAPA&apos;s experience on the impact assessment of agricultural R&amp;D: 15 years<lb/> using a multidimensional approach, presented at the ImpAR Conference, INRA, Paris, France.<lb/> www.alice.cnptia.embrapa.br/alice/bitstream/doc/1036444/1/EmbrapaExperienceontheimpact.pdf.<lb/></p>

			<figure>EMBRAPA (2015), VI Plano Director da Embrapa 2014-2034, EMBRAPA, MBRAPA, Secretaria de<lb/> Gestão e Desenvolvimento Institucional, Brasilia.<lb/> www.infoteca.cnptia.embrapa.br/handle/doc/1025506.<lb/> Rodrigues, G.S., C.C. de Almeida Buschinelli, and A.F. Dias Avila (2010), &quot;An environmental impact<lb/> assessment system for agricultural research and development ii: institutional learning experience<lb/> at Embrapa&quot;, Journal of technology management &amp; innovation Vol. 5, pp. 38–56.<lb/> www.jotmi.org/index.php/GT/article/view/art173.<lb/> 36.<lb/> http://bs.sede.embrapa.br/.<lb/></figure>

			<p>ex post impact assessment studies and publications related to a particular CRP (IEA, 2015a; IEA, 2015b; IEA<lb/> evaluation reports of CRPs and guiding documents). IEA commissioned evaluations are conducted by<lb/> independent and external teams. They have a large scope, including science quality and potential for future<lb/> development impact. The evaluation agenda is set by a four-year Rolling Evaluation Work Plan (REWP). The<lb/> creation of the IEA is closely linked with the donor&apos;s need to monitor their funding efficiency and effectiveness<lb/> towards achieving CGIAR&apos;s three SLOs through the development of result-based management (RBM). The<lb/> IEA also has a role in promoting good practices and capacity building in evaluation.<lb/></p>

			<p>The IEA was also created because, as the ISPC provided scientific and programmatic guidance, the<lb/> SPIA, which is a sub-group of the ISPC, could not evaluate performance arising from this advice. In the new<lb/> structure, SPIA&apos;s role in the development of ex post assessment methodologies for broader societal impacts<lb/> has however been maintained and sustained.<lb/> In terms of guidelines for ex post evaluation of broader impact, SPIA ex post assessment relies on case<lb/> studies that use a diversity of analytical frameworks since these are proposed by the external researchers who<lb/> respond to thematic calls for proposals (SPIA, 2015). Most SPIA case studies, however, rely on quantitative<lb/> econometric analysis, based on cost-benefit analysis and the design of counterfactuals. For environmental<lb/> impact assessment, the SPIA recommended the extension of the standard cost-benefit analysis to include<lb/> revealed and stated preferences to capture non-market effects. In the past, SPIA&apos;s mission included the<lb/> improvement of assessment methodologies and tackling transversal questions across the CGIAR research<lb/> portfolio. This way, SPIA performed state of the art analysis regarding impact assessment methodologies,<lb/> designed guidelines, and implemented them on a set of pilot cases. SPIA did this work for policy-oriented<lb/> impacts in 2006 and for environmental impact in 2008. In 2013, SPIA received a grant called SIAC<lb/> (Strengthening Impact Assessment in the CGIAR) for methodological developments. They include efforts to<lb/> have more accurate estimates of adoption rates, or cross-country impact assessment. Methodologies to<lb/> assess the full range of CGIAR societal impact were also targeted, like the effects on poverty and health,<lb/> natural resources management, or under-researched areas, like the impact of livestock research or social<lb/> science. An open call for proposals was launched in 2014 to gather case studies on these topics, with<lb/> researchers offering their own methodologies (mostly quantitative ones). It resulted in 30 case-studies being<lb/> selected (and currently investigated).<lb/> In terms of programme monitoring, the new SRF will be implemented over several phases, starting with<lb/> the so-called Phase II (2017–2022) portfolios of CRPs. The CGIAR Fund Council asked the CGIAR<lb/> Consortium to establish a clear link between the research carried out by the CRPs and the three CGIAR<lb/> System Level Outcomes (SLOs). SLOs are broken down into 10 Intermediate Development Outcomes (IDOs)<lb/> and 30 sub-IDOs, for which quantitative targets have been set. A result-based management policy is under<lb/> development. Its implementation, planned for 2017 will be driven by the CRPs, the ISPC and the System<lb/> Office.<lb/> Each CRP must derive its own targeted quantitative contribution to CGIAR global targets. CRPs define<lb/> their intended impact by developing impact pathways (outputs-to-outcomes-to-impacts) and related theories-<lb/>of-change (explicit assumptions about how to get from research to development results), which combine<lb/> qualitative and quantitative components. CRPs are expected to design their activities and monitor their<lb/> contribution to the sub-IDO level which is linked to the IDOS through programme theory and then up to the<lb/> SLOs. Methodologies are currently developed by each CRP, with the support of an inter-CRP Monitoring,<lb/> Evaluation and Learning Community of Practice. Sharing of these bottom-up initiatives has not been<lb/> organised yet at the level of the CGIAR.<lb/></p>

			<p>There are some issues with establishing targets. Time lag is clearly an issue as the CGIAR SLO&apos;s<lb/> timeline for targets is too near (2022), and will be achieved through contributing research performed<lb/> 20-30 years ago, before the CRPs existed. CRP recent external evaluations of Phase I funding have also<lb/> found that in many cases targets set to CRPs were unrealistic. CRPs&apos; evaluations have emphasised the need<lb/> to use the theories of change as dynamic concepts where the assumptions may become hypothesis to be<lb/> tested through research. Given the risky and protracted nature of research, result-based management that<lb/> involves adaptive management on the basis of lessons would have to focus on progress and results relatively<lb/> closer to research.<lb/> Researchers operating CRPs expressed their interest for proxies that can link CRPs&apos; research outputs<lb/> and outcomes with CGIAR SLO targets and system-wide indicators tracking impacts.<lb/></p>

			<head>Implementation<lb/></head>

			<p>Data collection is an issue for the implementation of ex post impact assessment at different levels of the<lb/> CGIAR system, as well as the implementation of monitoring approaches to CRPs&apos; progress towards impacts.<lb/> One issue regarding ex post impact assessment is that data sets for regression are not readily available.<lb/> Either researchers bring or collect their own measures and data, or they work on existing publically available<lb/> data, which can be inconsistent. Not all measures are monetary (notably for health and environment impacts).<lb/> For nutrition for instance, indicators deal with diverse scores of food supply and blood test measures. SPIA<lb/> gives methodological support for specific studies. For example, some recommendations were made to<lb/> improve the robustness of data in plant genetic research: research showed that photos or DNA fingerprinting<lb/> gave far better results for identifying improved varieties, and were far more robust, when compared to<lb/> traditional methods for collecting data (surveys, interviews, field observations). A similar recommendation is<lb/> tested for livestock genetics, but it is a costly investment. In terms of aggregation of ex post assessment,<lb/> some studies have calculated an Internal Rate of Return for CGIAR research. Renkow and Byerlee (2010) for<lb/> example have reported an aggregated ex post benefit cost ratio of all CGIAR research investments of 17.26%.<lb/> These estimates are however based heavily on assumptions regarding adoption rates and come with very<lb/> large margins. One respondent to this study considers that robust SPIA impact assessment designs could<lb/> also be discredited in some instances by inflated figures from centres&apos; self-assessment, or inconsistent data.<lb/> SPIA suggested that it &quot; could provide a label of impact assessment quality &quot; on a voluntary basis, based on a<lb/> peer review system of impact studies carried out by the centres.<lb/></p>

			<p>As far as the monitoring of CRPs is concerned, in January 2015 the IEA produced a series of<lb/> six guidance documents concerning the external evaluation of CRPs. Funding for monitoring and evaluation is<lb/> available through the &quot; programme management &quot; budget of CRPs. CRPs are expected to commission<lb/> evaluation of components of the CRP as an input to the IEA&apos;s evaluations. The problem of data availability is<lb/> complicated by issues related to the levels of assessment: most of the data are currently generated at the<lb/> centre level, but the assessment is to be performed at the CRP level. Each centre still develops its own<lb/> evaluation method for establishing the baseline of its accomplishments or the counterfactual for the evaluation<lb/> of a CRP&apos;s results in a particular geography. For example, IRRI calculates baseline values by conducting<lb/> household surveys, which provide rich data sets, but prove to be quite expensive, and may not be applicable<lb/> to all CGIAR centres. Nevertheless, no methodology and monitoring system has so far been shared among<lb/> CRPs to decide on results&apos; attribution versus contribution, outcome calculation methods, etc. In preparation for<lb/> the second funding phase (2017-2022), CRPs propose their own method and a critical need for comparability<lb/> and/or harmonisation has been identified.<lb/> Regarding the implementation of the monitoring initiative, CGIAR plans to enforce an &quot; annual reporting<lb/> or programme progress with financial reporting, and performance assessment &quot; . As a principle of the call for<lb/> Phase II (2017-2022), all CRPs and their lead centres are to follow a harmonised and homogeneous<lb/> monitoring and reporting framework. Standardisation of minimum requirements, consistency and alignment of<lb/> reporting are key to this framework and demand interoperability of platforms. This push for harmonisation in<lb/> the short term represents an enormous workload for CRPs&apos; managers, which calls for capacity building and<lb/> budget. A crucial issue is the continuous monitoring of development outcomes, impacts and targets<lb/> achievement at CRP and System levels, notably after the CRPs have ended. In particular, CRPs&apos; managers<lb/> ask for agreed upon, standard proxies for development outcomes and impact, to limit investments in<lb/> monitoring.<lb/></p>

			<head>Utilisation of evaluation results<lb/></head>

			<p>SPIA impact studies are used for communication purposes. Impact briefs<lb/> 37 about case studies are<lb/> regularly posted on the CGIAR website. They result from SPIA ex post evaluations, other external ex post<lb/> impact assessments commissioned by donors (e.g. ACIAR&apos;s report on wheat improvement in Afghanistan) or<lb/> by the CRPs themselves. There is an issue with too few case studies produced in contrast to CGIAR&apos;s<lb/> widening agenda. For example SPIA will only produce 25-30 case-studies through the SIAC grant, which is<lb/> insufficient to provide useful lessons for donors at the level of CGIAR. Some centres and programmes also<lb/> communicate figures to impress donors, which could affect CGIAR&apos;s reputation.<lb/> In terms of evaluation methodology, ISPC published a white paper<lb/> 38 with recommendations regarding<lb/> planning, domains for research and target groups, trade-offs and theory of change for CRPs&apos; design and<lb/> monitoring. Emphasis was placed on having feasible and realistic intermediate outcome objectives where<lb/> 37.<lb/> http://impact.cgiar.org/impact-briefs.<lb/> 38.<lb/> www.sciencecouncil.cgiar.org/sites/default/files/ISPC_WhitePaper_Prioritization.pdf.</p>


	</text>
</tei>
