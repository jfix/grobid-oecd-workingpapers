<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<titlePage>Please cite this paper as: <lb/>Wu, M. (2010), &quot; Comparing the Similarities and Differences of <lb/>PISA 2003 and TIMSS &quot; , OECD Education Working Papers, <lb/>No. 32, OECD Publishing, Paris. <lb/>http://dx.doi.org/10.1787/5km4psnm13nx-en <lb/>OECD Education Working Papers No. 32 <lb/>Comparing the Similarities <lb/>and Differences of PISA <lb/>2003 and TIMSS <lb/>Margaret Wu <lb/>Unclassified <lb/>EDU/WKP(2010)5 <lb/>Organisation de Coopération et de Développement Économiques <lb/>Organisation for Economic Co-operation and Development <lb/>22-Apr-2010 <lb/>___________________________________________________________________________________________ <lb/>English -Or. English <lb/>DIRECTORATE FOR EDUCATION <lb/>COMPARING THE SIMILARITIES AND DIFFERENCES OF PISA 2003 AND TIMSS <lb/>OECD Education Working Paper No. 32 <lb/>This paper was authored by Margaret Wu of the Assessment Research Centre, University of Melbourne. <lb/>Contact: <lb/>Pablo Zoido; Email: pablo.zoido@oecd.org; Tel: +33 1 45 24 96 07 <lb/>JT03282237 <lb/>Document complet disponible sur OLIS dans son format d&apos;origine <lb/>Complete document available on OLIS in its original format <lb/>EDU/WKP(2010)5 <lb/>Unclassified <lb/>English -Or. English <lb/>Cancels &amp; replaces the same document of 14 April 2010 <lb/>EDU/WKP(2010)5 <lb/></titlePage>

			<page>2 <lb/></page>

			<body>OECD DIRECTORATE FOR EDUCATION <lb/>OECD EDUCATION WORKING PAPERS SERIES <lb/>This series is designed to make available to a wider readership selected studies drawing on the work <lb/>of the OECD Directorate for Education. Authorship is usually collective, but principal writers are named. <lb/>The papers are generally available only in their original language (English or French) with a short <lb/>summary available in the other. <lb/>Comment on the series is welcome, and should be sent to either edu.contact@oecd.org or the <lb/>Directorate for Education, 2, rue André Pascal, 75775 Paris CEDEX 16, France. <lb/>The opinions expressed in these papers are the sole responsibility of the author(s) and do not <lb/>necessarily reflect those of the OECD or of the governments of its member countries. <lb/>Applications for permission to reproduce or translate all, or part of, this material should be sent to <lb/>OECD Publishing, rights@oecd.org or by fax 33 1 45 24 99 30. <lb/>---------------------------------------------------------------------------<lb/>www.oecd.org/edu/workingpapers <lb/>---------------------------------------------------------------------------<lb/>Applications for permission to reproduce or translate <lb/>all or part of this material should be made to: <lb/>Head of Publications Service <lb/>OECD <lb/>2, rue André-Pascal <lb/>75775 Paris, CEDEX 16 <lb/>France <lb/>Copyright OECD 2010 <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>3 <lb/></page>

			<body>ABSTRACT <lb/>This paper makes an in-depth comparison of the PISA (OECD) and TIMSS (IEA) mathematics <lb/>assessments conducted in 2003. First, a comparison of survey methodologies is presented, followed by an <lb/>examination of the mathematics frameworks in the two studies. The methodologies and the frameworks in <lb/>the two studies form the basis for providing explanations for the observed differences in PISA and TIMSS <lb/>results. At the country level, it appears that Western countries perform relatively better in PISA as <lb/>compared to their performance in TIMSS. In contrast, Asian and Eastern European countries tend to do <lb/>better in TIMSS than in PISA. This paper goes beyond making mere conjectures about the observed <lb/>differences in results between PISA and TIMSS. The paper provides supporting evidence through the use <lb/>of regression analyses to explain the differences. The analyses showed that performance differences at the <lb/>country level can be attributed to the content balance of the two tests, as well as the sampling definitions – <lb/>age-based and grade-based – in PISA and TIMSS respectively. Apart from mathematics achievement, the <lb/>paper also compares results from the two studies on measures of self-confidence in mathematics. Gender <lb/>differences are also examined in the light of contrasting results from the two studies. Overall, the paper <lb/>provides a comprehensive comparison between PISA and TIMSS, and, in doing so, it throws some light on <lb/>the interpretation of results of large-scale surveys more generally. <lb/>RESUME <lb/>Le présent document établit une comparaison détaillée des évaluations des mathématiques PISA <lb/>(OCDE) et TIMSS (IEA), toutes deux menées en 2003. Il présente tout d&apos;abord une comparaison des <lb/>méthodologies d&apos;enquête, puis un examen des cadres d&apos;évaluation des mathématiques. C&apos;est en effet par <lb/>une analyse des méthodologies et cadres d&apos;évaluation des deux études que l&apos;on peut expliquer les <lb/>différences constatées dans les résultats du PISA et ceux du TIMSS. Au niveau des pays, il apparaît que les <lb/>nations occidentales réussissent relativement mieux à l&apos;enquête PISA qu&apos;à l&apos;enquête TIMSS. En revanche, <lb/>les pays d&apos;Asie et d&apos;Europe de l&apos;Est ont tendance à obtenir de meilleures performances aux évaluations <lb/>TIMSS qu&apos;aux évaluations PISA. Au-delà de simples conjectures sur les différences observées, le présent <lb/>document fournit des éléments de preuves en utilisant des analyses de régression qui expliquent les <lb/>disparités de résultats. Les analyses ont démontré que les variations de performance au niveau national <lb/>peuvent être imputées à l&apos;équilibre des contenus des deux tests, ainsi qu&apos;aux définitions d&apos;échantillonnage <lb/>– fondées sur l&apos;âge ou fondées sur la classe – pour PISA et pour TIMSS, respectivement. Outre les <lb/>performances en mathématiques, le présent document compare les résultats des deux études sur les <lb/>mesures de la confiance en soi en mathématiques. Les différences entre les sexes sont également <lb/>examinées à la lumière des résultats contrastés des deux enquêtes. Dans l&apos;ensemble, ce document offre une <lb/>comparaison complète entre PISA et TIMSS et, ce faisant, éclaire plus généralement les interprétations des <lb/>résultats d&apos;enquêtes de grande envergure. <lb/></body>

			<front>EDU/WKP(2010)5 <lb/>4 <lb/>FOREWARD <lb/>This paper was authored by Margaret Wu of the Assessment Research Centre, University of <lb/>Melbourne, with contributions from Professor Kaye Stacey, University of Melbourne, and her team in <lb/>classifying PISA and TIMSS items, and in providing references and comments for the manuscript. <lb/>The author would like to express her thanks to Professor Patrick Griffin for reading the manuscript <lb/>and for providing comments. <lb/>EDU/WKP(2010)5 <lb/></front>

			<page>5 <lb/></page>

			<body>TABLE OF CONTENTS <lb/>CHAPTER 1 -INTRODUCTION .................................................................................................................. 7 <lb/>Purpose of the working paper ...................................................................................................................... 7 <lb/>Surveys for comparison ............................................................................................................................... 8 <lb/>Similarities and differences ......................................................................................................................... 8 <lb/>Focus of comparison ................................................................................................................................... 8 <lb/>Background information about the two surveys .......................................................................................... 9 <lb/>The aims of TIMSS and PISA ................................................................................................................. 9 <lb/>Organisation of the report ......................................................................................................................... 10 <lb/>CHAPTER 2 -SURVEY METHODOLOGIES ........................................................................................... 12 <lb/>Introduction ............................................................................................................................................... 12 <lb/>Population definition ................................................................................................................................. 12 <lb/></body>

			<titlePage>Implications of the different definitions ................................................................................................ 13 <lb/>Comparison of age distributions ............................................................................................................ 14 <lb/>Comparison of grade distributions ......................................................................................................... 15 <lb/>Sampling methods ..................................................................................................................................... 17 <lb/>Test characteristics .................................................................................................................................... 18 <lb/>Test length ............................................................................................................................................. 18 <lb/>Test design ............................................................................................................................................. 18 <lb/>Amount of assessment material ............................................................................................................. 19 <lb/>Scaling methodology ................................................................................................................................. 20 <lb/>Field operation procedures ........................................................................................................................ 22 <lb/>Calculator use ........................................................................................................................................ 22 <lb/>Questionnaires ........................................................................................................................................... 23 <lb/>Summary ................................................................................................................................................... 25 <lb/>CHAPTER 3 -COMPARISON OF PISA AND TIMSS MATHEMATICS FRAMEWORKS AND ITEM <lb/>FEATURES .................................................................................................................................................. 28 <lb/>Approaches to the development of the mathematics assessment frameworks .......................................... 28 <lb/>TIMSS mathematics assessment framework ............................................................................................. 28 <lb/>TIMSS mathematics content domains ................................................................................................... 29 <lb/>TIMSS mathematics cognitive domains ................................................................................................ 31 <lb/>PISA mathematics assessment framework ................................................................................................ 32 <lb/>Situation/context dimension .................................................................................................................. 34 <lb/>PISA mathematical content dimension – The overarching ideas .......................................................... 34 <lb/>Mathematical processes dimension ....................................................................................................... 37 <lb/>Classifying items according to competency clusters ............................................................................. 39 <lb/>A Comparison of PISA and TIMSS Mathematics Frameworks ............................................................... 39 <lb/>A comparison of PISA&apos;s content dimension with TIMSS&apos; content dimension ..................................... 39 <lb/>exchange rate ............................................................................................................................................. 40 <lb/>Question 1: EXCHANGE RATE ......................................................................................................... 40 <lb/>staircase ..................................................................................................................................................... 41 <lb/></titlePage>

			<front>EDU/WKP(2010)5 <lb/></front>

			<page>6 <lb/></page>

			<listBibl>Question 1: STAIRCASE ...................................................................................................................... 41 <lb/>CARPENTER ........................................................................................................................................... 42 <lb/>Question 1: CARPENTER .................................................................................................................... 42 <lb/>internet Relay chat ..................................................................................................................................... 43 <lb/>Question 1: INTERNET RELAY CHAT .............................................................................................. 43 <lb/>Question 2: INTERNET RELAY CHAT .............................................................................................. 43 <lb/>exports ....................................................................................................................................................... 44 <lb/>Question 2: EXPORTS .......................................................................................................................... 44 <lb/>A comparison of PISA&apos;s processes dimension with TIMSS&apos; cognitive domains ................................. 45 <lb/>Characteristics of tests and items .............................................................................................................. 46 <lb/>Item Format ........................................................................................................................................... 46 <lb/>Amount of reading ................................................................................................................................. 47 <lb/>Summary ................................................................................................................................................... 48 <lb/>CHAPTER 4 -COMPARISON OF PISA AND TIMSS ACHIEVEMENT RESULTS.............................. 50 <lb/>Comparisons of country mean scores ........................................................................................................ 50 <lb/>Explaining the Differences between PISA and TIMSS Results ................................................................ 56 <lb/>Years of Schooling and Age at time of testing .......................................................................................... 56 <lb/>Impact of Years of Schooling on Student Performance in Mathematics .................................................. 60 <lb/>The impact of differences in content balance between PISA and TIMSS ................................................ 64 <lb/>Achievement by content domains ............................................................................................................. 64 <lb/>Predicting PISA Mathematics Country Mean Scores ............................................................................... 68 <lb/>Implications of differential performance of countries in content domains ............................................... 71 <lb/>TIMSS Data domain and PISA Uncertainty overarching idea .................................................................. 71 <lb/>Examining the spread of PISA and TIMSS achievement distributions .................................................... 72 <lb/>Standard Deviations .................................................................................................................................. 72 <lb/>Percentiles ................................................................................................................................................. 74 <lb/>The impact of calculator availability on differences in performance in TIMSS and PISA ....................... 77 <lb/>Summary ................................................................................................................................................... 77 <lb/>CHAPTER 5 -GENDER DIFFERENCE AND ATTITUDES ................................................................... 79 <lb/>Introduction ............................................................................................................................................... 79 <lb/>Gender differences .................................................................................................................................... 79 <lb/>Gender differences for overall mathematics scale ................................................................................. 79 <lb/>Gender differences by mathematics content areas ................................................................................. 82 <lb/>Possible explanations for observed differences between PISA and TIMSS in gender gap ................... 83 <lb/>Gender difference in the spread of achievement distributions............................................................... 84 <lb/>Attitudinal scales ....................................................................................................................................... 88 <lb/>Self-confidence index ............................................................................................................................ 88 <lb/>Interest and motivation indices .............................................................................................................. 89 <lb/>Gender differences in attitudes towards mathematics ........................................................................... 91 <lb/>Socio-economic Background of Students ................................................................................................. 93 <lb/>Summary ................................................................................................................................................... 94 <lb/>CHAPTER 6 -CONCLUSIONS .................................................................................................................. 95 <lb/>Overview ................................................................................................................................................... 95 <lb/>The impact of content balance on achievement results ............................................................................. 95 <lb/>The impact of reading load on mathematics achievement results ............................................................. 97 <lb/>Grade-based and age-based samples ......................................................................................................... 97 <lb/>Correlated factors ...................................................................................................................................... 98 <lb/>Gender and attitudinal differences ............................................................................................................ 98 <lb/>And finally… ............................................................................................................................................ 99 <lb/>References ............................................................................................................................................... 100 <lb/>EDU/WKP(2010)5 <lb/></listBibl>

			<page>7 <lb/></page>

			<body>CHAPTER 1 -INTRODUCTION <lb/>Purpose of the working paper <lb/>1. <lb/>What would help explain differences between the results in students&apos; mathematics performance <lb/>from two respected international education surveys conducted in 2003: the OECD&apos;s Programme for <lb/>International Student Assessment (PISA) and the International Association for the Evaluation of <lb/>Educational Achievement (IEA)&apos;s Trends in International Mathematics and Science Study (TIMSS)? This <lb/>was a question asked by the PISA Governing Board (PGB) and was at the heart of the development of this <lb/>report. <lb/>2. <lb/>The results of these two international assessments have often been cross-referenced and <lb/>synthesised to present an overall picture of mathematical achievements. Valid cross-references of the <lb/>results from PISA and TIMSS require, however, a clear and accurate understanding of the two assessments <lb/>in terms of their objectives, assessment frameworks and the nature of the tasks and items that were <lb/>presented to students. The objective of this report is to provide such an understanding. 1 <lb/>3. <lb/>This objective is important as comparisons are inevitably made between the results when two <lb/>international surveys are carried out concurrently to measure mathematics achievement, and superficial <lb/>comparisons can often be misleading, inaccurate or simply inadequate. For instance, it is customary to <lb/>brush aside any comparison of PISA and TIMSS by stating that (1) PISA samples are age-based while <lb/>TIMSS samples are grade-based; and (2) PISA is not curriculum-driven while TIMSS is based on <lb/>curriculum. While these statements are generally correct, these explanations of the differences between the <lb/>surveys hardly convey any useful information in gaining an understanding of the comparative results of the <lb/>two surveys. Further, uninformed comparisons can be dangerous, particularly when somewhat emotionally <lb/>charged. Prais (2003) made a number of conjectures in a comparison of PISA and TIMSS results for the <lb/>United Kingdom. In a rejoinder to Prais&apos; article, Adams (2003) pointed out that Prais&apos; criticisms were <lb/>based on some misunderstanding of the two surveys. Furthermore, a comparison limited to the results of <lb/>one country is unlikely to have the power to reveal patterns of similarities and differences. As a result, this <lb/>working paper attempts to provide a comprehensive comparison of PISA and TIMSS, examining both <lb/>methodological similarities and differences, as well as similarities and differences in the results. Aspects in <lb/>relation to methodology include sampling, framework development and test construction. Aspects in <lb/>relation to results include comparisons and explanations of rank ordering of countries, observed gender <lb/>differences, as well as the impact of attitude on achievement. <lb/>4. <lb/>The major target audience of this report will be educational practitioners. The term &quot; educational <lb/>practitioners &quot; is widely defined here to include policy makers and researchers in the education field. The <lb/>report aims to help those involved in planning instructions and monitoring achievement in mathematics to <lb/>understand the differences and similarities between PISA and TIMSS, and to provide guidance on how to <lb/></body>

			<note place="footnote">1 . <lb/>This purpose was stated in the OECD&apos;s International Call for Tender for the development of thematic <lb/>reports on PISA 2003 (OECD/EXD/PCM/EDU(2003)56, p. 4). <lb/></note>

			<front>EDU/WKP(2010)5 <lb/></front>

			<page>8 <lb/></page>

			<body>relate and interpret the results from the two assessments. For example, PISA and TIMSS results can <lb/>identify relative strengths and weaknesses of students in the field of mathematics. This information, when <lb/>interpreted correctly, can in turn be used for an evaluation of current practices as well as future reforms in <lb/>curriculum and instruction. <lb/>Surveys for comparison <lb/>5. <lb/>This report uses results from the PISA 2003 mathematics assessment and TIMSS 2003 <lb/>population 2 mathematics assessment 2 as the basis for comparison. <lb/>6. <lb/>There are two reasons for the choice of these two surveys for comparison. First, they were both <lb/>conducted at around the same time 3 . The data collected represent a cross-sectional profile of students&apos; <lb/>mathematics achievement in 2003 in each participating country. On-going changes in educational reforms <lb/>within each country are not likely to account for the differences between the results of the two surveys, as <lb/>both surveys were conducted at about the same time. Second, although PISA assesses reading, science and <lb/>mathematics, the majority of testing time was devoted to mathematics in PISA 2003 so that the data <lb/>collected covered most mathematics content areas, as was typically the case in TIMSS studies. This makes <lb/>the results of the two surveys more comparable. <lb/>Similarities and differences <lb/>7. <lb/>While it is useful to identify differences between PISA and TIMSS, the identification of <lb/>similarities between the two surveys should also be valuable. First, where the findings from both surveys <lb/>agree, the results provide strong evidence for policy makers and researchers to take appropriate actions <lb/>based on the findings. For example, if gender differences in mathematics performance from both surveys <lb/>are consistent, then there is a clear message about the differential performance of girls and boys, despite <lb/>differences in grade and age in the two surveys. Second, the identification of the extent of similarities in <lb/>the survey methodologies could inform policy decisions regarding the best way to move forward so that <lb/>the two surveys complement, rather than duplicate, each other. For example, both surveys are currently <lb/>paper-and-pencil based tests with short tasks. It is possible that some extended performance tasks delivered <lb/>through computer-based testing could be included in one survey to tap into a different aspect of <lb/>mathematics performance, so that the two surveys can provide complementary information about all <lb/>aspects of mathematics literacy. Consequently, a useful comparison between the two surveys should go <lb/>beyond just looking for differences. This working paper provides a holistic comparison between the two <lb/>surveys, identifying both similarities and differences between the methodologies and the findings. <lb/>Focus of comparison <lb/>8. <lb/>More specifically, this report addresses four aspects of PISA and TIMSS: <lb/>• What findings are similar between the two surveys? Agreements between findings from the two <lb/>surveys will reinforce the underlying messages and provide some evidence of validity of the <lb/>results.<lb/>• What findings are different, or even contradictory, between the two surveys? What are possible <lb/>explanations for the differences?<lb/>2. <lb/>TIMSS Population 2 refers to the Grade 8 cohort. TIMSS Population 1 refers to the Grade 4 cohort. <lb/>3. <lb/>The PISA testing window was between March and August 2003 (OECD, 2005, p.46). In TIMSS, seven <lb/>Southern Hemisphere countries tested in October through December, 2002. Korea tested later in 2003. The <lb/>remaining countries tested mostly between April and June 2003 (IEA, 2003, p.18). <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>9 <lb/></page>

			<body>• Which issues are investigated by only one survey? What findings from the two surveys are <lb/>complementary?<lb/>• What lessons can we learn from a cross-comparison of the two surveys? How can we improve <lb/>each survey based on the findings of this report?<lb/>Background information about the two surveys <lb/>9. <lb/>PISA is conducted by the OECD while TIMSS is conducted by the IEA. To fully understand the <lb/>differences between PISA and TIMSS, it will be important to be familiar with the history and the <lb/> &quot; philosophies &quot; of IEA studies and OECD work. OECD, being a co-operative organisation between <lb/>governments, has policy-makers&apos; interests as the focus of its work. In contrast, IEA, formed as a united <lb/>body of research organisations, has the interests of researchers at the forefront of its studies. Although the <lb/>distinction between policy focus and research focus is a blurred one, as many research questions that drive <lb/>TIMSS are heavily influenced by policy considerations. Nevertheless, the different backgrounds of the two <lb/>organisations have resulted in setting different goals for the studies conducted. For example, in TIMSS it <lb/>was deemed important to link the survey results directly to instructional practices in the classrooms, while <lb/>in PISA, the measure of the outcome of schooling is deemed more important for governments in shaping <lb/>educational policies. The emphases in the main goals of each study in turn had an impact on population <lb/>definition and sampling procedures. For example, to examine instructional practices and relate these to <lb/>student achievement, one needs to sample classes. To sample classes, the population definition will need to <lb/>be grade based. In contrast, to compare outcomes of schooling, an age-based sample may place countries <lb/>on more equal footings for describing the preparedness of students for adult life. Consequently, a clear <lb/>understanding of the different objectives of each study is fundamental in subsequent analyses of the <lb/>comparisons of the surveys. <lb/>The aims of TIMSS and PISA <lb/>10. <lb/>In the introduction to the TIMSS 2003 International Mathematics Report (IEA, 2003), the aim of <lb/>the study is stated as follows: <lb/>The aim of TIMSS … is to improve the teaching and learning of mathematics and science by <lb/>providing data about students&apos; achievement in relation to different types of curricula, instructional <lb/>practices, and school environments. (p.13) <lb/>11. <lb/>The aim of TIMSS places teaching and learning at the forefront, with a special mention of linking <lb/>achievement to curricula and instructional practices. In contrast, in the OECD publication Learning for <lb/>Tomorrow&apos;s World – First Results from PISA 2003 (OECD, 2004), the aim of PISA is stated as follows: <lb/>PISA seeks to assess how well 15-year-olds are prepared for life&apos;s challenges. … focusing on young <lb/>people&apos;s ability to use their knowledge and skills to meet real-life challenges, rather than merely on <lb/>the extent to which they have mastered a specific school curriculum. (p.20) <lb/>12. <lb/>PISA&apos;s statement of aim indicates that the link between achievement and curricula is not <lb/>regarded as the main objective of the study. PISA adopts a &quot; literacy &quot; concept about the extent to which <lb/>students can apply knowledge and skills. An assessment of this literacy in various subject domains will <lb/>have direct policy relevance for governments. This is not to say that TIMSS does not produce useful <lb/>information for policy-makers, or that PISA results do not inform teaching and learning. Such a division <lb/>between policy focus and research focus is an oversimplification of the aims of the surveys. Many research <lb/>objectives in TIMSS were driven by policy considerations, and many policy objectives in PISA result in <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>10 <lb/></page>

			<body>research themes. However, it is the relative emphases of the two studies that are different. When a study is <lb/>designed for a main purpose, the results are usually not as readily useful for other purposes. <lb/>13. <lb/>It is not surprising that PISA has a policy orientation while TIMSS has a research orientation, <lb/>since the governing body of PISA consists of governmental departments, while institutional members of <lb/>IEA are research centres that may or may not be linked to the government of each country. Consequently, <lb/>the decision making processes in the two studies differ to some extent, since the participants of decision <lb/>making meetings are not the same group of people. However, there is some overlap of participants. Around <lb/>eleven countries have the same government department taking charge of both PISA and TIMSS in <lb/>participating in the decision-making processes at the international level, and about half of these countries <lb/>have the same person in charge of both projects 4 . From this point of view, PISA and TIMSS are not <lb/>entirely separate studies, since common experiences and problems in these two surveys have often been <lb/>cross-referenced when setting directions for each survey. <lb/>14. <lb/>Nevertheless, the total number of countries participating in each study (41 countries in PISA <lb/>2003, and 50 countries 5 in TIMSS 2003 Grade 8 cohort) is much larger than the number of countries taking <lb/>part in both studies (22 countries), and the cohort of countries that participated in each study has an impact <lb/>on the directions taken for the development of the assessment instruments. First, in both studies, there has <lb/>been an active involvement from participating countries in shaping the assessment instruments through <lb/>contributions and reviews of items. Second, the target difficulty level and cultural balance also need to <lb/>match the group of participating countries. Characteristics of items may be influenced by the background <lb/>of the participants of the surveys. For example, specific item contexts may be selected or avoided <lb/>depending on the level of socio-economic status of students in participating countries, since students&apos; <lb/>familiarity with the contexts can have some impact on the results. The items selected also need to work <lb/>well in all countries and languages. <lb/>Organisation of the report <lb/>15. <lb/>The content of this report is organised in six chapters. <lb/>16. <lb/>Chapter 2 compares survey methodologies used in the two surveys. The topics considered include <lb/>population definitions, sampling methods, test characteristics, scaling methods, field operations and <lb/>questionnaires administration. <lb/>17. <lb/>Chapter 3 provides a detailed comparison of the mathematics frameworks and test specifications <lb/>used in PISA and TIMSS. An attempt is made to align the two frameworks according to both the content <lb/>domains and cognitive domains. <lb/>18. <lb/>Chapter 4 examines the degree of similarities and differences between TIMSS and PISA <lb/>achievement results in terms of country mean scores. Various hypotheses are tested to explain the observed <lb/>differences in country performance in TIMSS and PISA. With the identification of a number of factors, <lb/>quantitative models are built to explain achievement differences. <lb/>19. <lb/>Chapter 5 focuses on comparisons of gender differences and students&apos; attitudes towards <lb/>mathematics, as reported in TIMSS and PISA. The choice of these student level variables was based on the <lb/>availability of published results provided by the two surveys. <lb/>4. <lb/>It is difficult to obtain these figures precisely, since there are often changes of personnel or changes of <lb/>contractors for running each study. <lb/>5 <lb/>Some of the participants in TIMSS are sub-regions of a country, for example, Basque country in Spain and <lb/>two provinces of Canada. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>11 <lb/></page>

			<body>20. <lb/>Chapter 6 summarises the findings, as well as draws attention to the implications of the findings <lb/>for PISA and TIMSS, and, in fact, for large-scale assessments more generally. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>12 <lb/></page>

			<body>CHAPTER 2 -SURVEY METHODOLOGIES <lb/>Introduction <lb/>21. <lb/>This chapter compares survey methodologies used in PISA and TIMSS. In particular, five aspects <lb/>of survey methodologies are compared: sampling, test characteristics, scaling methods, field operations and <lb/>test administration. On the whole, both PISA and TIMSS adopt similar survey methodologies typically <lb/>used for large-scale studies. Both surveys chose the methodologies to meet survey objectives while taking <lb/>account of the constraints. <lb/>22. <lb/>As international studies, both surveys attempt to provide information on mathematics <lb/>achievement at the national level for participating countries. Since it would be impractical to test every <lb/>student in each country, both PISA and TIMSS use sampling methodology to select a representative <lb/>sample from each country. The use of samples leads to the implementation of a set of statistical procedures <lb/>appropriate for drawing inferences from samples about the population. <lb/>23. <lb/>Both PISA and TIMSS have world experts in large-scale survey sampling methodology <lb/>providing clear directions for sampling within each country. The sampling methodologies used in both <lb/>studies are similar. <lb/>24. <lb/>In addition, both surveys use similar methodologies for estimating student performance and the <lb/>construction of a proficiency scale. These scaling methodologies are described in more detail below. There <lb/>is also some overlap in expertise in this scaling process, as some members of the technical advisory group <lb/>for PISA have also been involved in the scaling of TIMSS and the United States National Assessment of <lb/>Educational Progress (NAEP) 6 data. <lb/>25. <lb/>In terms of field operations, PISA and TIMSS have similar processes, with minor variations in <lb/>translation and verification procedures. As technical aspects of methodologies for international surveys <lb/>are described in detail in published reports, there is now a move towards forming standards for conducting <lb/>international, or large-scale, surveys. Consequently, it is not surprising that survey methodologies are <lb/>becoming more similar across different studies in broad terms, but there are variations at the level of <lb/>details. <lb/>26. <lb/>The following sections compare each aspect of survey methodologies between PISA and TIMSS <lb/>in more detail. <lb/>Population definition <lb/>27. <lb/>PISA and TIMSS have different population definitions. The target population of PISA is defined <lb/>as follows: <lb/>The desired base PISA target population in each country consisted of 15-year-old students attending <lb/>educational institutions located within the country, in grades 7 and higher. (p.46, OECD, 2005) <lb/>6. <lb/>TIMSS uses essentially the same scaling methodology as NAEP. <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>13 <lb/></page>

			<body>28. <lb/>Note that while PISA has an age-based population definition, there is a reference to grade so that <lb/>only students in grade 7 and above are included in the sample. <lb/>29. <lb/>TIMSS Grade 8 population definition for the 2003 survey is the following: <lb/>All students enrolled in the upper of the two adjacent grades that contain the largest proportion of 13-<lb/>year-olds at the time of testing. This grade level was intended to represent eight years of schooling, <lb/>counting from the first year of primary or elementary schooling, and was the eighth grade in most <lb/>countries. (p.110, IEA, 2004) <lb/>30. <lb/>The fact that TIMSS results are labelled as &quot; Grade 4 &quot; or &quot; Grade 8 &quot; has misled some to think that <lb/>Grade 8 students are selected from every country. The term &quot; Grade 8 &quot; refers to eight years of schooling, <lb/>rather than a grade labelled as Grade 8 in every country. Therefore, the TIMSS population definition aims <lb/>to control for the number of years of schooling. Note that while TIMSS has a grade-based sample, in fact, <lb/>there is a clear reference to age in selecting the appropriate grade for testing. That is, while the sample is <lb/>grade-based, the target population aims to capture 13 or 14-year-olds. This additional reference to age <lb/>provides some guidance to countries to select the correct grade. It helps to overcome some difficulties in <lb/>defining &quot; the first year of primary schooling &quot; , as there are variations across countries in formal and <lb/>informal education for very young children. In this sense, the desired population base of TIMSS is <lb/>somewhat age-based, but the operational population definition is grade-based. <lb/>Implications of the different definitions <lb/>31. <lb/>What are the implications of the differences in population definitions in PISA and TIMSS? In <lb/>broad terms, PISA examines the educational yield in the first 15 years of life of a child, and TIMSS <lb/>examines the educational yield of the first eight school grades in each country. <lb/>32. <lb/>So one might say that PISA asks the question: &quot; In each country, what has the education system <lb/>been able to do to raise the mathematics achievement of a child by the time he/she reaches 15 years of <lb/>age? &quot; In contrast, TIMSS asks the question: &quot; What has eight years of school grades achieved in raising the <lb/>mathematics standard of a child? &quot; In PISA, if Country X had lower mean achievement than Country Y, <lb/>one might conclude, in simplistic terms, that Country X had not been able provide as much effective <lb/>education for a child in the first 15 years of his/her life. In TIMSS, if Country X had lower mean <lb/>achievement than Country Y, one might conclude, in simplistic terms, that the first eight grades of schools <lb/>in Country X had not been as effective as the first eight grades of schools in Country Y. Of course there are <lb/>many other variables (such as socio-economic status of students or culture traditions that are not easily <lb/>amenable to policy manipulation) that need to be taken into account to make conclusions about the <lb/>effectiveness of any educational system, and the pictures are usually not as simplistic as the above <lb/>examples illustrate. See Box 2.4 for further information on student background variables selected for <lb/>providing insight into the making of effective education systems. <lb/>33. <lb/>The population definitions of PISA and TIMSS are consistent with the objectives of each survey. <lb/>If one wants to evaluate the effectiveness of the provision of an education system for each child when <lb/>he/she reaches the age at which he/she can leave school, then the PISA population definition will provide a <lb/>more comparable sample. On the other hand, if one wants to evaluate the effectiveness of instructional <lb/>practices in classrooms, then the TIMSS population definition will provide a more comparable sample as it <lb/>controls for the number of school grades a child attends. In some sense, PISA takes a look at a bigger <lb/>picture of education systems as a whole. In contrast, TIMSS focuses on specific issues of education, <lb/>namely, schools and classrooms. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>14 <lb/></page>

			<body>Comparison of age distributions <lb/>34. <lb/>The international average age of students in TIMSS &quot; Grade 8 &quot; assessment was 14.5, with country <lb/>mean age ranging from 13.7 (Scotland) to 15.5 (Ghana) (See Exhibit 2, IEA, 2003, pp. 20-22), while the <lb/>international average age of PISA 2003 students was 15.8 7 , with country mean age ranging from 15.7 to <lb/>15.9. Of course, since PISA samples are selected by age, there is little variation in student age across <lb/>countries. <lb/>35. <lb/>Within each country, there is also a spread of ages. As expected, in TIMSS, the variation of age <lb/>within each country is greater than the variation of age in PISA. The following provides a summary of the <lb/>comparison of age distributions between PISA and TIMSS. <lb/>36. <lb/>Students in PISA typically are aged between 15 years and 3 months and 16 years and 2 months, <lb/>within each country. That is, there is a one-year age span within each country&apos;s sample. The spread of <lb/>students&apos; age distribution is similar across all countries. <lb/>37. <lb/>In TIMSS, there is typically a two-year age span in each country&apos;s student sample. However, <lb/>there are differences across countries. For example, in England, the age difference between students is <lb/>generally less than one-year, as shown in the histogram in Figure 2.1. <lb/>Figure 2.1 <lb/>Age distribution of students in the TIMSS sample for England <lb/>38. <lb/>That is, both the TIMSS sample and PISA sample for England have similar spread of age <lb/>distribution, with a range of around one year. <lb/>39. <lb/>In contrast, for Hong Kong-China, the age distribution for the TIMSS sample has a large spread, <lb/>as shown in Figure 2.2. <lb/>7. <lb/>Computed across all countries in PISA 2003, with each country contributing equal weight. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>15 <lb/></page>

			<body>Figure 2.2 <lb/>Age distribution of students in the TIMSS sample for Hong Kong-China <lb/>40. <lb/>That is, the age of students in the TIMSS sample for Hong Kong-China covers around four years, <lb/>while the age differences of students in the Hong Kong-China PISA sample are within one year (but across <lb/>a number of grades). <lb/>41. <lb/>The United Kingdom and Hong Kong-China examples are two extreme cases. For most <lb/>countries, the age distribution of TIMSS samples covers around two years. <lb/>42. <lb/>The age distributions of sampled students may have an impact on student achievement. For <lb/>example, in TIMSS, different countries have different age cohorts due to country-specific regulations such <lb/>as age of entry into schools or policies on retention. The upper of the two adjacent grades that contain the <lb/>largest proportion of 13-year-olds could have more 14-year-olds than 13-year-olds, or have only 13-year-<lb/>olds. A country such as Norway, where children enter schools at a relatively younger age, may have <lb/>younger students at Grade 8, compared with countries where students enter schools at an older age. <lb/>43. <lb/>In contrast, the PISA sample could draw students from predominantly one grade level, or from <lb/>two or more grades, depending on the distribution of 15-year-olds across grades. The implications of <lb/>multiple grades versus single grade will need further investigation. The following shows some examples of <lb/>grade distributions in PISA. <lb/>Comparison of grade distributions <lb/>44. <lb/>Since TIMSS samples are grade-based, all students in each country&apos;s sample are in the same <lb/>grade. In contrast, for PISA, 15-year-olds may be in a number of different grades in each country. There <lb/>are considerable variations across countries in terms of the number of different grades covered in the PISA <lb/>sample. For example, in Iceland and Japan, all students in the PISA sample are from the same grade. In <lb/>Korea, the majority of students are from the same grade, as shown in Figure 2.3. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>16 <lb/></page>

			<body>Figure 2.3 <lb/>Grade distribution of PISA sample in Korea <lb/>45. <lb/>In some countries, the majority of the students in the PISA sample come from two grades, for <lb/>example, in Austria and the Czech Republic (see Figure 2.4, for an example). <lb/>Figure 2.4 <lb/>Grade distribution of PISA sample in Austria <lb/>46. <lb/>In some countries, there are many 15-year-olds who are in grades below the modal grade, such as <lb/>in Hong Kong-China and Portugal. <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>17 <lb/></page>

			<body>Figure 2.5 <lb/>Grade distribution of PISA sample in Hong Kong-China <lb/>47. <lb/>One might hypothesise that, if many 15-year-olds in a country are in grades well below the modal <lb/>grade, then an age-based sample will tend to lower the average performance of students compared to a <lb/>grade-based sample. On the other hand, in countries where there are many 15-year-olds who are in higher <lb/>grades than the modal grade, one might expect 15-year-olds to perform relatively better than just students <lb/>in a single grade, since school grade level reflects the number of years of schooling. In comparing country <lb/>performance in PISA and TIMSS, the grade distribution of the PISA sample in each country, and the age <lb/>distributions of the TIMSS samples, should be taken into account. These are discussed in more detail in <lb/>Chapter 4. <lb/>Sampling methods <lb/>48. <lb/>The sampling design for both PISA and TIMSS is a two-stage stratified sample for most <lb/>countries. The first stage of sampling is the selection of schools from a list of all schools satisfying the <lb/>target population definition for the survey. The selection of schools is carried out using probability-<lb/>proportional-to-size (PPS) method. That is, the probability that a particular school will be selected is <lb/>proportional to the number of eligible students in that school. When an equal number of students are then <lb/>selected from each of the chosen schools, the probability that a particular student will be chosen will be the <lb/>same for all students in the population. If PPS is not used, and each school has the same probability of <lb/>being chosen, then students from very small schools are more likely to be in the sample than students from <lb/>very large schools. Consequently, there will likely be larger standard errors and bias of estimates if PPS is <lb/>not used. <lb/>In PISA, the second stage of sampling is the selection of, typically, 35 students from each sampled school, <lb/>while, in TIMSS, the second stage of sampling is the selection of one intact class 8 . In PISA and TIMSS, <lb/>various standards have been set in relation to school level and student level exclusions, and both surveys <lb/>8. <lb/>In most countries, one class per school was selected. But some countries selected two classes per school. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>18 <lb/></page>

			<body>attempt to minimise the proportion of exclusions to ensure that the sample collected is representative of the <lb/>target population for each country. <lb/>49. <lb/>Both PISA and TIMSS require a minimum of 150 schools to be selected in each country. PISA <lb/>also recommends that the total number of sampled students is at least 5250, while TIMSS requires a <lb/>minimum of 4000 students to be selected in each country. <lb/>50. <lb/>The achieved degree of precision for estimates of mean student performance by country is not too <lb/>different in magnitude in PISA and TIMSS, with standard errors slightly smaller in PISA than in TIMSS <lb/>(see Table 9 in Chapter 4). For the 22 countries that participated in both PISA and TIMSS, the average <lb/>standard error for country means is 3.2 score points in PISA, and 3.3 score points in TIMSS, although the <lb/>score points are not on the same scale. If PISA scores are placed on the TIMSS scale (with the same <lb/>overall TIMSS standard deviation for the 22 countries), then the average standard error (over the 22 <lb/>countries) in PISA is 2.7 score points on the TIMSS scale, about 0.6 score points less than the average <lb/>standard error in TIMSS. <lb/>Test characteristics <lb/>Test length <lb/>51. <lb/>In PISA, each student took two hours of testing, with a break after one hour. In TIMSS, the <lb/>testing time for each student was 90 minutes and was divided into two sessions, with a break in between. <lb/>Test design <lb/>52. <lb/>To allow for coverage of all mathematics content areas in the assessment, but at the same time <lb/>not placing too much burden on individual students, both PISA and TIMSS utilise matrix sampling test <lb/>design (see Box 2.1). That is, test items are placed in a number of test booklets with linking items across <lb/>booklets, and each student takes only one test booklet. In this way, each student only answers a fraction of <lb/>the test items, out of a large number of items being tested in the whole assessment. <lb/>53. <lb/>In PISA, 13 test booklets were rotated among students, with each booklet containing items from <lb/>at least two subject domains from mathematics, reading, science and problem solving. In addition, every <lb/>booklet contained at least one mathematics item cluster. In TIMSS, 12 test booklets were rotated among <lb/>students, with each booklet containing both mathematics and science items. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>19 <lb/></page>

			<body>Box 2.1 Matrix sampling test design in PISA and TIMSS <lb/>Cluster rotation design used to form test booklets for PISA 2003 <lb/>Booklet <lb/>Cluster 1 <lb/>Cluster 2 <lb/>Cluster 3 <lb/>Cluster 4 <lb/>1 <lb/>M1 <lb/>M2 <lb/>M4 <lb/>R1 <lb/>2 <lb/>M2 <lb/>M3 <lb/>M5 <lb/>R2 <lb/>3 <lb/>M3 <lb/>M4 <lb/>M6 <lb/>PS1 <lb/>4 <lb/>M4 <lb/>M5 <lb/>M7 <lb/>PS2 <lb/>5 <lb/>M5 <lb/>M6 <lb/>S1 <lb/>M1 <lb/>6 <lb/>M6 <lb/>M7 <lb/>S2 <lb/>M2 <lb/>7 <lb/>M7 <lb/>S1 <lb/>R1 <lb/>M3 <lb/>8 <lb/>S1 <lb/>S2 <lb/>R2 <lb/>M4 <lb/>9 <lb/>S2 <lb/>R1 <lb/>PS1 <lb/>M5 <lb/>10 <lb/>R1 <lb/>R2 <lb/>PS2 <lb/>M6 <lb/>11 <lb/>R2 <lb/>PS1 <lb/>M1 <lb/>M7 <lb/>12 <lb/>PS1 <lb/>PS2 <lb/>M2 <lb/>S1 <lb/>13 <lb/>PS2 <lb/>M1 <lb/>M3 <lb/>S2 <lb/>Note: M denotes a mathematics item cluster, R a reading item cluster, S a science item cluster and PS a problem solving item cluster. <lb/>To enable linking between booklets, each of these item clusters appears in four of the test booklets in each of the four possible <lb/>positions (i.e. in Cluster 1, Cluster 2, Cluster 3 or Cluster 4). <lb/>TIMSS 2003 booklet design <lb/>Student <lb/>Booklet <lb/>Part I <lb/>Part II <lb/>Block 1 <lb/>Block 2 <lb/>Block 3 <lb/>Block 4 <lb/>Block 5 <lb/>Block 6 <lb/>1 <lb/>M1 <lb/>M2 <lb/>S6 <lb/>S7 <lb/>M5 <lb/>M7 <lb/>2 <lb/>M2 <lb/>M3 <lb/>S5 <lb/>S8 <lb/>M6 <lb/>M8 <lb/>3 <lb/>M3 <lb/>M4 <lb/>S4 <lb/>S9 <lb/>M13 <lb/>M11 <lb/>4 <lb/>M4 <lb/>M5 <lb/>S3 <lb/>S10 <lb/>M14 <lb/>M12 <lb/>5 <lb/>M5 <lb/>M6 <lb/>S2 <lb/>S11 <lb/>M9 <lb/>M13 <lb/>6 <lb/>M6 <lb/>M1 <lb/>S1 <lb/>S12 <lb/>M10 <lb/>M14 <lb/>7 <lb/>S1 <lb/>S2 <lb/>M6 <lb/>M7 <lb/>S5 <lb/>S7 <lb/>8 <lb/>S2 <lb/>S3 <lb/>M5 <lb/>M8 <lb/>S6 <lb/>S8 <lb/>9 <lb/>S3 <lb/>S4 <lb/>M4 <lb/>M9 <lb/>S13 <lb/>S11 <lb/>10 <lb/>S4 <lb/>S5 <lb/>M3 <lb/>M10 <lb/>S14 <lb/>S12 <lb/>11 <lb/>S5 <lb/>S6 <lb/>M2 <lb/>M11 <lb/>S9 <lb/>S13 <lb/>12 <lb/>S6 <lb/>S1 <lb/>M1 <lb/>M12 <lb/>S10 <lb/>S14 <lb/>Note: M denotes mathematics and S science. To enable linking between booklets, all blocks will appear in at least two of the 12 <lb/>booklets. Trend items are placed in Part I, and new items are placed in Part II, except for M5 and M6 which appear in both Part I and <lb/>Part II. Calculators are not allowed for Part I items, but they are allowed for Part II items. <lb/>Amount of assessment material <lb/>54. <lb/>Both PISA and TIMSS developed approximately 210 minutes of mathematics assessment <lb/>material, and these are placed in the test booklets with some duplication across the booklets 9 . Interestingly, <lb/>in PISA, the 210 minutes of test material were made up of 85 test items (with 94 score points in total), <lb/>while, in TIMSS, the 210 minutes of test material were made up of 194 items (with 215 score points in <lb/>9. <lb/>The final 2003 tests contain 210 minutes of test material, although many more items were developed and <lb/>field tested, from which 210 minutes of test material were selected. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>20 <lb/></page>

			<body>total). So, on average, each PISA item should take about 2.5 minutes to complete, while a TIMSS item <lb/>should take about 1 minute to complete. That is, on average, it is expected that each PISA item would take <lb/>more than twice the time to complete than a TIMSS item. <lb/>55. <lb/>In PISA, each student took between 30 to 90 minutes of mathematics assessment (with an <lb/>average of 65 minutes), while, in TIMSS, half of the students took 30 minutes of mathematics assessment, <lb/>and the other half took 60 minutes of mathematics assessment (with an average of 45 minutes). While, on <lb/>average, PISA students were given more time to answer mathematics items, the average number of items <lb/>administered to PISA students was 26 (total of 29 score points), as compared to an average of 42 items <lb/>(total of 47 score points) administered to each TIMSS student. <lb/>56. <lb/>The reported test reliability for TIMSS is 0.89, and 0.85 for PISA. This is not surprising, as test <lb/>reliability is closely related to the number of items (or total score points) administered to each student. It <lb/>appears that PISA developed more extended mathematics tasks in the assessment, but was not able to turn <lb/>the extended assessment into more score points. In some sense, PISA&apos;s attempt to build longer tasks to <lb/>reflect real-life contexts is at some expense of test reliability. <lb/>57. <lb/>A more detailed comparison of item types in PISA and TIMSS is given in Chapter 3. <lb/>Scaling methodology <lb/>58. <lb/>Both PISA and TIMSS use item response theory (IRT) to model student responses to test items. <lb/>Item response theory is particularly useful for the matrix sampling test design in PISA and TIMSS to <lb/> &quot; equate &quot; student scores when students took different test booklets, since, in these cases, raw scores on the <lb/>tests were not directly comparable as different sets of test items were administered to each student. See <lb/>Box 2.2 for a brief description of IRT. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>21 <lb/></page>

			<body>Box 2.2 Overview of Item Response Modelling <lb/>Principles of Item Response Theory (IRT) <lb/>Item response theory pertains to the use of mathematical functions to model the probability of success on an item <lb/>as a function of the characteristics of an item (e.g., difficulty) and characteristics of a person (e.g., ability). Typically, <lb/>item difficulty parameter and ability parameter are defined on the same measurement scale, so that the relationship <lb/>between the ability of a person and the difficulty of an item is well defined through a mathematical function. In the case <lb/>of the one-parameter item response model, the probability of success on an item is given by <lb/>( <lb/>) <lb/>( <lb/>) <lb/>δ <lb/>θ <lb/>δ <lb/>θ <lb/>− <lb/>+ <lb/>− <lb/>= <lb/>exp <lb/>1 <lb/>exp <lb/>success <lb/>of <lb/>y <lb/>probabilit <lb/>whereθ is the person&apos;s ability on the measurement scale, and δ is the item difficulty on the same measurement <lb/>scale. When a person&apos;s ability is equal to the item difficulty, the probability of success is 0.5. Consequently, if a person <lb/>is administered a number of test items with known item difficulty parameters, one can make an estimate of the <lb/>person&apos;s ability based on the observed patterns of successes and failures on the items. The advantage of this <lb/>approach is that the estimate of a person&apos;s ability is invariant (within measurement error) of the particular set of items <lb/>administered. This property of the one-parameter item response model is particularly useful when there is a rotated <lb/>booklet test design where students take different sets of the items such as the booklets in PISA and TIMSS. <lb/>In addition, since person ability and item difficulty are defined on the same measurment scale, one can make <lb/>statements about a student (located at a point on the scale) with respect to his/her likelihood of being successful on <lb/>various items which are also located on the scale according to the item difficulty parameter. In this way, one can <lb/>provide descriptors of skills and knowledge to illustrate typical capcacities of students located at various poinsts on the <lb/>measurement scale. <lb/>There are also many different item response models, where different mathematical functions are used to describe <lb/>the probability of success. In particular, the two-parameter and three-parameter item response models are also <lb/>commonly used. The two-parameter item response model has an additional parameter describing the discrimination <lb/>power of each item. For example, opend-ended items typically provide more discrimination power than multiple-choice <lb/>items in separating students on the measurement scale (Routitsky &amp; Turner, 2003). The three-parameter model has an <lb/>additional &quot; guessing &quot; parameter for multiple-choice items. <lb/>In general, the choice of a particular item response model is often influenced by different schools of thought. All <lb/>item response models have theoretical and practical advantages and drawbacks. A detailed discussion on the <lb/>differences between item response models is beyond the scope of this report. Some references on item response <lb/>theory include Embretson and Reise, 2000, and van der Linden and Hambleton, 1997. <lb/>59. <lb/>There is, however, a difference in the item response models used in PISA and TIMSS. In PISA, <lb/>the one-parameter item response model was used (OECD, 2005), while in TIMSS, the three-parameter <lb/>model was used (IEA, 2004). There appears to be no clearly documented findings comparing the one-<lb/>parameter and three-parameter item response models. In TIMSS 1995, student responses were modelled <lb/>using both the one-parameter model and the three-parameter model, but IEA did not publish any direct <lb/>comparisons of the two scaling methods <lb/>60. <lb/>From a theoretical point of view, the three-parameter item response model takes into account the <lb/>discrimination power of individual items, that is, to what extent the item can separate poor ability students <lb/>from high ability students, as well as guessing factors for multiple-choice items. Consequently, if test items <lb/>are greatly different in terms of discrimination power, it is possible that the two scaling methods could <lb/>produce a different &quot; spread &quot; of the student ability distributions. The ranking of the countries is likely to be <lb/>unchanged (up to the accuracy of the proficiency estimates), provided that the items do not exhibit large <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>22 <lb/></page>

			<body>differential item functioning (DIF) across countries. While DIF was checked after the field trial in PISA, <lb/>this is an issue that still remains to be investigated. <lb/>61. <lb/>Both PISA and TIMSS use plausible values methodology (Mislevy, 1991; Mislevy, et al, 1992) <lb/>for estimating student achievement distributions, as well as replication methods for computing standard <lb/>errors of estimates. TIMSS uses the jacknife replication method, and PISA uses the balanced repeated <lb/>replication method. Both methods use the same approach in estimating standard errors for complex <lb/>samples. For more detail, consult the technical reports of PISA and TIMSS. See Box 2.3 for a brief <lb/>description of the Plausible Values methodology. <lb/>Box 2.3 Plausible Values <lb/>There are at least two different approaches to estimate student achievement distributions. The first is an indirect <lb/>approach where an estimate of each student&apos;s ability is first made based on the student&apos;s item responses, and these <lb/>ability estimates are then aggregated to form population characteristics. The second approach is a direct estimation <lb/>method where a parametric model for the ability distribution is assumed (for example, as a normal distribution with <lb/>mean µ and variance σ <lb/>2 ). The parameters of the distribution are then estimated directly using student item responses. <lb/>The plausible values methodology is a direct estimation method for population characteristics. Plausible values can be <lb/>regarded as computational tools for building the population ability distribution. This direct estimation method <lb/>overcomes some of the problems encountered in the indirect approach where inaccuracies in individual student ability <lb/>estimates cause biases in population estimates. Readers can refer to Wu (2005b) for a more detailed explanation on <lb/>plausible values. <lb/>Field operation procedures <lb/>62. <lb/>Field operation procedures in PISA and TIMSS are very similar, with provisions for translation <lb/>verification, marker training and marker reliability studies, national centre monitors, school visits, and a <lb/>whole suite of quality control procedures to ensure the standards of survey operations across all <lb/>participating countries. It can be said both PISA and TIMSS adopt similar field operation procedures with <lb/>only minor variations. For example, in PISA, source documents are prepared in both English and French, <lb/>and a double translation is carried out using English and French source documents separately. In TIMSS, <lb/>the source language is in English, and an independent double translation is carried out. Both PISA and <lb/>TIMSS have independent translation verification processes in place, where international translation <lb/>companies verify the translated instruments. The respective study centres then review and check for the <lb/>reconciliation of the various translations. <lb/>63. <lb/>Test administration procedures are also very similar between the two studies, with some minor <lb/>differences such as lengths of testing time. However, policies on calculator use are somewhat different <lb/>between the two surveys. Since the use of calculators may have an impact on mathematics achievement, <lb/>the following section examines this issue more closely. <lb/>Calculator use <lb/>64. <lb/>PISA and TIMSS have different rulings on calculator use. PISA adopts an open position on the <lb/>use of calculators, as the following shows: <lb/>National centres decided whether calculators should be provided for their students on the basis of <lb/>standard national practice. No items in the pool required a calculator, but some items involved <lb/>solution steps for which the use of a calculator could facilitate computation. In developing the <lb/>mathematics items, test developers were particularly mindful to ensure that the items were as <lb/>calculator-neutral as possible. (p.16, OECD, 2005) <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>23 <lb/></page>

			<body>65. <lb/>TIMSS did not allow the use of calculators in 1995 and 1999. However, in 2003, for Grade 8, <lb/>calculators were allowed for some items. For Part I of the TIMSS test booklets, calculators were not <lb/>allowed. For Part II, calculators were allowed. As for PISA, &quot; TIMSS mathematics items were designed so <lb/>that they could be answered readily without the use of a calculator (p.374, IEA, 2003) &quot; . <lb/>66. <lb/>The differences between PISA and TIMSS in the policy on calculator use reflect differences in <lb/>the nature of the two assessments. <lb/>The PISA assessment focuses on problem situations that arise from the real world, and the use of <lb/>calculators is very much a part of everyday life (whether at work or at home). However, it should be <lb/>stressed that intensive computation is not a key focus of the PISA test, and there will not be purely <lb/>computational items that depend solely on the use of the calculator. (p.365, OECD, 2005) <lb/>67. <lb/>In contrast, TIMSS clearly regards computation skills without the use of calculators as one <lb/>important strand of mathematics, as reflected in the inclusion of mathematics items where calculators were <lb/>not allowed. For TIMSS 2003 Grade 4 assessment, no calculators were allowed. <lb/>68. <lb/>The impact of calculator use on achievement results is examined in Chapter 4. <lb/>Questionnaires <lb/>69. <lb/>Apart from the mathematics assessment instruments, contextual information about students and <lb/>schools is also collected in both PISA and TIMSS. This information is not only useful in its own right, it <lb/>also provides analytic power for gaining an in-depth understanding of achievement results. PISA <lb/>administers a student questionnaire and a school questionnaire, while TIMSS administers four <lb/>questionnaires: a student questionnaire, a teacher questionnaire, a school questionnaire and a curriculum <lb/>questionnaire. Since the sampling design of PISA does not involve the selection of intact classes, it is <lb/>difficult to administer a teacher questionnaire. As students come from different classes in each school, it <lb/>becomes difficult to identify all the teachers of sampled students and link students to teachers. <lb/>Consequently, in PISA, limited amount of teacher information is collected through the student and <lb/>school questionnaires. In contrast, TIMSS collects extensive teacher information including teacher <lb/>background, school and classroom climate, instructional approaches and implemented curriculum. In <lb/>addition, TIMSS administers a Curriculum questionnaire seeking information on the process of curriculum <lb/>development in each country, as well as the mathematics topics included in each country&apos;s curriculum. <lb/>Box 2.4 How PISA and TIMSS collected information from students in the 2003 surveys <lb/>Student questionnaire in TIMSS at Grade 8 <lb/>Students took 30 minutes to answer 23 questions in total. There were seven main sections: <lb/>About you -questions about the students and their family, including date of birth, sex, number of books at <lb/>home, home possessions (16 items, of which 12 are country specific), language spoken at home, <lb/>educational level of parents and their own educational expectations. <lb/>Mathematics in school – students&apos; attitudes towards learning mathematics, how frequently students are <lb/>taught or learn in different suggested ways in mathematics lessons (14 different ways). <lb/>Science in school – students&apos; attitudes towards learning science, how frequently students are taught or learn <lb/>in different suggested ways in science lessons (14 different ways). <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>24 <lb/></page>

			<note place="footnote">Computers – where students use computers and how frequently they complete 4 suggested educational <lb/>tasks. <lb/>Your school – students&apos; attitudes towards school and teachers (4 statements) and school climate and sense <lb/>of belonging (5 statements). <lb/>Things you do outside of school – how frequently students spend time on 9 selected activities before or after <lb/>school (including leisure pursuits and both school and paid work), frequency of extra lessons or tutoring and <lb/>homework in mathematics and science. <lb/>More about you – how many people live at the students&apos; home and whether or not the students and their <lb/>parents or guardians were born in the country of assessment and if applicable at what age the students <lb/>arrived in the country of assessment. <lb/>PISA student questionnaires <lb/>PISA administers a main student questionnaire, as well as providing countries the option of administering <lb/>additional shorter questionnaires to collect information such as students&apos; use of computers. There were three possible <lb/>questionnaires for students to complete in 2003. All participating students took about 30 minutes to answer 38 <lb/>questions in the main student questionnaire. There were six main sections: <lb/>About you – Grade at school, type of educational programme, date of birth, sex <lb/>You and your family – who lives at home with the students, parents&apos; employment status, occupation <lb/>(including job title and tasks) and education (level completed and qualifications), country of birth of both <lb/>student and parents, and, if applicable, at what age the student arrived in the country of assessment; <lb/>language most often spoken at home; home possessions (16 items, including a computer, of which 3 are <lb/>country specific); number of 5 selected home possessions; number of books at home. <lb/>Your education – pre-primary school attendance, age student started primary school, grade repetition if <lb/>applicable, level of education student expects to complete, attitudes towards school. <lb/>Your school – reasons student attends the school, views on teachers in the school, sense of belonging at <lb/>school, punctuality, number of hours spent on homework, extra classes or with tutors outside school. <lb/>Learning mathematics – views on mathematics learning (8 statements), confidence in mathematics (8 <lb/>tasks), studying mathematics (10 statements), time spent on learning mathematics, different ways of <lb/>studying mathematics (14 statements). <lb/>Your mathematics classes – number and length in minutes of mathematics classes per week, number of <lb/>students in the mathematics class, cooperative and competitive ways of learning mathematics (10 <lb/>statements), mathematics classroom environment, including disciplinary climate and teacher support (11 <lb/>statements). <lb/>In addition, students in 21 countries took five minutes to complete eight questions in an optional questionnaire on <lb/>Educational career. <lb/>Educational career -Absence of at least two months and/or change of school in primary (ISCED 1) or lower <lb/>secondary (ISCED 2) school; change of study programme in current grade; type of mathematics class; <lb/>mathematics mark in last school report and whether it was above or below the pass mark; and expected job <lb/>at the age of 30. <lb/>Students in 32 countries took five minutes to complete nine questions in an optional questionnaire on Information <lb/>and Communication Technology (ICT). <lb/>ICT -Availability of computers at home, school and elsewhere and how frequently students use these; the <lb/>number of years students have been using a computer; how frequently students use computers for specific <lb/>tasks (12 tasks); students&apos; confidence in completing different tasks on a computer (23 tasks); students&apos; <lb/>attitudes towards using computers; and how students learned to use computers and the Internet. <lb/></note>

			<front>EDU/WKP(2010)5 <lb/></front>

			<page>25 <lb/></page>

			<body>70. <lb/>The extent to which student, teacher and school background information is collected has an <lb/>impact on the analyses that can be carried out. Box 2.3 presents an overview of the questions that students <lb/>answered in PISA and TIMSS in 2003. Both PISA and TIMSS examine students&apos; &quot; self-confidence &quot; and <lb/> &quot; interest and motivation &quot; in mathematics. A comparison of these results is given in Chapter 5. PISA also <lb/>collects information on students&apos; learning strategies and how learner characteristics influence mathematics <lb/>performance. TIMSS international report has a brief presentation of students&apos; socio-economic status (SES), <lb/>while PISA devotes a large section of the report to the analysis of SES in relation to achievement. TIMSS <lb/>presents a detailed report on the profiles of teachers, including cross-country comparisons of teacher <lb/>qualifications, gender, age, experience, and professional activities and support for teachers. <lb/>71. <lb/>Both PISA and TIMSS report on school contexts for learning, including student-teacher <lb/>relationships, school resources, and classroom climate. TIMSS further presents details of the mathematics <lb/>classroom in terms of resources used in mathematics classes, content taught, and assessment methods. The <lb/>main difference between PISA and TIMSS in relation to contextual information is that classroom level <lb/>information is collected from teachers directly in TIMSS, while, in PISA, information is aggregated within <lb/>each school from students&apos; responses to questions about the classroom environment. <lb/>Summary <lb/>72. <lb/>The following is a summary of similarities and differences between PISA and TIMSS Grade 8 <lb/>discussed in this Chapter. <lb/>Aspects of <lb/>Survey <lb/>Specific Point <lb/>Similarities <lb/>Differences <lb/>Sampling <lb/>Population versus <lb/>sample <lb/>Both surveys are sample-based <lb/>Population <lb/>definition <lb/>PISA is age-based. TIMSS is <lb/>grade-based <lb/>Age distribution <lb/>Typically, there is a two-year <lb/>age span in each country&apos;s <lb/>sample of students in TIMSS, <lb/>and one-year age span in <lb/>PISA. <lb/>Grade distribution <lb/>Typically, there are two grades <lb/>involved in each country in the <lb/>PISA sample, and one grade in <lb/>TIMSS. <lb/>Sampling method Both surveys use a two-stage <lb/>sampling method, where schools <lb/>are <lb/>first <lb/>selected <lb/>using <lb/>probability proportional to size <lb/>method. <lb/>In TIMSS, the second stage of <lb/>sampling selects intact classes. <lb/>In PISA, the second stage of <lb/>sampling selects students at <lb/>random within each school. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>26 <lb/></page>

			<body>Aspects of <lb/>Survey <lb/>Specific Point <lb/>Similarities <lb/>Differences <lb/>Sample size <lb/>Both PISA and TIMSS require a <lb/>minimum of 150 schools to be <lb/>selected. <lb/>PISA recommends that the total <lb/>number of sampled students is at <lb/>least 5250, while TIMSS <lb/>requires a minimum of 4000 <lb/>students to be selected in each <lb/>country. <lb/>Test <lb/>Characteristics <lb/>Test length <lb/>73. <lb/>The testing time is <lb/>two hours in PISA and 90 <lb/>minutes in TIMSS. <lb/>Test design <lb/>Both surveys use rotated test <lb/>booklet design. PISA uses 13 <lb/>booklets. TIMSS uses 12 <lb/>booklets. <lb/>Amount <lb/>of <lb/>assessment <lb/>material <lb/>Both <lb/>PISA <lb/>and <lb/>TIMSS <lb/>developed approximately 210 <lb/>minutes <lb/>of <lb/>mathematics <lb/>assessment material <lb/>There are 94 total score points <lb/>in PISA, and 215 total score <lb/>points in TIMSS. <lb/>Scaling <lb/>Methodologies <lb/>Item <lb/>response <lb/>model <lb/>Both surveys use item response <lb/>modelling and plausible values <lb/>methodologies for estimating <lb/>student ability distributions <lb/>PISA uses the one-parameter <lb/>item response model. TIMSS <lb/>used the three-parameter item <lb/>response model. <lb/>Field <lb/>Operations <lb/>Translation <lb/>Both PISA and TIMSS have <lb/>translation verification process <lb/>in place. <lb/>In PISA, source documents are <lb/>prepared in both English and <lb/>French, <lb/>and <lb/>a <lb/>double <lb/>translation is carried out using <lb/>English and French source <lb/>documents <lb/>separately. <lb/>In <lb/>TIMSS, the source language is <lb/>in English, and an independent <lb/>double translation is carried <lb/>out. <lb/>Quality monitor <lb/>Both PISA and TIMSS have <lb/>similar procedures for marker <lb/>training and marker reliability <lb/>studies, national centre monitors <lb/>and school visits. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>27 <lb/></page>

			<body>Aspects of <lb/>Survey <lb/>Specific Point <lb/>Similarities <lb/>Differences <lb/>Test <lb/>administration <lb/>Calculator use <lb/>In PISA, calculators are <lb/>allowed. In TIMSS, calculators <lb/>are allowed for only Part II of <lb/>the test. <lb/>More students have access to <lb/>calculators in PISA than in <lb/>TIMSS. <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>28 <lb/></page>

			<body>CHAPTER 3 -COMPARISON OF PISA AND TIMSS MATHEMATICS FRAMEWORKS AND <lb/>ITEM FEATURES <lb/>Approaches to the development of the mathematics assessment frameworks <lb/>74. <lb/>PISA and TIMSS adopted different approaches to the development of the assessment <lb/>frameworks. Each survey developed the assessment framework to meet their objectives which are <lb/>somewhat different. In PISA, the aim is to assess the extent to which education systems have prepared 15-<lb/>year-olds &quot; to play constructive roles as citizens in society &quot; (p.24, OECD, 2003), so the assessment focuses <lb/>on &quot; what are the skills citizens require to play constructive roles in society? &quot; . In TIMSS, the assessment is <lb/>to improve teaching and learning of mathematics, so the assessment provides information about student <lb/>achievement levels in relation to what students have learned in schools. This difference in the orientation <lb/>of the purposes of the two assessments led to the different approaches to the development of the <lb/>frameworks. A review of each framework is given below, followed by a comparison between the two <lb/>frameworks. <lb/>TIMSS mathematics assessment framework <lb/>75. <lb/>The overall design of TIMSS evolves around the TIMSS Curriculum Model where three levels of <lb/>curriculum, the intended curriculum, the implemented curriculum and the attained curriculum, form the <lb/>major organising principle of the TIMSS study (p.3. IEA, 2003b). Questionnaires designed for students, <lb/>teachers and school principals aim to capture the first two levels of curriculum structure, namely, the <lb/>intended curriculum and the implemented curriculum, while the assessment attempts to capture the attained <lb/>curriculum. TIMSS stresses that the usefulness of the TIMSS results to policy makers &quot; depends on <lb/>achievement measures being based, as closely as possible, on what students in their systems have actually <lb/>been taught &quot; (p.5. IEA, 2003b). <lb/>76. <lb/>To ensure that the test contents are aligned with what students were taught in the participating <lb/>countries, a survey was conducted to collect information on the curricula in participating countries. <lb/>Mathematics topics that were regarded as important in a significant number of countries were included in <lb/>the framework. However, TIMSS stresses that &quot; the frameworks do not consist solely of content and <lb/>behaviours included in the curricula of all participating countries &quot; (p.5., IEA, 2003b). The following six <lb/>factors underlie the principles of the inclusion of mathematics content domains in the assessment <lb/>framework (p.5, IEA, 2003b): <lb/>• Inclusion of the content in the curricula of a significant number of participating countries;<lb/>• Alignment of the content domains with the reporting categories of TIMSS 1995 and TIMSS <lb/>1999;<lb/>• The likely importance of the content to future developments in mathematics and science <lb/>education;<lb/>• Appropriateness for the populations of students being assessed;<lb/>• Suitability for being assessed in a large-scale international study;<lb/>• Contribution to overall test balance and coverage of content and cognitive domains.<lb/>EDU/WKP(2010)5 <lb/></body>

			<page>29 <lb/></page>

			<body>77. <lb/>Of the six factors listed above, the third one does not quite fit in with the principle that the <lb/>achievement measures reflect what students have actually been taught, since there is an implication that, if <lb/>a topic is deemed important for future developments in mathematics, then it may be included whether or <lb/>not students have been taught the topic. From this point of view, the TIMSS framework is not completely <lb/>driven by national curricula. The framework also seeks to set some directions for future directions in <lb/>mathematics education. <lb/>78. <lb/>There are two organising dimensions underlying TIMSS 2003 framework: Mathematics content <lb/>domains and mathematics cognitive domains. These are discussed separately below. <lb/>TIMSS mathematics content domains <lb/>79. <lb/>Box 3.1 lists the five mathematics content domains in the TIMSS framework. Number, algebra, <lb/>measurement, geometry and data are familiar labels to mathematics educators, as they mirror closely the <lb/>content domains that are often found in the mathematics curricula of most countries. For more detail, refer <lb/>to the TIMSS 2003 Frameworks document (IEA, 2003b). <lb/>Box 3.1 The TIMSS mathematics content domains <lb/>There are five main content domains in the TIMSS 2003 mathematics assessment: <lb/>Number <lb/>Whole numbers <lb/>Fractions and decimals <lb/>Integers <lb/>Ratio, proportion and percent <lb/>Algebra <lb/>Patterns <lb/>Algebraic expressions <lb/>Equations and formulas <lb/>Relationships <lb/>Measurement <lb/>Attributes and units <lb/>Tools, techniques and formula <lb/>Geometry <lb/>Lines and angles <lb/>Two-and three-dimensional shapes <lb/>Congruence and similarity <lb/>Locations and spatial relationships <lb/>Symmetry and transformations <lb/>Data <lb/>Data collection and organisation <lb/>Data representation <lb/>Data interpretation <lb/>Uncertainty and probability <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>30 <lb/></page>

			<body>80. <lb/>The TIMSS framework document includes a list of topics covered by each content domain (see <lb/>Box 3.1), and, within each topic, a set of assessment outcomes to illustrate the specific tasks that students <lb/>will typically be assessed on. <lb/>81. <lb/>For example, Box 3.1 lists four topics for the content area, number: whole numbers, fractions and <lb/>decimals, integers, and ratio, proportion, and percent. Within the topic area of ratio, proportion, and <lb/>percent, the assessment outcomes (topic bullets) are: <lb/>• Identify and find equivalent ratios.<lb/>• Divide a quantity in a given ratio.<lb/>• Convert percents to fractions or decimals, and vice versa.<lb/>• Solve problems involving percents.<lb/>• Solve problems involving proportions.<lb/>82. <lb/>The TIMSS framework appears very comprehensive in its lists of the main topics and assessment <lb/>outcomes within each content area. A check of the actual items in the TIMSS 2003 tests showed that, out <lb/>of a total of 19 topics, only one topic (Data collection and organisation in the data content domain) was not <lb/>assessed in the TIMSS 2003 tests. Out of a total of 87 assessment outcomes (topic bullets), 67 were <lb/>covered by items in the actual tests 10 . However, some topic bullets were assessed by numerous items, while <lb/>others were assessed by only one item. The proportions of items in different content domains are given in <lb/>Table 3.1. <lb/>Table 3.1 Number and proportions of items in TIMSS by content domain <lb/>No. of <lb/>Items in <lb/>TIMSS <lb/>2003 <lb/>tests <lb/>Proportion <lb/>of items <lb/>TIMSS <lb/>Target <lb/>proportion <lb/>of items <lb/>International <lb/>average of <lb/>% of time <lb/>taught in <lb/>schools <lb/>Number <lb/>57 <lb/>30% <lb/>30% <lb/>21 % <lb/>Algebra <lb/>47 <lb/>24% <lb/>25% <lb/>27% <lb/>Measurement <lb/>31 <lb/>16% <lb/>15% <lb/>10% <lb/>Geometry <lb/>31 <lb/>16% <lb/>15% <lb/>26% <lb/>Data <lb/>28 <lb/>14% <lb/>15% <lb/>10% <lb/>Total <lb/>194 <lb/>100% <lb/>100% <lb/>94% <lb/>1 <lb/>1. <lb/>The total does not equal 100%, as 6% of the content domains reported by teachers are not covered by the five areas listed in <lb/>the table. <lb/>83. <lb/>The last column in Table 3.1 shows the international average of the percentage of time in <lb/>mathematics class devoted to each TIMSS content area during the school year, as reported by teachers 11 . <lb/>There is some discrepancy between the proportions of items in TIMSS tests and the average percentages <lb/>of time the content domains are taught across all TIMSS participating countries. The TIMSS mathematics <lb/>framework does not use content coverage as the sole criterion for determining the relative weights of the <lb/>10 . <lb/>We obtained slightly different figures depending on whether we used the item list provided by the <lb/>Australian TIMSS National Research Coordinator, or the item list in the released data set. <lb/>11 . <lb/>Figures are obtained from Exhibit 7.4 of the TIMSS 2003 International Mathematics Report (IEA, 2003). <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>31 <lb/></page>

			<body>content domains. Rather, the TIMSS framework &quot; represents a consensus among the countries participating <lb/>in TIMSS 2003 about the mathematics students at these grades should be expected to have learned. &quot; (IEA, <lb/>2003, p.180) <lb/>TIMSS mathematics cognitive domains <lb/>84. <lb/>TIMSS mathematics cognitive domains relate to the types of cognitive skills required in doing <lb/>mathematics, generic across all mathematics content domains. To achieve a balanced test, it is desirable to <lb/>ensure that each type of skills and abilities is covered by a sufficient number of items. There are four <lb/>cognitive domains in TIMSS 2003: <lb/>• Knowing facts and procedures<lb/>• Using concepts<lb/>• Solving routine problems<lb/>• Reasoning<lb/>85. <lb/>These four cognitive domains are listed in order of the complexity of the tasks, from <lb/>straightforward problems to complex tasks. However, the TIMSS framework stresses that cognitive <lb/>complexity should not be confused with item difficulty, in that there is a range of item difficulties <lb/>associated with each cognitive domain (IEA, 2003b, p.25. That is, within each cognitive domain, there are <lb/>easy items as well as difficult items. As these labels of cognitive domains are not necessarily familiar to <lb/>the reader, a brief description of each cognitive domain is given below. <lb/>Knowing facts and procedures <lb/>86. <lb/>This cognitive domain covers basic language of mathematics and essential mathematical facts <lb/>and properties, as well as the use of mathematics for solving routine problems typically encountered in <lb/>everyday life. There are four categories of skills covered by this cognitive domain: <lb/>• Recall – e.g., knowing number facts, mathematical conventions/notations.<lb/>• Recognise/identify – e.g., recognising different representations of the number system.<lb/>• Compute – e.g., carrying out arithmetic computation, expanding algebraic expressions.<lb/>• Use tools – e.g., reading scales, using straightedge and compass.<lb/>Using concepts <lb/>87. <lb/>This cognitive domain is about the ability to make connections of knowledge, judge the validity <lb/>of mathematical statements and create mathematical representations. There are five categories of skills <lb/>under this cognitive domain: <lb/>• Know – e.g., knowing concepts such as inclusion and exclusion, generality, mathematical <lb/>relationships.<lb/>• Classify – e.g., grouping objects, shapes, numbers according to common properties.<lb/>• Represent – e.g., presenting information using tables, diagrams, graphs; moving between <lb/>equivalent representations of mathematical relationships.<lb/>• Formulate – e.g., modelling problems or situations with equations or expressions.<lb/>EDU/WKP(2010)5 <lb/></body>

			<page>32 <lb/></page>

			<body>• Distinguish – e.g., identifying valid and invalid inferences from questions and answers.<lb/>Solving routine problems <lb/>88. <lb/>This cognitive domain relates to problem solving, where the problems are routine in that they are <lb/>typically encountered as classroom exercises or in textbooks. There are five categories of skills identified <lb/>under this cognitive domain: <lb/>• Select – e.g., choosing an appropriate algorithm or strategy to solve a problem<lb/>• Model – e.g., generating an appropriate model using equations or diagrams.<lb/>• Interpret – e.g., understanding a given model presented as equations or diagrams.<lb/>• Apply – e.g., using knowledge of facts, procedures, and concepts to solve routine problems.<lb/>• Verify/Check – e.g., Checking and evaluating the correctness and reasonableness of the solution<lb/>Reasoning <lb/>89. <lb/>This cognitive domain is about solving non-routine problems using logical, systematic thinking <lb/>and various forms of reasoning. Eight categories of skills have been identified in relation to this cognitive <lb/>dimension: <lb/>• Hypothesize/Conjecture/Predict – e.g. discussing ideas, specifying an outcome resulting from an <lb/>unperformed operation<lb/>• Analyze – e.g. making valid inferences from given information, decomposing geometric figures<lb/>• Evaluate – e.g. critically evaluating mathematical ideas, methods, etc.<lb/>• Generalize – e.g. restating results in more widely applicable terms<lb/>• Connect – e.g. linking related mathematical ideas or objects<lb/>• Synthesize/Integrate – e.g. combining results to solve a problem<lb/>• Solve non-routine problems – e.g. applying mathematical procedures in unfamiliar contexts<lb/>• Justify/Prove – e.g. providing evidence for the validity of a statement using mathematical results<lb/>90. <lb/>The proportions of items classified by the TIMSS cognitive domains are given in Table 3.2. <lb/>Table 3.2 Proportions of items in TIMSS by cognitive domains <lb/>Proportion of items in TIMSS tests <lb/>Target proportion of items <lb/>Knowing facts and procedures <lb/>23% <lb/>15% <lb/>Using concepts <lb/>19% <lb/>20% <lb/>Solving routine problems <lb/>36% <lb/>40% <lb/>Reasoning <lb/>22% <lb/>25% <lb/>Total <lb/>100% <lb/>100% <lb/>PISA mathematics assessment framework <lb/>91. <lb/>The PISA mathematics framework begins with a formal definition of mathematical literacy for <lb/>OECD/PISA: <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>33 <lb/></page>

			<body>Mathematical literacy is an individual&apos;s capacity to identify and understand the role that mathematics <lb/>plays in the world, to make well-founded judgements and to use and engage with mathematics in <lb/>ways that meet the needs of that individual&apos;s life as a constructive, concerned and reflective <lb/>citizen. (p. 24, OECD, 2003) <lb/>92. <lb/>This definition, when stated alone, does not show how it differs from the mathematics <lb/>construct of TIMSS. In fact, the TIMSS framework states the following: <lb/>Prime reasons for inclusion of mathematics (in school curricula) are the increasing awareness that <lb/>effectiveness as citizens and success in the workplace are greatly enhanced by knowing and, more <lb/>important, being able to use mathematics. The number of vocations that demand a high level of <lb/>proficiency in the use of mathematics, or mathematical modes of thinking, has burgeoned with the <lb/>advance of technology, and with modern management methods. (p.30, IEA 2003b) <lb/>93. <lb/>What makes the PISA mathematics framework different from the TIMSS framework, and <lb/>different from typical mathematics curricula in most countries, is the fact that PISA does not make the <lb/>assumption that school mathematics will necessarily prepare students to be mathematically literate in their <lb/>future lives as effective citizens. This is evident in the following: <lb/>Rather than being limited to the curriculum content students have learned, the assessments focus on <lb/>determining if students can use what they have learned in the situations they are likely to encounter <lb/>in their daily lives. (p.24, OECD, 2003) <lb/>94. <lb/>That is, PISA sets out to establish the mathematical knowledge and skills required to be <lb/>mathematically literate citizens, and assesses students on these. This is a more direct way to obtain <lb/>measures of whether students can meet future challenges in life, rather than via a proxy that school <lb/>achievement in mathematics is an indicator of students&apos; capacity to use mathematics to solve everyday <lb/>problems. This orientation may have come about in recent years when mathematics educators <lb/>observed that many students regarded school mathematics as an academic discipline divorced from <lb/>real life (e.g., Bonotto, 2003; Verschaffel, Greer &amp; de Corte, 2000). Further, the theory of Realistic <lb/>Mathematics Education (RME) developed in the Netherlands (de Lange, 1996; Gravemeijer, 1999) over <lb/>the past 30 years has gathered support from around the world (de Lange, 1996; Romberg &amp; de Lange, <lb/>1998). Two principles underlie RME: (1) Mathematics must be connected to the real-world; (2) <lb/>Mathematics should be seen as a human activity. <lb/>95. <lb/>PISA has not gone so far as claiming that schools are not preparing students adequately to be <lb/>mathematically literate citizens. But the approach PISA has adopted does not make the assumption that <lb/>schools do prepare students well. One would hope that the different approaches to organising the <lb/>mathematics assessment frameworks in PISA and TIMSS would not lead to different results, since one <lb/>of the main aims of schooling must be to prepare students for their wellbeing in their future lives. If the <lb/>two surveys yield different results that cannot be explained by methodological differences, a close <lb/>examination would be called for to understand the differences. <lb/>96. <lb/>The PISA approach to defining mathematical literacy stems from the definition of &quot; literacy &quot; <lb/>in James Gee&apos;s &quot; Preamble to a literacy program &quot; (1998) where literacy refers to the use of language. <lb/>Each human language has words and rules, but to use a language effectively, one needs to know how <lb/>to combine words and rules in complex ways to convey a vast array of ideas. Analogous to language, <lb/>mathematics also consists of building blocks such as symbols, terms and rules. But to use mathematics <lb/>effectively, one needs to know how to combine the building blocks of mathematics to solve specific <lb/>problems. That is, mathematics literacy is much more than just knowing mathematical symbols and <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>34 <lb/></page>

			<body>rules. Mathematics literacy is about how people use their mathematical knowledge to actually solve <lb/>real-world problems. <lb/>97. <lb/>PISA identifies three dimensions for the organisation of the mathematics framework. These <lb/>dimensions are: (1) situation or context; (2) mathematical content; and (3) mathematical processes. These <lb/>three dimensions are described below. <lb/>Situation/context dimension <lb/>98. <lb/>The following paragraph from the PISA framework describes the situation/context dimension: <lb/>The situation is the part of the student&apos;s world in which the tasks are placed. It is located at a certain <lb/>distance from the students. For OECD/PISA, the closest situation is the students&apos; personal life; next <lb/>is school life, work life and leisure, followed by the local community and society as encountered in <lb/>daily life. Furthest away are scientific situations. Four situation-types will be defined and used for <lb/>problems to be solved: personal, educational/occupational, public, and scientific. (p. 32, OECD, <lb/>2003). <lb/>99. <lb/>The PISA framework specifies that, as far as possible, the target proportions of each <lb/>situation/context type should be about equal. The actual proportions of items by situation/context in the <lb/>PISA assessment are given in Table 3.3. <lb/>Table 3.3 Number and proportions of mathematics items in PISA 2003 by situation/context dimension <lb/>Number of items <lb/>Proportion of items <lb/>Personal <lb/>18 <lb/>21% <lb/>Educational/occupational <lb/>20 <lb/>24% <lb/>Public <lb/>29 <lb/>34% <lb/>Scientific <lb/>18 <lb/>21% <lb/>Total <lb/>85 <lb/>100% <lb/>100. <lb/>The PISA framework also discusses situation/context from the point of view of the distance <lb/>between the problem and the mathematics involved. Tasks involving only mathematical objects without <lb/>any reference to matters outside the mathematical world are termed &quot; intra-mathematical &quot; , while tasks <lb/>involving real-world objects are termed &quot; extra-mathematical &quot; . PISA places an emphasis on extra-<lb/>mathematical tasks. Of the 85 items, only one was classified as intra-mathematical (and this one was also <lb/>classified as &quot; scientific &quot; in Table 3.3), while all other items were extra-mathematical. That is, while <lb/>PISA does not preclude intra-mathematical tasks in the framework, the items in the actual assessment are <lb/>essentially all extra-mathematical. <lb/>PISA mathematical content dimension – The overarching ideas <lb/>101. <lb/>PISA adopts a phenomenological organisation for mathematical content as described below: <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>35 <lb/></page>

			<body>Since the goal of OECD/PISA is to assess students&apos; capacity to solve real problems, our strategy has <lb/>been to define the range of content that will be assessed using a phenomenological approach to <lb/>describing the mathematical concepts, structures or ideas. This means describing content in relation to <lb/>the phenomena and the kinds of problems for which it was created. This approach ensures a focus in <lb/>the assessment that is consistent with the domain definition, yet covers a range of content that <lb/>includes what is typically found in other mathematics assessments and in national mathematics <lb/>curricula. (p.34, OECD, 2003). <lb/>102. <lb/>In other words, PISA considers the world around us, and categorises the tasks that are typically <lb/>encountered in everyday life, and uses these categories as the basis for organising the mathematics content. <lb/>This approach possibly explains why there is only one intra-mathematical item in the PISA test, since <lb/>intra-mathematical tasks are not often encountered in most people&apos;s everyday life, outside the school <lb/>environment. <lb/>103. <lb/>Interestingly, PISA appears to suggest that the phenomenological approach is more inclusive than <lb/>school curriculum, as in the last sentence of the above quote, &quot; This approach … covers a range of content <lb/>that includes what is typically found … in national mathematics curricula. &quot; But from the point of view of <lb/>intra-mathematical and extra-mathematical tasks, and that PISA focuses on individual&apos;s &quot; everyday life &quot; <lb/>instead of focusing on mathematics as in &quot; how mathematics is used in the world &quot; , it would appear that <lb/>PISA&apos;s approach results in a subset of the tasks found in national mathematics curricula. This point <lb/>will be further discussed later in this report. <lb/>104. <lb/>On the other hand, PISA&apos;s approach could be viewed as more inclusive from the point of view <lb/>that the tasks often involved skills from multiple (traditional) content domains, and no isolated knowledge <lb/>or skill is tested without checking whether these skills can be applied to real-life situations. <lb/>105. <lb/>It should be noted that PISA makes a distinction between approaches to assessment and teaching, <lb/>as the following paragraph shows: <lb/>Mathematical concepts, structures and ideas have been invented as tools to organise the phenomena of <lb/>the natural, social and mental world. In schools, the mathematics curriculum has been logically <lb/>organised around content strands (e.g., arithmetic, algebra, geometry) and their detailed topics that <lb/>reflect historically well-established branches of mathematical thinking, and that facilitate the <lb/>development of a structured teaching syllabus. (p. 34, OECD, 2003) <lb/>106. <lb/>It is important to recognise that, while PISA organises the mathematic content differently from <lb/>typical school mathematics curriculum, PISA does not suggest that the organisation based on <lb/>phenomenological approach is necessarily appropriate for organising a structured teaching syllabus. <lb/>Clearly, students cannot be taught tasks involving skills from multiple content domains without having <lb/>been taught basic building blocks of mathematics knowledge and procedures in each content domain. This <lb/>distinction between assessment and teaching is an important one, in that the comparison between PISA and <lb/>TIMSS is focused on assessment, and not on teaching, although there is a strong relationship between the <lb/>two. <lb/>107. <lb/>PISA&apos;s phenomenological approach to organising mathematics content identifies four areas, <lb/>called overarching ideas. The four overarching ideas are quantity, space and shape, change and <lb/>relationships, uncertainty. The PISA mathematics framework provides the following descriptions for <lb/>each of the four overarching ideas (OECD, 2003, pp. 36-37). <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>36 <lb/></page>

			<body>Quantity <lb/>108. <lb/>This overarching idea focuses on the need for quantification in order to organise the world. <lb/>Important aspects include an understanding of relative size, the recognition of numerical patterns, and <lb/>the use of numbers to represent quantities and quantifiable attributes of real-world objects (counts and <lb/>measures). Furthermore, quantity deals with the processing and understanding of numbers that are <lb/>represented to us in various ways. <lb/>109. <lb/>An important aspect of dealing with quantity is quantitative reasoning, Essential components <lb/>of quantitative reasoning are number sense, representing numbers in various ways, understanding the <lb/>meaning of operations, having a feel for the magnitude of numbers, mathematically elegant <lb/>computations, mental arithmetic and estimating. <lb/>Space and shape <lb/>110. <lb/>Patterns are encountered everywhere: in spoken words, music, video, traffic, building <lb/>constructions and art. Shapes can be regarded as patterns: houses, office buildings, bridges, starfish, <lb/>snowflakes, town plans, cloverleaves, crystals and shadows. Geometric patterns can serve as relatively <lb/>simple models of many kinds of phenomena, and their study is possible and desirable at all levels <lb/>(Grünbaum, 1985). <lb/>111. <lb/>The study of shape and constructions requires looking for similarities and differences when <lb/>analysing the components of form and recognising shapes in different representations and different <lb/>dimensions. The study of shapes is closely connected to the concept of &quot; grasping space &quot; . This means <lb/>learning to know, explore and conquer, in order to live, breathe and move with more understanding in the <lb/>space in which we live (Freudenthal, 1973). <lb/>112. <lb/>To achieve this requires understanding the properties of objects and their relative positions. We <lb/>must be aware of how we see things and why we see them as we do. We must learn to navigate through <lb/>space and through constructions and shapes. This means understanding the relationship between shapes <lb/>and images or visual representations, such as that between a real city and photographs and maps of the <lb/>same city. It includes also understanding how three-dimensional objects can be represented in two <lb/>dimensions, how shadows are formed and must be interpreted, what perspective is and how it functions. <lb/>Change and relationships <lb/>113. <lb/>Every natural phenomenon is a manifestation of change, and the world around us displays a <lb/>multitude of temporary and permanent relationships among phenomena. Examples are organisms changing <lb/>as they grow, the cycle of seasons, the ebb and flow of tides, cycles of unemployment, weather changes <lb/>and stock exchange indices. Some of these change processes involve and can be described or modelled by <lb/>straightforward mathematical functions: linear, exponential, periodic or logistic, either discrete or <lb/>continuous. But many relationships fall into different categories, and data analysis is often essential to <lb/>determine the kind of relationship that is present. Mathematical relationships often take the shape of <lb/>equations or inequalities, but relations of a more general nature (e.g., equivalence, divisibility, inclusion, to <lb/>mention but a few) may appear as well. <lb/>114. <lb/>Functional thinking – that is, thinking in terms of and about relationships – is one of the most <lb/>fundamental disciplinary aims of the teaching of mathematics (MAA, 1923). Relationships may be given a <lb/>variety of different representations, including symbolic, algebraic, graphical, tabular and geometrical. <lb/>Different representations may serve different purposes and have different properties. Hence translation <lb/>between representations often is of key importance in dealing with situations and tasks. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>37 <lb/></page>

			<body>Uncertainty <lb/>115. <lb/>The present &quot; information society &quot; offers an abundance of information, often presented as <lb/>accurate, scientific and with a degree of certainty. However, in daily life we are confronted with uncertain <lb/>election results, collapsing bridges, stock market crashes, unreliable weather forecasts, poor predictions for <lb/>population growth, economic models that don&apos;t align, and many other demonstrations of the uncertainty of <lb/>our world. <lb/>116. <lb/>Uncertainty is intended to suggest two related topics: data and chance. These phenomena are <lb/>respectively the subject of mathematical study in statistics and probability. Relatively recent <lb/>recommendations concerning school curricula are unanimous in suggesting that statistics and probability <lb/>should occupy a much more prominent place than has been the case in the past (Committee of Inquiry into <lb/>the Teaching of Mathematics in Schools, 1982; LOGSE, 1990; MSEB, 1990; NCTM, 1989; NCTM, <lb/>2000).. <lb/>117. <lb/>Specific mathematical concepts and activities that are important in this area are collecting data, <lb/>data analysis and display/visualisation, probability and inference. <lb/>118. <lb/>Table 3.4 shows the number and proportion of PISA items classified according to the <lb/>overarching ideas. <lb/>Table 3.4 Number and proportions of mathematics items in PISA 2003 by overarching ideas <lb/>Number of items <lb/>Proportion of items <lb/>Quantity <lb/>23 <lb/>27.0% <lb/>Space and shape <lb/>20 <lb/>23.5% <lb/>Change and relationships <lb/>22 <lb/>26.0% <lb/>Uncertainty <lb/>20 <lb/>23.5% <lb/>Total <lb/>85 <lb/>100.0% <lb/>Mathematical processes dimension <lb/>119. <lb/>The PISA mathematics framework deems the mathematical processes dimension to be the most <lb/>important one, and devotes lengthy discussions to it. As an introduction to describing the mathematical <lb/>processes dimension, the PISA framework begins with a description of the process of mathematisation <lb/>which characterises the way mathematics problems are solved in the real world (OECD, 2003, p.27). <lb/>120. <lb/>There are five steps that characterise the process of mathematisation (OECD, 2003, p.38): <lb/>1. Starting with a problem situated in reality <lb/>2. Organising it according to mathematical concepts <lb/>3. Gradually trimming away the reality through processes such as making assumptions about which <lb/>features of the problem are important, generalising and formalising (which promote the <lb/>mathematical features of the situation and transform the real problem into a mathematical <lb/>problem that faithfully represents the situation <lb/>4. Solving the mathematical problem <lb/>5. Making sense of the mathematical solution in terms of the real situation. <lb/>121. <lb/>The cycle of mathematisation involves an iterative process of moving between a real-world <lb/>problem to a mathematical problem, and then moving from a mathematical problem to a mathematical <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>38 <lb/></page>

			<body>solution and then to a real solution. The process ends with making reflections of the solutions in terms of <lb/>the real-world problem. <lb/>The Competencies <lb/>122. <lb/>To be able to successfully carry out the mathematisation process, an individual will need to draw <lb/>upon a number of competencies. PISA mathematics framework identifies eight competencies in relation to <lb/>the mathematisation process: <lb/>• Thinking and reasoning<lb/>• Argumentation<lb/>• Communication<lb/>• Modelling<lb/>• Problem posing and solving<lb/>• Representation<lb/>• Using symbolic, formal and technical language and operations<lb/>• Use of aids and tools<lb/>123. <lb/>Note that each of the above competencies can be described at different levels. The <lb/>mathematisation process required to solve a particular problem may draw upon different competencies at <lb/>different levels. <lb/>124. <lb/>While an explication of these competencies is useful in teaching and learning, it is difficult to <lb/>assess these competencies individually, since problem-solving tasks typically involve a combination of the <lb/>competencies, and, in a large-scale assessment, the behaviours of students in relation to each competency <lb/>would be difficult to observe. Consequently, PISA summarises the competencies into three broad clusters: <lb/>the reproduction cluster, the connections cluster, and the reflection cluster. Each cluster involves all eight <lb/>competencies, but at different levels. <lb/>125. <lb/>The PISA framework provides the following definitions for the three competency clusters. <lb/>The reproduction cluster <lb/>The competencies in this cluster essentially involve reproduction of practised knowledge. They <lb/>include those most commonly used in standardised assessments and classroom tests. These <lb/>competencies are knowledge of facts and of common problem representations, recognition of <lb/>equivalents, recollection of familiar mathematical objects and properties, performance of routine <lb/>procedures, application of standard algorithms and technical skills, manipulation of expressions <lb/>containing symbols and formulae in standard form, and carrying out computations. (p.42, OECD, <lb/>2003) <lb/>The connections cluster <lb/>The connections cluster competencies build on the reproduction cluster competencies in taking <lb/>problem solving to situations that are not simply routine, but still involved familiar, or quasi-familiar, <lb/>settings. (p.43, OECD, 2003) <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>39 <lb/></page>

			<body>The reflection cluster <lb/>The competencies in this cluster include an element of reflectiveness on the part of the student about <lb/>the processes needed or used to solve a problem. They relate to students&apos; abilities to plan solution <lb/>strategies and implement them in problem settings that contain more elements and may be more <lb/> &quot; original &quot; (or unfamiliar) than those in the connections cluster. (p.46, OECD, 2003) <lb/>Classifying items according to competency clusters <lb/>126. <lb/>Each PISA item was classified into one of the three competency clusters. The classification <lb/>process involved an examination of the levels of the eight competencies required to answer an item. An <lb/>item is assigned to a competency cluster according to the highest level of the competencies required. The <lb/>following table shows the number of PISA items in each of the competency clusters. <lb/>Table 3.5 Number and proportions of mathematics items in PISA 2003 by competency clusters <lb/>Number of items <lb/>Proportion of items <lb/>Reproduction <lb/>26 <lb/>31% <lb/>Connection <lb/>40 <lb/>47% <lb/>Reflection <lb/>19 <lb/>22% <lb/>Total <lb/>85 <lb/>100% <lb/>A Comparison of PISA and TIMSS Mathematics Frameworks <lb/>127. <lb/>TIMSS mathematics framework identifies two dimensions to ensure coverage of mathematics <lb/>assessment tasks: content domains and cognitive domains. PISA mathematics framework identifies three <lb/>dimensions for coverage of mathematics assessment tasks: situation/context dimension, content dimension <lb/>and processes dimension. Of these three dimensions, PISA&apos;s content dimension can be related to TIMSS&apos; <lb/>content domains, while PISA&apos;s processes dimension can be related to TIMSS&apos; cognitive domains. <lb/>128. <lb/>TIMSS does not explicitly state a situation/context dimension as PISA does. This is not <lb/>surprising, since PISA stresses on assessing students&apos; capacity to solve problems encountered in life, PISA <lb/>items are typically embedded within some situation/context. In contrast, to test knowledge and skills based <lb/>on curriculum topics, many TIMSS items do not involve matters outside the mathematical world ( &quot; intra-<lb/>mathematical &quot; ). This is an important distinction between the assessments of PISA and TIMSS. <lb/>A comparison of PISA&apos;s content dimension with TIMSS&apos; content dimension <lb/>129. <lb/>TIMSS&apos; content dimension identifies five content domains: number, algebra, measurement, <lb/>geometry and data. These content domains are familiar to most mathematics educators as many school <lb/>mathematics curricula and textbooks are organised around these content domains. In contrast, PISA&apos;s <lb/>content dimension consists of four overarching ideas: quantity, space and shape, change and <lb/>relationships, and uncertainty. This classification is less familiar to mathematics educators. To facilitate <lb/>comparisons between PISA overarching ideas and TIMSS content domains, PISA items were classified <lb/>according to TIMSS content domains, as shown in Table 3.6. Discussions about the relationships between <lb/>each PISA overarching idea and the TIMSS content domains are given following Table 3.6. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>40 <lb/></page>

			<body>Number of PISA items by TIMSS content domains <lb/>Table 3.6 Tally of PISA items classified by PISA overarching ideas and TIMSS content domains <lb/>PISA overarching ideas <lb/>Quantity <lb/>Space and <lb/>shape <lb/>Change and <lb/>relationships <lb/>Uncertainty <lb/>Total <lb/>TIMSS <lb/>content <lb/>domains <lb/>Number <lb/>23 <lb/>1 <lb/>3 <lb/>5 <lb/>32 <lb/>Algebra <lb/>7 <lb/>7 <lb/>Measurement <lb/>6 <lb/>2 <lb/>8 <lb/>Geometry <lb/>12 <lb/>12 <lb/>Data <lb/>1 <lb/>10 <lb/>15 <lb/>26 <lb/>Total <lb/>23 <lb/>20 <lb/>22 <lb/>20 <lb/>85 <lb/>Quantity <lb/>130. <lb/>This overarching idea is not dissimilar to the number strand in TIMSS. It can be seen from Table <lb/>3.6 that all 23 quantity items were classified as TIMSS number content domain. However, there are quite a <lb/>few non quantity items also classified as TIMSS number strand. That is, it appears that the PISA quantity <lb/>domain is a subset of TIMSS number domain. Box 3.2 shows a PISA quantity item that has been classified <lb/>as number against TIMSS content domains. <lb/>Box 3.2 An example PISA quantity item classified as number against TIMSS content domains <lb/>EXCHANGE RATE <lb/>Mei-Ling from Singapore was preparing to go to South Africa for 3 months as an exchange <lb/>student. She needed to change some Singapore dollars (SGD) into South African Rand (ZAR). <lb/>Question 1: EXCHANGE RATE <lb/>M413Q01 -0 1 9 <lb/>Mei-Ling found out that the exchange rate between Singapore dollars and South African Rand <lb/>was: <lb/>1 SGD = 4.2 ZAR <lb/>Mei-Ling changed 3000 Singapore dollars into South African Rand at this exchange rate. <lb/>How much money in South African Rand did Mei-Ling get? <lb/>131. <lb/>Box 3.3 shows a PISA item that has been classified as TIMSS number content domain, but as <lb/>PISA space and shape overarching idea. For this item, while the underlying operation to be carried out is <lb/>division (which is part of the number content domain), but the mathematisation process relates to dealing <lb/>with space and shape to identify the relevant information before the division operation could be carried out. <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>41 <lb/></page>

			<body>Box 3.3 An example PISA quantity item classified as number against TIMSS content domains <lb/>STAIRCASE <lb/>The diagram below illustrates a staircase with 14 steps and a total height of 252 cm: <lb/>Question 1: STAIRCASE <lb/>M547Q01 <lb/>What is the height of each of the 14 steps? <lb/>Height = ................................................. cm. <lb/>Space and shape <lb/>132. <lb/>Judging from the PISA items that have been classified as space and shape, it appears that this <lb/>overarching idea covers some topics of geometry and measurement as defined by the TIMSS <lb/>framework, for example, two-and three-dimensional shapes and estimates of length, circumference, <lb/>area and volume. However, checking through the list of TIMSS topics under measurement and <lb/>geometry, it appears that many topics are not cover by PISA space and shape domain, such as those <lb/>listed under lines and angles, or under congruence and similarity. In TIMSS, measurement and geometry <lb/>cover a significant number of topics of formal definitions and operations involving lines, angles, <lb/>polygons, Euclidean and Coordinate Geometry. Since these are often intra-mathematical, PISA does <lb/>not seem to cover these kinds of knowledge and skills. A check in Table 3.6 shows that most of the <lb/>PISA space and shape items are classified as geometry or measurement under TIMSS classification <lb/>scheme. But not all TIMSS geometry and measurement items are included under PISA space and <lb/>shape overarching idea. <lb/>133. <lb/>Box 3.4 shows a PISA item classified as space and shape in PISA, and as measurement <lb/>against TIMSS content domains. <lb/>Step-height <lb/>Step-depth <lb/>Total height 252 cm <lb/>Total depth 400 cm <lb/>Plateau <lb/>EDU/WKP(2010)5 <lb/>42 <lb/>Box 3.4 An example PISA space and shape item classified as measurement against TIMSS content domains <lb/>CARPENTER <lb/>Question 1: CARPENTER <lb/>M266Q01 <lb/>A carpenter has 32 metres of timber and wants to make a border around a garden bed. He is <lb/>considering the following designs for the garden bed. <lb/>Circle either Yes or No for each design to indicate whether the garden bed can be made with 32 <lb/>metres of timber. <lb/>Garden bed design <lb/>Using this design, can the garden bed be made with 32 <lb/>metres of timber? <lb/>Design A <lb/>Yes / No <lb/>Design B <lb/>Yes / No <lb/>Design C <lb/>Yes / No <lb/>Design D <lb/>Yes / No <lb/>Change and relationships <lb/>134. <lb/>From the definitions for this overarching idea, it seems reasonable to map the overarching idea <lb/>of change and relationships to the traditional curriculum strand algebra. Indeed, the four PISA items <lb/>classified as TIMSS content domain algebra are change and relationships items in PISA (see Table 3.6). <lb/>10 m <lb/>6 m <lb/>10 m <lb/>10 m <lb/>10 m <lb/>6 m <lb/>6 m <lb/>6 m <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>43 <lb/></page>

			<body>However, a large number of items classified as change and relationships in PISA are classified by other <lb/>traditional strands, with the most number in the data strand. This may not be surprising as the <lb/>descriptions linking change and relationships to natural phenomenon include statistical data such as for <lb/>unemployment or the stock exchange. Looking down the column of change and relationships in Table <lb/>3.6, the items appear as number, algebra, measurement, and data items by TIMSS content domains. This <lb/>shows that the overarching idea, change and relationships, plays a part in most of the traditional <lb/>mathematics strands. From this point of view, change and relationships is probably the least well <lb/>matched overarching idea among the four to traditional curriculum strands. <lb/>135. <lb/>Box 3.5 shows two PISA change and relationships items classified as TIMSS measurement <lb/>content domain <lb/>Box 3.5 An example PISA change and relationships item classified as measurement against TIMSS content <lb/>domains <lb/>INTERNET RELAY CHAT <lb/>Mark (from Sydney, Australia) and Hans (from Berlin, Germany) often communicate with each <lb/>other using &quot; chat &quot; on the Internet. They have to log on to the Internet at the same time to be able <lb/>to &quot; chat &quot; . <lb/>To find a suitable time to &quot; chat &quot; , Mark looked up a chart of world times and found the following: <lb/>Question 1: INTERNET RELAY CHAT <lb/>M402Q01 -0 1 9 <lb/>At 7:00 PM in Sydney, what time is it in Berlin? <lb/>Question 2: INTERNET RELAY CHAT <lb/>M402Q02 -0 1 9 <lb/>Mark and Hans are not able to chat between 9:00 AM and 4:30 PM their local time, as they have <lb/>to go to school. Also, from 11:00 PM till 7:00 AM their local time they won&apos;t be able to chat <lb/>because they will be sleeping. <lb/>When would be a good time for Mark and Hans to chat? Write the local times in the table. <lb/>Place <lb/>Time <lb/>Sydney <lb/>Berlin <lb/>Greenwich 12 Midnight <lb/>Berlin 1:00 AM <lb/>Sydney 10:00 AM <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>44 <lb/></page>

			<body>Uncertainty <lb/>136. <lb/>This overarching idea can be linked to TIMSS&apos; data strand. Interestingly, while this strand <lb/>covers chance and data, TIMSS chooses to label it data, and PISA chooses to label it uncertainty (chance). <lb/>It might be less confusing if both surveys just use the label chance and data as for traditional curriculum <lb/>mathematics strand. Of the 20 items classified as uncertainty in PISA, 15 are classified as data, but five are <lb/>classified as number. These five items involve computing averages and percentages, which could be <lb/>regarded as number or data (statistics). Box 3.6 shows a PISA uncertainty item classified as number (topic <lb/>percentages) against TIMSS content domains. <lb/>Box 3.6 An example PISA uncertainty item classified as number against TIMSS content domains <lb/>EXPORTS <lb/>The graphics below show information about exports from Zedland, a country that uses zeds as its <lb/>currency. <lb/>Question 2: EXPORTS <lb/>M438Q02 <lb/>What was the value of fruit juice exported from Zedland in 2000? <lb/>137. <lb/>The last column in Table 3.6 shows the number of PISA items as classified by TIMSS content <lb/>domains. Notably there are few PISA items in the algebra domain, and a relatively large number of <lb/>20.4 <lb/>25.4 <lb/>27.1 <lb/>37.9 <lb/>42.6 <lb/>0 <lb/>5 <lb/>10 <lb/>15 <lb/>20 <lb/>25 <lb/>30 <lb/>35 <lb/>40 <lb/>45 <lb/>1996 <lb/>1997 <lb/>1998 <lb/>1999 <lb/>2000 <lb/>Total annual exports from Zedland in <lb/>millions of zeds, 1996-2000 <lb/>Distribution of exports from <lb/>Zedland in 2000 <lb/>Year <lb/>Tobacco <lb/>7% <lb/>Wool <lb/>5% <lb/>Cotton fabric <lb/>26% <lb/>Fruit juice <lb/>9% <lb/>Rice <lb/>13% <lb/>Tea <lb/>5% <lb/>Meat <lb/>14% <lb/>Other <lb/>21% <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>45 <lb/></page>

			<body>items in the number and data domains. This is not surprising, given that PISA defines the assessment <lb/>through an examination of the range of quantitative reasoning used by citizens in everyday life in <lb/>situations such as &quot; shopping, travelling, cooking, dealing with personal finances, judging political <lb/>issues, etc. &quot; (p.24, OECD, 2003). Such a list of situations/contexts appears to preclude applications of <lb/>advanced mathematics in very specialised fields, such as the use of transformational geometry in <lb/>animation, solving differential equations in engineering, forecasting trends and building mathematical <lb/>models using calculus. While these applications may well be beyond that expected of 15-year-olds and <lb/>do not appear in either PISA or TIMSS, the underlying mathematical concepts stem from algebra. That <lb/>is, while ordinary citizens are not required to know a great deal of algebra to become mathematically <lb/>literate citizens, specialists in mathematics do need to have knowledge in algebra. Algebra is needed for <lb/>the advance of the modern world, but needed only by a few specialists who can work with technological <lb/>developments This seems the key distinction between the orientations of the PISA and TIMSS <lb/>frameworks, where PISA focuses on everyday needs of citizens in terms of using mathematics, while <lb/>TIMSS focuses on mathematics as a discipline to be used for potential applications in all fields. <lb/>138. <lb/>Finally, with regard to mathematics content classifications, PISA does not provide further <lb/>breakdown beyond the overarching ideas. This is a mechanism to avoid testing fragments of skills. While <lb/>this reflects the emphasis on literacy in PISA, for the purposes of this report such a broad classification <lb/>scheme poses a potential problem. The classification of a PISA item into one traditional mathematics <lb/>content domain is a matter of judgement, as the framework is not designed with this in mind. In <lb/>particular it is challenging to map items in the change and relationships overarching idea to traditional <lb/>content domains. Consequently, it is acknowledged that the classification of PISA test items by traditional <lb/>mathematics content domains could vary considerably. <lb/>Number of TIMSS items by PISA overarching ideas <lb/>139. <lb/>A further comparison between PISA and TIMSS frameworks is illustrated by a re-classification <lb/>of TIMSS items by PISA overarching ideas. Table 3.7 shows the distribution of TIMSS items cross-<lb/>classified by TIMSS content domains and PISA overarching ideas. <lb/>Table 3.7 Tally of TIMSS items classified by PISA overarching ideas and TIMSS content domains <lb/>PISA overarching ideas <lb/>Quantity <lb/>Space and <lb/>shape <lb/>Change and <lb/>relationships <lb/>Uncertainty <lb/>Total <lb/>TIMSS <lb/>content <lb/>domains <lb/>Number <lb/>51 <lb/>3 <lb/>3 <lb/>57 <lb/>Algebra <lb/>13 <lb/>1 <lb/>33 <lb/>47 <lb/>Measurement <lb/>15 <lb/>15 <lb/>1 <lb/>31 <lb/>Geometry <lb/>30 <lb/>1 <lb/>31 <lb/>Data <lb/>1 <lb/>10 <lb/>17 <lb/>28 <lb/>Total <lb/>80 <lb/>49 <lb/>48 <lb/>17 <lb/>194 <lb/>Table 3.7 shows that PISA overarching idea Quantity is closely related to TIMSS Number content domain, <lb/>Space and Shape to TIMSS Geometry and Measurement content domains, Change and relationships to <lb/>TIMSS Algebra content domain, and Uncertainty to TIMSS Data content domain. In addition, the TIMSS <lb/>test does not have many items classified as PISA Uncertainty overarching idea. <lb/>A comparison of PISA&apos;s processes dimension with TIMSS&apos; cognitive domains <lb/>140. <lb/>The descriptions for PISA&apos;s processes dimension are not dissimilar to those for TIMSS cognitive <lb/>domains. Both are concerned with cognitive demands (other than mathematics content) in the process of <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>46 <lb/></page>

			<body>solving a mathematics problem. Below are some comparisons between the three PISA competency clusters <lb/>and the four TIMSS cognitive domains. <lb/>141. <lb/>The PISA reproduction cluster can be linked to TIMSS cognitive domain knowing facts and <lb/>procedures, and perhaps also covers some of the skills listed in TIMSS using concepts domain. The PISA <lb/>connections cluster can be related to TIMSS using concepts and solving routine problems domains. The <lb/>PISA reflection cluster can be linked to TIMSS reasoning domain, where non-routine problems are <lb/>presented to students. <lb/>142. <lb/>To facilitate comparisons between PISA competency clusters and TIMSS cognitive domains, <lb/>PISA items were classified according to TIMSS cognitive domains. Table 3.8 shows a tally of the PISA <lb/>items cross-classified according to PISA competency clusters and TIMSS cognitive domains. <lb/>Table 3.8 Tally of PISA items classified by PISA competency clusters and TIMSS cognitive domains <lb/>PISA competency clusters <lb/>Reproduction <lb/>Connections <lb/>Reflection <lb/>Total <lb/>TIMSS cognitive <lb/>domains <lb/>Knowing facts <lb/>and procedures <lb/>15 <lb/>8 <lb/>0 <lb/>23 <lb/>Using concepts <lb/>5 <lb/>8 <lb/>1 <lb/>14 <lb/>Solving routine <lb/>problems <lb/>4 <lb/>8 <lb/>3 <lb/>15 <lb/>Reasoning <lb/>2 <lb/>16 <lb/>15 <lb/>33 <lb/>Total <lb/>26 <lb/>40 <lb/>19 <lb/>85 <lb/>143. <lb/>As expected, most of the items classified as PISA reproduction items are classified as TIMSS <lb/>knowing facts and procedures items. And most of the items classified as PISA reflection items are <lb/>classified as TIMSS reasoning items. However, for items classified as PISA connections items, there is a <lb/>spread across the TIMSS classifications, but with more items in the TIMSS reasoning domain. Overall, <lb/>the proportions of PISA items classified by TIMSS cognitive domains are quite different from the <lb/>proportions of TIMSS items in the cognitive domains (compare with Table 3.2) where the most number of <lb/>TIMSS items are in the solving routine problems domain, and the most number of PISA items are in the <lb/>reasoning domain. It appears that PISA has succeeded in moving a little away from routine problem <lb/>solving, and moving towards assessing students&apos; ability to solve non-routine problems. However, about <lb/>one quarter of the items are still in the knowing facts and procedures domain, where students are assessed <lb/>on recall and applications of basic procedures. The inclusion of these lower level items is necessary, <lb/>otherwise students at lower levels on the mathematical proficiency scale will not find PISA tests <lb/>accessible. Nevertheless, the differences between PISA and TIMSS in the distributions of items across the <lb/>cognitive domains reflect the different approaches to the development of the frameworks. <lb/>Characteristics of tests and items <lb/>144. <lb/>In this section, item features, such as item format, unit structure, and amount of reading involved, <lb/>are compared between PISA and TIMSS. <lb/>Item Format <lb/>145. <lb/>Both PISA and TIMSS use multiple-choice and constructed-response item formats, as both <lb/>surveys recognise that a multiple-choice format is not suited for students to demonstrate their abilities to <lb/>communicate solutions, make interpretations, construct models and perform other more complex tasks. <lb/>On the other hand, the cost of scoring constructed-response items can become prohibitively expensive in <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>47 <lb/></page>

			<body>large-scale assessments, so the objectively scored multiple-choice item format is also used. Table 3.9 <lb/>shows the proportions of items in multiple-choice or constructed-response format for the two surveys. <lb/>Table 3.9 Proportions of items by item format <lb/>PISA <lb/>TIMSS <lb/>Multiple-choice <lb/>33% <lb/>66% <lb/>Constructed-response <lb/>67% <lb/>34% <lb/>146. <lb/>Table 3.9 shows that PISA has far more items in constructed-response format than TIMSS. In <lb/>fact, two thirds of the items in PISA are in constructed-response format, while only one third of the items <lb/>in TIMSS are in constructed-response format. <lb/>147. <lb/>In general, constructed-response items are more discriminating than multiple-choice items (e.g., <lb/>Routitsky and Turner, 2003), so one would expect PISA tests to show higher test reliability than TIMSS if <lb/>the same number of items is administered. However, TIMSS tests have a higher test reliability than PISA <lb/>tests (see the section Amount of assessment material in Chapter 2), as more items are administered in <lb/>TIMSS, on average, to each student, even though the administration time is shorter. That is, TIMSS items <lb/>tend to be shorter items, while PISA items require more time to answer. For example, PISA items have <lb/>more words in the questions, and will require more time to process the information. The following section <lb/>compares the amount of reading in PISA and TIMSS. <lb/>Amount of reading <lb/>148. <lb/>PISA items involve more reading than TIMSS items. To convey real-world problems situations, <lb/>more words are required to explain about the problem setting, the constraints, and other parameters that <lb/>will need to be assumed. A sample of items was randomly selected from PISA and TIMSS, and the number <lb/>of words in the stem of each item is recorded, as shown in Table 3.10. <lb/>Table 3.10 Number of words in item stem in randomly selected PISA and TIMSS items <lb/>TIMSS cluster M02 <lb/>Question number <lb/>Number of words in <lb/>item stem <lb/>PISA booklet 3 <lb/>Question number <lb/>Number of words in <lb/>item stem <lb/>1 <lb/>19 <lb/>1 <lb/>55 <lb/>2 <lb/>25 <lb/>2 <lb/>59 <lb/>3 <lb/>32 <lb/>3 <lb/>33 <lb/>4 <lb/>20 <lb/>4 <lb/>72 <lb/>5 <lb/>23 <lb/>5 <lb/>30 <lb/>6 <lb/>34 <lb/>6 <lb/>130 <lb/>7 <lb/>5 <lb/>7 <lb/>78 <lb/>8 <lb/>9 <lb/>8 <lb/>33 <lb/>9 <lb/>55 <lb/>9 <lb/>78 <lb/>10 <lb/>35 <lb/>10 <lb/>12 <lb/>11 <lb/>18 <lb/>11 <lb/>34 <lb/>12 <lb/>18 <lb/>12 <lb/>50 <lb/>13 <lb/>11 <lb/>13 <lb/>53 <lb/>14 <lb/>6 <lb/>14 <lb/>23 <lb/>15 <lb/>22 <lb/>15 <lb/>101 <lb/>Mean <lb/>22 <lb/>Mean <lb/>56 <lb/>Standard deviation <lb/>13 <lb/>Standard deviation <lb/>32 <lb/>Standard error <lb/>3.4 <lb/>Standard error <lb/>7.9 <lb/>149. <lb/>From a random sample of 15 items in TIMSS and 15 items in PISA, the average number of <lb/>words in a PISA item stem is about twice as many as the average number of words in a TIMSS item stem. <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>48 <lb/></page>

			<body>That is, the reading load is considerably heavier in PISA than in TIMSS. In addition, PISA items require <lb/>more interpretation of the problem statements over and above the straightforward reading of words. <lb/>Unit structure <lb/>150. <lb/>Given the amount of texts required in PISA to explain problem settings and contexts, it is often <lb/>more efficient to ask more than one question within a problem setting, to reduce the amount of reading for <lb/>each individual task. In cases where more than one question is asked in relation to one stimulus material, <lb/>the group of questions is referred to as a unit. In PISA, 41% of the items are stand-alone items, while the <lb/>rest are grouped in units. In TIMSS, 85% of the items are stand-alone items. The effect of having unit <lb/>structures instead of stand-alone items is that items within a unit are typically more similar to each other, <lb/>thus violating the local independence assumption of items under item response modelling. That is, items <lb/>within a unit could potentially collect the same piece of information, rather than independent pieces of <lb/>information, about a student&apos;s proficiency being assessed. This, of course, will be the extreme case. In fact, <lb/>in PISA, care was taken to ensure that the correctness of a response for an item would not depend on the <lb/>correctness of the answer on another item. But it is possible that students may have some familiarity (or <lb/>unfamiliarity) with particular contexts and situations, and the correctness of the responses to items within a <lb/>unit could be a little more similar than they would have been, had the contexts/situations been completely <lb/>different. Therefore, instead of having collected, say, 20 independent pieces of information, one had only <lb/>collected 18 pieces of information, Consequently, the reported test reliability could be a little inflated. The <lb/>achievement scores, however, are not expected to be biased due to the use of units in the assessment, since <lb/>duplicate information would still be &quot; correct &quot; information. <lb/>Summary <lb/>151. <lb/>This chapter compares PISA and TIMSS frameworks and notes similarities and differences. The <lb/>extent of the impact of these differences on achievement results is discussed in Chapter 4. The following is <lb/>a summary of similarities and differences between the PISA and TIMSS frameworks. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>49 <lb/></page>

			<body>Aspects of the <lb/>Frameworks <lb/>Similarities <lb/>Differences <lb/>Approach <lb/>Both PISA and TIMSS framework <lb/>development <lb/>involved <lb/>extensive <lb/>consultative <lb/>processes <lb/>with <lb/>participating <lb/>countries <lb/>and <lb/>mathematics education experts. <lb/>PISA framework is primarily expert driven <lb/>with endorsements by countries. TIMSS <lb/>framework is primarily country driven with <lb/>endorsements by experts. <lb/>Organising <lb/>principles <lb/>Both PISA and TIMSS organise the <lb/>framework with content and cognitive <lb/>dimensions <lb/>Content <lb/>organisation <lb/>TIMSS adopts traditional mathematics <lb/>content domains: Number, algebra, <lb/>measurement, geometry and data. <lb/>PISA uses phenomenological approach to <lb/>categorise problems based on the kinds of <lb/>applications <lb/>of <lb/>mathematics. <lb/>Four <lb/>overarching ideas are identified: quantity, <lb/>space <lb/>and <lb/>shape, <lb/>change <lb/>and <lb/>relationships, uncertainty. <lb/>Content balance <lb/>TIMSS covers a wider range of curriculum <lb/>contents than PISA. PISA has few items in <lb/>algebra, measurement and geometry, but <lb/>more items in number and data. <lb/>Cognitive <lb/>dimension <lb/>While the labels are different for areas <lb/>of the cognitive dimension, both PISA <lb/>and <lb/>TIMSS <lb/>describe <lb/>cognitive <lb/>processes (or competencies) in terms <lb/>of progressions from simple to complex <lb/>tasks, with knowing facts/reproduction <lb/>at <lb/>the <lb/>lowest <lb/>level, <lb/>and <lb/>reasoning/reflection at the highest <lb/>level. <lb/>Item format <lb/>Two-thirds of the items in TIMSS are of <lb/>multiple-choice format, while only one-third <lb/>of the items in PISA are multipl-choice <lb/>items <lb/>Amount of reading <lb/>On average, PISA items have around twice <lb/>as many words as TIMSS items. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>50 <lb/></page>

			<body>CHAPTER 4 -COMPARISON OF PISA AND TIMSS ACHIEVEMENT RESULTS <lb/>152. <lb/>This chapter compares PISA and TIMSS mathematics achievement scores for countries <lb/>participating in both surveys, and identifies factors that are associated with the observed differences in <lb/>results. <lb/>Comparisons of country mean scores <lb/>153. <lb/>In reporting student scores, both PISA and TIMSS transformed students&apos; IRT scores into a metric <lb/>that had a mean of 500 and a standard deviation of 100 based on a specified reference population. In PISA, <lb/>for mathematics the reference population was the group of OECD countries in the 2003 survey, while in <lb/>TIMSS, the reference population was the group of participating countries in 1995 Grade 8 TIMSS <lb/>mathematics survey (this was to ensure comparability of results to the 1995 data). Consequently, PISA and <lb/>TIMSS do not have strictly aligned definitions for central location and spread for the distributions of <lb/>reported scores, given that the reference populations are different (See Box 4.1 for explanations of central <lb/>location and spread of distributions). That is, a value of 500 in PISA is not directly comparable to a value <lb/>of 500 in TIMSS, since 500 is the mean for a different set of countries in PISA than in TIMSS. Further, <lb/>one PISA score point is not the same as one TIMSS score point in representing achievement differences, <lb/>since the scores have been multiplied by a factor proportional to the standard deviation of the respective <lb/>scores distribution. Consequently, a direct comparison of the reported scores of countries in TIMSS and <lb/>PISA is only valid for comparing rankings of countries, but not how far apart the countries are from each <lb/>other. <lb/>154. <lb/>The reported PISA and TIMSS country mean scores for the 22 countries/regions that participated <lb/>in both PISA and TIMSS are shown in Table 4.1 in columns 2 and 5, with associated standard errors for <lb/>the means in columns 3 and 6. It is important to note that data for England are included in this report for <lb/>illustration, but that England did not meet the required response rates and therefore did not satisfy the <lb/>technical requirements to confidently say that results were comparable internationally in both the TIMSS <lb/>and PISA 2003 surveys. For example, the country average for England was not published in the release of <lb/>the PISA 2003 initial results (OECD, 2004). All achievement results for England should therefore be <lb/>interpreted with caution. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>51 <lb/></page>

			<body>Box 4.1 Central Location and Spread of Distributions <lb/>The following are two example distributions of mathematics scores for two groups of students. It can be seen that <lb/>the performance of the first group is lower than that of the second group, in that the distribution of the first group is <lb/>further to the left of the scale (i.e., lower scores). To describe the general &quot; location &quot; of a distribution, statistics such as <lb/>mean and median are useful. These statistics are referred to as statistics for central tendency. They provide <lb/>information about where the &quot; centre &quot; of the distribution is located. <lb/>Further, it can be seen that the distribution for the first group is more spread out than that for the second group. <lb/>That is, the range of scores for the first group is wider. Statistics such as range and variance are useful to describe the <lb/>spread of a distribution. <lb/>800 <lb/>700 <lb/>600 <lb/>500 <lb/>400 <lb/>300 <lb/>200 <lb/>100 <lb/>400 <lb/>300 <lb/>200 <lb/>100 <lb/>0 <lb/>Frequency <lb/>Mean =413.349494 <lb/>Std. Dev. =102.8566933 <lb/>N =5,835 <lb/>800 <lb/>700 <lb/>600 <lb/>500 <lb/>400 <lb/>300 <lb/>200 <lb/>100 <lb/>400 <lb/>300 <lb/>200 <lb/>100 <lb/>0 <lb/>Frequency <lb/>Mean =542.508653 <lb/>Std. Dev. =83.2732641 <lb/>N =5,796 <lb/>155. <lb/>The countries in Table 4.1 are arranged in decreasing order of PISA country means (column 2). A <lb/>glance down the column of TIMSS country means (column 5) shows that these are also in approximately <lb/>decreasing order, with some disordering. For example, while the Flemish Community of Belgium has a <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>52 <lb/></page>

			<body>similar mean score to Hong Kong-China in PISA, there is a large difference in TIMSS scores between the <lb/>two regions. The correlation between PISA and TIMSS country mean scores is 0.84, showing that, in <lb/>general, there is a reasonable agreement between the results of the two surveys. Note that Indonesia and <lb/>Tunisia have mean scores much lower than those of the other countries. If these two countries are omitted <lb/>in the table, the correlation between the remaining 20 countries is 0.66, which still indicates an association <lb/>between the PISA and TIMSS scores, although the relationship is not extremely strong. <lb/>156. <lb/>To facilitate comparisons between PISA and TIMSS scores, two sets of standardised scores were <lb/>computed. The PISA country mean scores were standardised to have a mean of zero and a standard <lb/>deviation of one (column 4) for the 22 countries. Similarly, TIMSS country mean scores were standardised <lb/>to have a mean of zero and a standard deviation of one (column 7) for the same set of countries. That is, a <lb/>value of 1.01 for Hong Kong-China in PISA indicates that the mean PISA score for Hong Kong-China is <lb/>1.01 PISA standard deviations away from the average of the 22 country means. Similarly, a value of 1.66 <lb/>for Hong Kong-China in TIMSS indicates that the mean TIMSS score for Hong Kong-China is 1.66 <lb/>TIMSS standard deviations away from the mean of the 22 countries. That is, the TIMSS score for Hong <lb/>Kong-China is actually relatively higher than the PISA score when measured in terms of the &quot; distances &quot; <lb/>from the other countries under comparison. <lb/>157. <lb/>These standardised scores are now comparable between PISA and TIMSS. Had countries <lb/>performed in the same way in PISA and TIMSS, one would expect the standardised PISA and TIMSS <lb/>scores to be very similar for each country. If a country has very different standardised scores in PISA and <lb/>in TIMSS, then one might conclude that the country performed differently in PISA and in TIMSS. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>53 <lb/></page>

			<body>Table 4.1 PISA and TIMSS country mean scores for countries participating in both Surveys in 2003 <lb/>PISA <lb/>TIMSS <lb/>Country <lb/>mean <lb/>Standard <lb/>error <lb/>Standardised <lb/>score <lb/>Country <lb/>mean <lb/>Standard <lb/>error <lb/>Standardised <lb/>score <lb/>Belgium (Fl.) <lb/>553 <lb/>(2.1) <lb/>1.04 <lb/>537 <lb/>(2.8) <lb/>0.63 <lb/>Hong Kong-China <lb/>550 <lb/>(4.5) <lb/>0.99 <lb/>586 <lb/>(3.3) <lb/>1.72 <lb/>Korea <lb/>542 <lb/>(3.2) <lb/>0.83 <lb/>589 <lb/>(2.2) <lb/>1.79 <lb/>Quebec, Canada <lb/>541 <lb/>(5.0) <lb/>0.81 <lb/>543 <lb/>(3.0) <lb/>0.77 <lb/>Netherlands <lb/>538 <lb/>(3.1) <lb/>0.74 <lb/>536 <lb/>(3.8) <lb/>0.61 <lb/>Japan <lb/>534 <lb/>(4.0) <lb/>0.67 <lb/>570 <lb/>(2.1) <lb/>1.37 <lb/>Ontario, Canada <lb/>531 <lb/>(3.5) <lb/>0.61 <lb/>521 <lb/>(3.1) <lb/>0.28 <lb/>Australia <lb/>524 <lb/>(2.1) <lb/>0.48 <lb/>505 <lb/>(4.6) <lb/>-0.08 <lb/>Scotland <lb/>524 <lb/>(2.3) <lb/>0.47 <lb/>498 <lb/>(3.7) <lb/>-0.23 <lb/>New Zealand <lb/>523 <lb/>(2.3) <lb/>0.47 <lb/>494 <lb/>(5.3) <lb/>-0.32 <lb/>Sweden <lb/>509 <lb/>(2.6) <lb/>0.19 <lb/>499 <lb/>(2.6) <lb/>-0.21 <lb/>England 1 <lb/>507 <lb/>(2.9) <lb/>0.15 <lb/>498 <lb/>(4.7) <lb/>-0.23 <lb/>Basque country, Spain <lb/>502 <lb/>(2.8) <lb/>0.05 <lb/>487 <lb/>(2.7) <lb/>-0.48 <lb/>Slovak Republic <lb/>498 <lb/>(3.3) <lb/>-0.02 <lb/>508 <lb/>(3.3) <lb/>-0.01 <lb/>Norway <lb/>495 <lb/>(2.4) <lb/>-0.08 <lb/>461 <lb/>(2.5) <lb/>-1.05 <lb/>Hungary <lb/>490 <lb/>(2.8) <lb/>-0.18 <lb/>529 <lb/>(3.2) <lb/>0.46 <lb/>Latvia <lb/>483 <lb/>(3.7) <lb/>-0.30 <lb/>508 <lb/>(3.2) <lb/>-0.01 <lb/>United States <lb/>483 <lb/>(2.9) <lb/>-0.31 <lb/>504 <lb/>(3.3) <lb/>-0.10 <lb/>Russian Federation <lb/>468 <lb/>(4.2) <lb/>-0.59 <lb/>508 <lb/>(3.7) <lb/>-0.01 <lb/>Italy <lb/>466 <lb/>(3.1) <lb/>-0.65 <lb/>484 <lb/>(3.2) <lb/>-0.54 <lb/>Indonesia <lb/>360 <lb/>(3.9) <lb/>-2.68 <lb/>411 <lb/>(4.8) <lb/>-2.16 <lb/>Tunisia <lb/>359 <lb/>(2.5) <lb/>-2.70 <lb/>410 <lb/>(2.2) <lb/>-2.18 <lb/>Average <lb/>499 <lb/>508 <lb/>0.00 <lb/>Standard deviation <lb/>51.9 <lb/>45.1 <lb/>1.00 <lb/>1. England did not achieve the required response rate set by PISA and set by TIMSS for ensuring that a representative sample is <lb/>drawn for the country. The reliability of the mean scores therefore should be treated with some reservation. <lb/>158. <lb/>The distances between countries in standardised PISA and TIMSS scores for the 22 countries <lb/>are shown graphically in Figure 4.1 <lb/>EDU/WKP(2010)5 <lb/>54 <lb/>Figure 4.1 Plot of PISA and TIMSS standardised country mean scores <lb/>BFL <lb/>BFL <lb/>HKG <lb/>HKG <lb/>KOR <lb/>KOR <lb/>NLD <lb/>NLD <lb/>JPN <lb/>JPN <lb/>AUS <lb/>AUS <lb/>SCO <lb/>SWE <lb/>ENG <lb/>ENG <lb/>SVK <lb/>NOR <lb/>NOR <lb/>USA <lb/>USA <lb/>RUS <lb/>RUS <lb/>ITA <lb/>ITA <lb/>IDN <lb/>IDN <lb/>TUN <lb/>TUN <lb/>BSQ <lb/>BSQ <lb/>ONT <lb/>ONT <lb/>QUE <lb/>QUE <lb/>SCO <lb/>NZL <lb/>NZL <lb/>SWE <lb/>SVK <lb/>LVA <lb/>LVA <lb/>-3 <lb/>-2.5 <lb/>-2 <lb/>-1.5 <lb/>-1 <lb/>-0.5 <lb/>0 <lb/>0.5 <lb/>1 <lb/>1.5 <lb/>2 <lb/>2.5 <lb/>BFL <lb/>HKG <lb/>KOR <lb/>NLD <lb/>JPN <lb/>AUS <lb/>SCO <lb/>NZL <lb/>SWE <lb/>ENG <lb/>SVK <lb/>NOR <lb/>HUN <lb/>LVA <lb/>USA <lb/>RUS <lb/>ITA <lb/>IDN <lb/>TUN <lb/>BSQ <lb/>ONT <lb/>QUE <lb/>PISA <lb/>TIMSS <lb/>EDU/WKP(2010)5 <lb/>55 <lb/>159. <lb/>A number of observations can be made about Figure 4.1. First, for TIMSS scores, there is a large <lb/> &quot; gap &quot; between Asian countries and other countries, while, for PISA scores, the &quot; distance &quot; between Asian <lb/>countries and other countries narrows. This means that there is a greater difference between the group of <lb/>Asian countries and other countries in performance on the TIMSS test. This difference is not so marked <lb/>in PISA. Second, there appears to be more separation between countries in achievement levels in PISA, <lb/>while, in TIMSS, there is a clustering of countries around the overall mean score. Third, the countries that <lb/> &quot; moved up &quot; from PISA to TIMSS are mostly Asian countries and Eastern European countries, and the <lb/>countries that moved &quot; down &quot; from PISA to TIMSS tend to be Western countries. As a consequence, most <lb/>English-speaking countries are ahead of Eastern European countries in PISA, but many of them are behind <lb/>Eastern Europeans countries in TIMSS. This difference is more clearly shown in Figure 4.2. The countries <lb/>above the line are those that performed relatively better in TIMSS while those below the line performed <lb/>relatively better in PISA within the comparison of the 22 countries. <lb/>Figure 4.2 Standardised country mean scores: PISA 2003 versus TIMSS 2003 <lb/>1.00 <lb/>0.00 <lb/>-1.00 <lb/>-2.00 <lb/>-3.00 <lb/>2.00 <lb/>1.00 <lb/>0.00 <lb/>-1.00 <lb/>-2.00 <lb/>TIMSSStdScr <lb/>QUE <lb/>ONT <lb/>BSQ <lb/>USA <lb/>TUN <lb/>SWE <lb/>SVK <lb/>SCO <lb/>RUS <lb/>NZL <lb/>NOR <lb/>NLD <lb/>LVA <lb/>KOR <lb/>JPN <lb/>ITA <lb/>IDN <lb/>HUN <lb/>HKG <lb/>BFL <lb/>AUS <lb/>1. England did not meet participation requirements either in TIMSS 2003 or PISA 2003. Mean scores therefore are not comparable to <lb/>those of other participating countries. <lb/>160. <lb/>Interestingly, the same pattern of differences was observed between PISA 2000 and TIMSS 1999 <lb/>results (Wu, 2005), where countries with higher standardised PISA mean scores were Australia, Canada, <lb/>Finland, New Zealand, the United Kingdom and the United States, and countries with higher standardised <lb/>TIMSS scores were the Czech Republic, Hungary, Italy, Japan, Korea, Hong Kong-China, Latvia and the <lb/>Russian Federation. The consistent findings for two PISA and TIMSS cycles indicate that the observed <lb/>pattern is unlikely to be due to chance. There is likely to be systematic differences between the PISA and <lb/>TIMSS tests in relation to systematic differences in education systems in countries. The following sections <lb/>present an examination of possible factors to explain the observed performance differences. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>56 <lb/></page>

			<body>Explaining the Differences between PISA and TIMSS Results <lb/>161. <lb/>To identify factors that may explain the observed differences between PISA and TIMSS country <lb/>rankings as shown in Figure 4.2, test characteristics of PISA and TIMSS are examined, with a focus on <lb/>those characteristics where PISA and TIMSS differ. These include age/grade sampling method, <lb/>mathematics content differences and reading demand of test items (see Chapters 2 and 3). <lb/>Years of Schooling and Age at time of testing <lb/>162. <lb/>In Chapter 2, age and grade differences between PISA and TIMSS were discussed. TIMSS <lb/>population definition controls for the number of years of schooling, but the age of students varies more <lb/>across countries than in PISA. In PISA, the age of sampled students is controlled, but the number of years <lb/>of schooling varies across countries. Could these explain, at least in part, the observed differences in results <lb/>between the two surveys? To answer this question, we first examine the inter-relationship between the two <lb/>variables: age at time of TIMSS testing, and years of schooling at time of PISA testing. We then relate these <lb/>variables to achievement results. <lb/>163. <lb/>For each country, the average age at time of TIMSS 2003 testing is given in the TIMSS <lb/>mathematics report (Exhibit 2, IEA, 2003). Table 4.2 (column 2) shows the average age for Korea and <lb/>Norway, as an example. <lb/>Table 4.2 Relationship between age at time of testing in TIMSS and years of schooling at time of testing in <lb/>PISA <lb/>Average age at time of <lb/>TIMSS test <lb/>(at 8 years of schooling) <lb/>Number of years of schooling <lb/>at time of PISA test <lb/>(at 15.7 years old) <lb/>Korea <lb/>14.6 <lb/>around 9 <lb/>Norway <lb/>13.8 <lb/>around 10 <lb/>164. <lb/>Korea&apos;s sample in TIMSS has an average age of 14.6. That is, students with 8 years of schooling <lb/>in Korea are around 14.6 years of age. Therefore, when students in Korea reach 15.7 years old (the average <lb/>of the PISA sample), one would expect the students to have 9 years of schooling. Similarly, for Norway, <lb/>students are 13.8 years of age when they are in Grade 8. One would then expect 15.7 year-olds (the PISA <lb/>sample) to be in Grade 10. More generally, the number of years of schooling at time of PISA testing can be <lb/>approximately given by <lb/>number of years of schooling at time of PISA testing = (15.7 – age at time of TIMSS testing) + 8, <lb/>as (15.7 – age at time of TIMSS testing) gives the additional number of years of schooling between TIMSS <lb/>and PISA population definitions, to the 8 years of schooling controlled for in the TIMSS samples. So, in <lb/>fact, the two variables, age at time of TIMSS testing, and years of schooling at time of PISA testing are <lb/>essentially the same variable, as one is a linear transformation of the other: <lb/>number of years of schooling at time of PISA testing = 23.7 – age at time of TIMSS testing <lb/>(1) <lb/>165. <lb/>To verify this relationship between age at time of TIMSS testing, and years of schooling at time of <lb/>PISA testing, an attempt was made to compute the average years of schooling at the time of testing in PISA <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>57 <lb/></page>

			<body>using information from the PISA survey. An approximate estimate was made based on the following three <lb/>pieces of information: <lb/>6. The grade variable from the PISA student questionnaire. This variable is meant to provide the <lb/>number of years of schooling. However, it turned out that this variable alone was too &quot; coarse &quot; for <lb/>the purpose of estimating years of schooling to the accuracy of fractions of a year. <lb/>7. The start of the academic year in each country. <lb/>8. The actual testing date of PISA in each country. <lb/>166. <lb/>Combining (2) and (3), one is able to estimate fractions of a year of schooling of the Grade the <lb/>students were placed at the time of PISA testing. It should be noted that there is not a great deal of <lb/>confidence that the grade variable (point 1, above) actually captures the number of years of schooling that <lb/>is comparable across countries. The estimated number of years of schooling for each country, derived as <lb/>described, is shown in Table 4.3, as well as the age at the time of testing for TIMSS, for comparison. The <lb/>entries are arranged in increasing order of the number of years of schooling at the time of testing of PISA, <lb/>as estimated from PISA data. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>58 <lb/></page>

			<body>Table 4.3 Number of years of schooling at the time of PISA testing (estimated from PISA data) versus Average <lb/>age at time of TIMSS testing <lb/>Number of years <lb/>of schooling at the <lb/>time <lb/>of <lb/>PISA <lb/>testing (estimated <lb/>from PISA data) <lb/>Average age at time <lb/>of TIMSS testing <lb/>Tunisia <lb/>8.52 <lb/>14.8 <lb/>Latvia <lb/>8.52 <lb/>15 <lb/>Sweden <lb/>8.67 <lb/>14.9 <lb/>Hungary <lb/>8.79 <lb/>14.5 <lb/>Indonesia <lb/>8.96 <lb/>14.5 <lb/>Netherlands <lb/>9.18 <lb/>14.3 <lb/>Hong Kong-China <lb/>9.20 <lb/>14.4 <lb/>Slovak Republic <lb/>9.21 <lb/>14.3 <lb/>Korea <lb/>9.25 <lb/>14.6 <lb/>Japan <lb/>9.29 <lb/>14.4 <lb/>Belgium (Fl.) <lb/>9.29 <lb/>14.1 <lb/>Russian <lb/>F d ti <lb/>9.32 <lb/>14.2 <lb/>Italy <lb/>9.40 <lb/>13.9 <lb/>Spain-Basque <lb/>9.40 <lb/>14.1 <lb/>Canada-Ontario <lb/>9.48 <lb/>13.8 <lb/>Canada-Quebec <lb/>9.48 <lb/>14.2 <lb/>New Zealand <lb/>9.51 <lb/>14.1 <lb/>United States <lb/>9.55 <lb/>14.2 <lb/>Australia <lb/>9.63 <lb/>13.9 <lb/>Norway <lb/>9.66 <lb/>13.8 <lb/>England <lb/>10.31 <lb/>14.3 <lb/>Scotland <lb/>10.69 <lb/>13.7 <lb/>167. <lb/>Table 4.3 shows that, by and large, PISA students in the Western countries tend to have had a <lb/>higher number of years of schooling than the PISA students in Asian and Eastern European countries. At <lb/>the same time, there appears to be a negative relationship between the number of years of schooling at the <lb/>time of testing of PISA and the age at the time of testing of TIMSS, as one would expect from equation (1). <lb/>This relationship can be seen more readily in a scatter plot of the two variables, as shown in Figure 4.3. <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>59 <lb/></page>

			<body>Figure 4.3 PISA years of schooling versus TIMSS age of testing <lb/>15.00 <lb/>14.75 <lb/>14.50 <lb/>14.25 <lb/>14.00 <lb/>13.75 <lb/>Age <lb/>11.00 <lb/>10.50 <lb/>10.00 <lb/>9.50 <lb/>9.00 <lb/>8.50 <lb/>YrsSchooling <lb/>QUE <lb/>ONT <lb/>BSQ <lb/>USA <lb/>TUN <lb/>SWE <lb/>SVK <lb/>SCO <lb/>RUS <lb/>NZL <lb/>NOR <lb/>NLD <lb/>LVA <lb/>KOR <lb/>JPN <lb/>ITA <lb/>IDN <lb/>HUN <lb/>HKG <lb/>ENG <lb/>BFL <lb/>AUS <lb/>168. <lb/>The correlation between PISA years of schooling (estimated from PISA data) and TIMSS age of <lb/>testing is -0.77 (R 2 = 0.59), showing a strong relationship between these two variables. This is particularly <lb/>striking given that the variable, number of years of schooling in PISA, was an approximation constructed <lb/>for this report. Consequently, the age at time of TIMSS testing could be regarded as a proxy variable for <lb/>years of schooling in PISA, given the theoretical relationship between these two variables as shown in <lb/>Equation (1), as well as the empirical validation of the relationship as shown in Figure 4.3. England and <lb/>Scotland appear to be outliers in Figure 4.3. If England and Scotland are removed from Figure 4.3, the <lb/>correlation between the two variables is -0.9. This strong relationship between the two variables in Figure <lb/>4.3 suggests that Grade-based samples in TIMSS have been well controlled for the number of years of <lb/>schooling. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>60 <lb/></page>

			<body>Box 4.2 The interpretations of R and R <lb/>2 in regression models <lb/>Consider a regression analysis where a dependent variable Y is to be explained, or predicted, by an independent <lb/>variable, X. The regression equation can be written as <lb/>Y a bX <lb/>= + <lb/>where a is called a regression constant and b is a regression coefficient. a and b are to be estimated in the <lb/>regression analysis from the ( X and Y ) data pairs. Typically, regression analysis also reports R and R 2 . In the case <lb/>where there is only one explanatory variable ( X in the above example), R is the correlation coefficient between X and <lb/>Y . The correlation coefficient, R, is a measure of association between two variables, ranging between -1 and 1. If a <lb/>plot of ( X and Y ) pairs fall exactly on a straight line with a positive gradient, then the correlation coefficient will be 1. <lb/>If the line has a negative gradient, then the correlation coefficient will be -1. If there is no association between X and <lb/>Y , then the correlation coefficient will be zero. The square of the correlation coefficient (R 2 ) is called the coefficient of <lb/>determination. R <lb/>2 can be shown to be a measure of the proportion of sample variation in the dependent variable Y <lb/>which is explained by the values of the independent variable X : <lb/>2 <lb/>explained variation in <lb/>total variation in <lb/>Y <lb/>R <lb/>Y <lb/>= <lb/>When there are more than one explanatory (or independent) variable, as shown below: <lb/>0 <lb/>1 1 <lb/>2 2 <lb/>Y b b X b X <lb/>= + <lb/>+ <lb/>+L <lb/>R <lb/>2 shows the proportion of variation in Y explained by the combined set of the independent variables. In this case, R <lb/>is called the coefficient of multiple correlation, and R <lb/>2 is called the coefficient of multiple determination. <lb/>Impact of Years of Schooling on Student Performance in Mathematics <lb/>169. <lb/>Table 4.4 shows the list of countries arranged in order of how much better the countries <lb/>performed in TIMSS than in PISA. In this table, the metric for expressing score differences is in &quot; TIMSS <lb/>score &quot; unit, and not in standardised mean scores as computed in Table 4.1. That is, PISA scores have been <lb/>converted to have the same mean and standard deviation of the TIMSS scores for the 22 countries, and <lb/>then the difference between TIMSS and PISA scores is computed. In this way, the magnitude of the <lb/>differences can be more easily interpreted than standardised scores, since one can discuss the average gain <lb/>in TIMSS score unit, with one additional year of schooling, for example. The countries at the top of Table <lb/>4.4 performed better in TIMSS than in PISA; the countries at the bottom of the table performed better in <lb/>PISA. In addition, for each country, the average age of students in the TIMSS 12 assessment is also shown. <lb/>12. <lb/>The average age at time of testing in TIMSS is taken from the TIMSS 2003 International Report (IEA, <lb/>2003). <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>61 <lb/></page>

			<body>Table 4.4 Comparative performance in PISA and TIMSS, versus age of testing in TIMSS <lb/>Difference in <lb/>country mean <lb/>scores (TIMSS -<lb/>PISA), in &quot;TIMSS <lb/>score&quot; unit <lb/>Average age at time of <lb/>testing in TIMSS <lb/>Number of years of <lb/>schooling at time of PISA <lb/>testing (estimated from <lb/>PISA data) <lb/>Better in TIMSS <lb/>Korea <lb/>43.16 <lb/>14.6 <lb/>9.25 <lb/>Hong Kong-China <lb/>33.08 <lb/>14.4 <lb/>9.20 <lb/>Japan <lb/>31.19 <lb/>14.4 <lb/>9.29 <lb/>Hungary <lb/>28.50 <lb/>14.5 <lb/>8.79 <lb/>Russian Federation <lb/>26.26 <lb/>14.2 <lb/>9.32 <lb/>Tunisia <lb/>23.48 <lb/>14.8 <lb/>8.52 <lb/>Indonesia <lb/>23.25 <lb/>14.5 <lb/>8.96 <lb/>Latvia <lb/>13.26 <lb/>15.0 <lb/>8.52 <lb/>United States <lb/>9.69 <lb/>14.2 <lb/>9.55 <lb/>Italy <lb/>4.64 <lb/>13.9 <lb/>9.40 <lb/>Slovak Republic <lb/>0.40 <lb/>14.3 <lb/>9.21 <lb/>Better in PISA <lb/>Quebec, Canada <lb/>-1.77 <lb/>14.2 <lb/>9.48 <lb/>Netherlands <lb/>-6.01 <lb/>14.3 <lb/>9.18 <lb/>Ontario, Canada <lb/>-15.09 <lb/>13.8 <lb/>9.48 <lb/>England <lb/>1 <lb/>-17.21 <lb/>14.3 <lb/>10.31 <lb/>Sweden <lb/>-18.03 <lb/>14.9 <lb/>8.67 <lb/>Belgium (Fl.) <lb/>-18.53 <lb/>14.1 <lb/>9.29 <lb/>Basque country, Spain <lb/>-23.59 <lb/>14.1 <lb/>9.40 <lb/>Australia <lb/>-25.24 <lb/>13.9 <lb/>9.63 <lb/>Scotland <lb/>-31.86 <lb/>13.7 <lb/>10.69 <lb/>New Zealand <lb/>-35.57 <lb/>14.1 <lb/>9.51 <lb/>Norway <lb/>-43.99 <lb/>13.8 <lb/>9.66 <lb/>1. <lb/>PISA results for England are not comparable and therefore this score point difference should be interpreted with caution. <lb/>170. <lb/>Table 4.4 shows that the Asian and Eastern European countries tend to perform better in TIMSS <lb/>than in PISA. But it also appears that the Asian and Eastern European countries have a slightly older cohort <lb/>in the TIMSS assessment. The relationship between differential performance in TIMSS and PISA, and age <lb/>at time of testing in TIMSS can be better seen from a scatter plot of the two variables, as shown in Figure <lb/>4.4, where the vertical scale shows the difference between TIMSS and PISA score (Column 3 of Table <lb/>4.4). <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>62 <lb/></page>

			<body>Figure 4.4 Relationship between performance in TIMSS and PISA and age at time of testing in TIMSS <lb/>15.00 <lb/>14.75 <lb/>14.50 <lb/>14.25 <lb/>14.00 <lb/>13.75 <lb/>Age at time of testing in TIMSS <lb/>40.00 <lb/>20.00 <lb/>0.00 <lb/>-20.00 <lb/>-40.00 <lb/>TIMSS -PISA (in TIMSS Score Unit) <lb/>QUE <lb/>ONT <lb/>BSQ <lb/>USA <lb/>TUN <lb/>SWE <lb/>SVK <lb/>SCO <lb/>RUS <lb/>NZL <lb/>NOR <lb/>NLD <lb/>LVA <lb/>KOR <lb/>JPN <lb/>ITA <lb/>IDN <lb/>HUN <lb/>HKG <lb/>ENG <lb/>BFL <lb/>AUS <lb/>171. <lb/>Apart from Sweden, Figure 4.4 shows a positive relationship between (TIMSS – PISA) and the <lb/>age at time of testing in TIMSS. The correlation between the two variables is 0.58 (R 2 =0.34). This is <lb/>significantly different from zero with p=0.004. If Sweden is removed from the data set, the correlation <lb/>between the two variables is 0.713 (R 2 =0.51), significantly different from zero with p=0.0003. That is, <lb/>countries with an older cohort at the time of testing in TIMSS tend to perform better in TIMSS than in <lb/>PISA. In contrast, Norway and Scotland performed a great deal better in PISA than in TIMSS (see Figure <lb/>4.1), and it appears that these two countries/regions have the youngest cohorts (13.7 and 13.8 years old <lb/>respectively) among the 22 TIMSS countries. <lb/>172. <lb/>Of all 50 participating countries in TIMSS 2003 Grade 8 cohort 13 , Sweden is the only Western <lb/>country with an older cohort (14.9 years old), while all other Western countries have an average age less <lb/>than 14.3 years. The case for Sweden is discussed further in this chapter when both TIMSS and PISA <lb/>advantage indices are presented and an explanation is provided for why Sweden appears as an outlier in <lb/>Figure 4.4. <lb/>173. <lb/>Overall, there is a relationship between the differential performance in TIMSS and PISA, and the <lb/>age of the cohort in the TIMSS sample. It should be remembered, however, that the age of the cohort in the <lb/>13 <lb/>There 46 countries and four benchmarking participants in the TIMSS 2003 Grade 8 cohort. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>63 <lb/></page>

			<body>TIMSS sample is also a proxy for the number of years of schooling at time of PISA testing, as discussed in <lb/>the previous sections of this chapter. <lb/>174. <lb/>A plot of years of schooling in PISA (as estimated from the PISA data) and differential <lb/>performance between TIMSS and PISA is shown in Figure 4.5. <lb/>Figure 4.5 Relationship between years of schooling in PISA and differential performance between TIMSS and <lb/>PISA <lb/>11.00 <lb/>10.50 <lb/>10.00 <lb/>9.50 <lb/>9.00 <lb/>8.50 <lb/>YrsSchooling <lb/>40.00 <lb/>20.00 <lb/>0.00 <lb/>-20.00 <lb/>-40.00 <lb/>Difference <lb/>QUE <lb/>ONT <lb/>BSQ <lb/>USA <lb/>TUN <lb/>SWE <lb/>SVK <lb/>SCO <lb/>RUS <lb/>NZL <lb/>NOR <lb/>NLD <lb/>LVA <lb/>KOR <lb/>JPN <lb/>ITA <lb/>IDN <lb/>HUN <lb/>HKG <lb/>ENG <lb/>BFL <lb/>AUS <lb/>175. <lb/>Figure 4.5 shows a negative relationship between the number of years of schooling in PISA and <lb/>the differential performance between TIMSS and PISA, where the correlation between the two variables <lb/>is -0.52 (R 2 =0.27). This relationship is not as strong as that between the age at the time of testing in TIMSS <lb/>and the differential performance between TIMSS and PISA, possibly due to the fact that the estimated <lb/>number of years of schooling in PISA had to be constructed indirectly from a number of data sources. <lb/>Nevertheless, Figure 4.5 still shows an association between the two variables. <lb/>176. <lb/>The relationships shown in Figures 4.3, 4.4 and 4.5 suggest that the difference between <lb/>performance in TIMSS and PISA could be related to the age of testing in TIMSS or the number of years of <lb/>schooling in PISA. For future cycles of testing, it will be helpful to capture more reliable information on <lb/>the number of years of schooling at the time of testing, to help to understand and interpret the achievement <lb/>results. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>64 <lb/></page>

			<body>The impact of differences in content balance between PISA and TIMSS <lb/>177. <lb/>From a number of studies (e.g., Routitsky and Zammit, 2002; Zabulionis, 2001) carried out on <lb/>the comparisons of mathematics education in the international context, it is not surprising to hypothesise <lb/>that differences in content balance in PISA and TIMSS may lead to the observed differences in country <lb/>performances as shown in Figures 4.1 and 4.2. In particular, there are different mathematics traditions <lb/>between countries. The Asian and Eastern European countries stress formal mathematics, while the <lb/>Western countries place an emphasis on problem solving and application skills (e.g., Leung, Graf &amp; <lb/>Lopez-Real, 2006). Since the TIMSS assessment is more curricular focused and the PISA assessment is <lb/>more problem oriented, this could explain the observed differences in the results of the two assessments. <lb/>178. <lb/>In making comparisons between the content of the PISA and TIMSS assessments, PISA items <lb/>were classified according to the TIMSS content domain classifications (see Table 3.6). The decision to re-<lb/>classify PISA items according to TIMSS classifications rather than the other way round is because national <lb/>curricula are mostly structured by traditional mathematics content areas. It will be easier to compare <lb/>proportions of items in different traditional mathematics content areas in PISA and TIMSS to those in the <lb/>national curricula. <lb/>179. <lb/>A comparison of the proportions of PISA and TIMSS items by TIMSS content domain <lb/>classifications is given in Table 4.5. <lb/>Table 4.5 Number and proportions of items in PISA and TIMSS by content domains <lb/>PISA <lb/>TIMSS <lb/>Differences in <lb/>percentages <lb/>between PISA <lb/>and TIMSS <lb/>Number <lb/>32 <lb/>38% <lb/>57 <lb/>30% <lb/>8% <lb/>Algebra <lb/>7 <lb/>8% <lb/>47 <lb/>24% <lb/>-16% <lb/>Measurement <lb/>8 <lb/>9% <lb/>31 <lb/>16% <lb/>-7% <lb/>Geometry <lb/>12 <lb/>14% <lb/>31 <lb/>16% <lb/>-2% <lb/>Data <lb/>26 <lb/>31% <lb/>28 <lb/>14% <lb/>17% <lb/>Total <lb/>85 <lb/>100% <lb/>194 <lb/>100% <lb/>180. <lb/>It can be seen from Table 4.5 that, in PISA, there are more items in the content domains of <lb/>number and data, while, in TIMSS, the proportions of items in each content domain are more evenly <lb/>spread. There are fewer algebra and measurement items in PISA when compared to item proportions in <lb/>TIMSS. Given the &quot; literacy &quot; orientation of the PISA assessment, the distribution of PISA items across <lb/>the content domains is not surprising. If one surveys the mathematics that most people have to deal with in <lb/>everyday life, one does not come across solving equations very often. Instead, in everyday life, one needs <lb/>to interpret information in tables or graphs, or calculate prices/discounts, so that there is a predominance <lb/>of number and data applications that one has to deal with. This is not to say that algebra is not an <lb/>important part of mathematics. Algebra is very important for many applications in the technological world. <lb/>But, by and large, only a small proportion of people specialise in these applications. <lb/>Achievement by content domains <lb/>181. <lb/>It will be of interest to examine whether there are differences across countries in achievement <lb/>scores by content domains. TIMSS 2003 International Mathematics Report published averaged scaled <lb/>scores by mathematics content areas by country (see IEA, 2003, Exhibit 3.1). The following is an extract <lb/>from the TIMSS report for the 22 countries that participated in both PISA and TIMSS. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>65 <lb/></page>

			<body>Table 4.6 TIMSS achievement scores by content areas <lb/>Number Algebra Measurement Geometry Data <lb/>Australia <lb/>498 <lb/>499 <lb/>511 <lb/>491 <lb/>531 <lb/>Belgium (Fl.) <lb/>539 <lb/>523 <lb/>535 <lb/>527 <lb/>546 <lb/>England <lb/>1 <lb/>485 <lb/>492 <lb/>505 <lb/>492 <lb/>535 <lb/>Canada-Ontario <lb/>516 <lb/>515 <lb/>520 <lb/>513 <lb/>538 <lb/>Canada-Quebec <lb/>546 <lb/>529 <lb/>541 <lb/>542 <lb/>544 <lb/>Hong Kong-<lb/>China <lb/>586 <lb/>580 <lb/>584 <lb/>588 <lb/>566 <lb/>Hungary <lb/>529 <lb/>534 <lb/>525 <lb/>515 <lb/>526 <lb/>Indonesia <lb/>421 <lb/>418 <lb/>394 <lb/>413 <lb/>418 <lb/>Italy <lb/>480 <lb/>477 <lb/>500 <lb/>469 <lb/>490 <lb/>Japan <lb/>557 <lb/>568 <lb/>559 <lb/>587 <lb/>573 <lb/>Korea <lb/>586 <lb/>597 <lb/>577 <lb/>598 <lb/>569 <lb/>Latvia <lb/>507 <lb/>508 <lb/>500 <lb/>515 <lb/>506 <lb/>Netherlands <lb/>539 <lb/>514 <lb/>549 <lb/>513 <lb/>560 <lb/>New Zealand <lb/>481 <lb/>490 <lb/>500 <lb/>488 <lb/>526 <lb/>Norway <lb/>456 <lb/>428 <lb/>481 <lb/>461 <lb/>498 <lb/>Russian <lb/>Federation <lb/>505 <lb/>516 <lb/>507 <lb/>515 <lb/>484 <lb/>Scotland <lb/>484 <lb/>488 <lb/>508 <lb/>491 <lb/>531 <lb/>Slovak Republic <lb/>514 <lb/>505 <lb/>508 <lb/>501 <lb/>495 <lb/>Spain-Basque <lb/>490 <lb/>490 <lb/>488 <lb/>456 <lb/>499 <lb/>Sweden <lb/>496 <lb/>480 <lb/>512 <lb/>467 <lb/>539 <lb/>Tunisia <lb/>419 <lb/>405 <lb/>407 <lb/>427 <lb/>387 <lb/>United States <lb/>508 <lb/>510 <lb/>495 <lb/>472 <lb/>527 <lb/>1. England did not meet participation requirements in TIMSS 2003. Mean scores are therefore not comparable to other countries. <lb/>182. <lb/>Table 4.6 shows that the patterns of performance across mathematics content areas are quite <lb/>different between countries. For example, in Australia, the average score for data is 32 points higher than <lb/>for algebra 14 . In contrast, in the Russian Federation, it is just the opposite, where the score for algebra is <lb/>32 points higher than the score for data. Such differential patterns of content area achievement scores <lb/>across countries will lead to different aggregate scores if proportions of items from different content areas <lb/>change. As described in the previous section, PISA and TIMSS have quite different proportions of items <lb/>from each content area, as shown in Table 4.5. <lb/>183. <lb/>To assess the impact on mathematics achievement score when proportions of items from different <lb/>content areas change, indices of &quot; PISA advantage &quot; and &quot; TIMSS advantage &quot; were constructed. The <lb/>methodology is presented below and is based only on TIMSS achievement scores and not PISA <lb/>achievement scores. Also, the mapping of PISA items to TIMSS content domains that was presented in <lb/>Chapter 3 is used and the reader is reminded that no definitive categorisation of PISA items into TIMSS <lb/>content domains is possible. With these caveats in mind the methodology, using Australia as an example, is <lb/>as follows: <lb/>184. <lb/>The TIMSS average scores by mathematics content areas for Australia are given below: <lb/>14 <lb/>The standard errors of these estimates are around 2 to 5, so a mean difference of 32 is certainly significant. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>66 <lb/></page>

			<body>TIMSS mean score by content area <lb/>Mean of the <lb/>five content <lb/>areas <lb/>15 <lb/>Number <lb/>Algebra <lb/>Measurement <lb/>Geometry <lb/>Data <lb/>Australia <lb/>498 <lb/>499 <lb/>511 <lb/>491 <lb/>531 <lb/>506 <lb/>185. <lb/>The final column shows the mean of the five content area scores (506). The difference between <lb/>each content area score and the mean of the five content areas is then calculated, as shown below: <lb/>Deviation of content area score from the mean of the five content areas (506) <lb/>Number <lb/>Algebra <lb/>Measurement <lb/>Geometry <lb/>Data <lb/>Australia <lb/>-8 <lb/>-7 <lb/>5 <lb/>-15 <lb/>25 <lb/>186. <lb/>The deviation from each content area is then weighted by the proportion of items in each <lb/>assessment. For example, for PISA, the proportions of items are as follows: <lb/>PISA item distribution by content areas <lb/>Number <lb/>Algebra <lb/>Measurement <lb/>Geometry <lb/>Data <lb/>38% <lb/>8% <lb/>9% <lb/>14% <lb/>31% <lb/>187. <lb/>The PISA advantage index is computed as the weighted sum of the deviations of content area <lb/>scores from the mean of the five content areas, weighted by item proportions in PISA. For example, the <lb/>PISA advantage index for Australia is: <lb/>(-8) × 0.38 + (-7) × 0.08 + 5 × 0.09 + (-15) × 0.14 + 25 × 0.31 = 2.42 <lb/>188. <lb/>The same method is applied to calculate the TIMSS advantage index, using the proportion of <lb/>TIMSS items listed below: <lb/>TIMSS item distribution by content areas <lb/>Number <lb/>Algebra <lb/>Measurement <lb/>Geometry <lb/>Data <lb/>30% <lb/>24% <lb/>16% <lb/>16% <lb/>14% <lb/>189. <lb/>The TIMSS advantage index for Australia is: <lb/>(-8) × 0.30 + (-7) × 0.24 + 5 × 0.16 + (-15) × 0.16 + 25 × 0.14 = -2.18 <lb/>190. <lb/>An overall Content Advantage Index is computed as the difference between PISA advantage <lb/>index and TIMSS advantage index. So, for Australia, the Content Advantage Index is given by: <lb/>2.42 – (-2.18) = 4.6 <lb/>15 <lb/>Note that this is the mean of the scores for the five content areas. This mean will be close to the average <lb/>scaled score for overall mathematics achievement, but not necessarily exactly the same. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>67 <lb/></page>

			<body>191. <lb/>The Content Advantage Index shows how much more advantage a country has in PISA as <lb/>compared to in TIMSS. Thus, from the above computations, the PISA assessment gives Australia more <lb/> &quot; advantage &quot; (4.6 units), than the TIMSS assessment in that Australia is likely to score higher in PISA than <lb/>in TIMSS, given the composition of items from different content areas in PISA and in TIMSS. <lb/>192. <lb/>The PISA and TIMSS advantage indices, and the Content Advantage Indices were computed for <lb/>all 22 countries, and these are shown in Table 4.7, together with the difference in TIMSS and PISA <lb/>achievement mean scores (in TIMSS score unit), arranged in order of the difference in performance in <lb/>TIMSS and PISA. <lb/>Table 4.7 Indices of PISA and TIMSS advantage by country <lb/>Difference in <lb/>country mean <lb/>scores (TIMSS <lb/>-PISA), in <lb/>&quot;TIMSS score&quot; <lb/>unit <lb/>PISA <lb/>advantage <lb/>index <lb/>TIMSS <lb/>advantage <lb/>index <lb/>Content <lb/>advantage <lb/>index (PISA <lb/>adv – TIMSS <lb/>adv) <lb/>Better in TIMSS <lb/>Korea <lb/>43.16 <lb/>-2.85 <lb/>1.29 <lb/>-4.14 <lb/>Hong Kong <lb/>33.08 <lb/>-1.32 <lb/>0.86 <lb/>-2.18 <lb/>Japan <lb/>31.19 <lb/>-1.58 <lb/>-1.71 <lb/>0.14 <lb/>Hungary <lb/>28.50 <lb/>0.34 <lb/>1.10 <lb/>-0.76 <lb/>Russian <lb/>F d ti <lb/>26.26 <lb/>-4.32 <lb/>1.15 <lb/>-5.47 <lb/>Tunisia <lb/>23.48 <lb/>-0.94 <lb/>1.35 <lb/>-2.29 <lb/>Indonesia <lb/>23.25 <lb/>3.36 <lb/>1.45 <lb/>1.92 <lb/>Latvia <lb/>13.26 <lb/>0.05 <lb/>0.06 <lb/>-0.01 <lb/>United States <lb/>9.69 <lb/>5.27 <lb/>1.00 <lb/>4.27 <lb/>Italy <lb/>4.64 <lb/>-0.06 <lb/>-1.05 <lb/>0.99 <lb/>Slovak Republic <lb/>0.40 <lb/>0.45 <lb/>1.44 <lb/>-0.99 <lb/>Better in PISA <lb/>Canada-Quebec <lb/>-1.77 <lb/>2.55 <lb/>-0.25 <lb/>2.80 <lb/>Netherlands <lb/></body>

			<listBibl>-6.01 <lb/>5.64 <lb/>-1.58 <lb/>7.22 <lb/>Canada-Ontario <lb/>-15.09 <lb/>2.20 <lb/>-1.31 <lb/>3.51 <lb/>England <lb/>-17.21 <lb/>1.94 <lb/>-3.57 <lb/>5.51 <lb/>Sweden <lb/>-18.03 <lb/>6.45 <lb/>-2.55 <lb/>8.99 <lb/>Belgium (Fl.) <lb/>-18.53 <lb/>3.75 <lb/>-0.42 <lb/>4.18 <lb/>Spain-Basque <lb/>-23.59 <lb/>3.16 <lb/>0.95 <lb/>2.22 <lb/>Australia <lb/>-25.24 <lb/>2.41 <lb/>-2.04 <lb/>4.45 <lb/>Scotland <lb/>-31.86 <lb/>1.55 <lb/>-3.69 <lb/>5.25 <lb/>New Zealand <lb/>-35.57 <lb/>1.28 <lb/>-3.17 <lb/>4.45 <lb/>Norway <lb/>-43.99 <lb/>4.80 <lb/>-4.73 <lb/>9.53 <lb/>193. <lb/>A simple scatter plot between (TIMSS-PISA) scores (column 3) and the Content Advantage <lb/>Index (column 6) is given in Figure 4.6. <lb/>EDU/WKP(2010)5 <lb/></listBibl>

			<page>68 <lb/></page>

			<body>Figure 4.6 Relationship between difference in TIMSS and PISA scores and the Content Advantage Index <lb/>10.00 <lb/>7.50 <lb/>5.00 <lb/>2.50 <lb/>0.00 <lb/>-2.50 <lb/>-5.00 <lb/>Content Advantage Index <lb/>40.00 <lb/>20.00 <lb/>0.00 <lb/>-20.00 <lb/>-40.00 <lb/>TIMSS -PISA <lb/>QUE <lb/>ONT <lb/>BSQ <lb/>USA <lb/>TUN <lb/>SWE <lb/>SVK <lb/>SCO <lb/>RUS <lb/>NZL <lb/>NOR <lb/>NLD <lb/>LVA <lb/>KOR <lb/>JPN <lb/>ITA <lb/>IDN <lb/>HUN <lb/>HKG <lb/>ENG <lb/>BFL <lb/>AUS <lb/>194. <lb/>Figure 4.6 shows a clear relationship between the difference in TIMSS and PISA mean scores <lb/>and the content advantage index. When the relative content advantage in PISA is lower than in TIMSS, <lb/>(TIMSS score – PISA score) tends to be positive, and when the relative content advantage is higher in <lb/>PISA than in TIMSS, (TIMSS score – PISA score) tends to be negative. The correlation between the two <lb/>variables in Figure 4.6 is -0.81 (R 2 =0.66) with p&lt;0.001. Note that, among the 22 countries, Sweden has the <lb/>second highest content advantage index in favour of PISA. This offsets the fact that students in Sweden <lb/>have fewer years of schooling at the time of PISA testing (and therefore a higher age cohort in TIMSS). <lb/>Predicting PISA Mathematics Country Mean Scores <lb/>195. <lb/>In the previous sections, two factors have been separately identified to have an association with <lb/>the observed differences between countries&apos; TIMSS and PISA mean scores: Content Balance and Age at <lb/>time of TIMSS testing (or Years of schooling at time of PISA testing). In this section, we explore the <lb/>combined impact of these two factors on the differences between PISA and TIMSS mean scores. To do <lb/>this, multiple regressions were carried out. To formulate the regression models, first consider a theoretical <lb/>relationship: <lb/>PISA = TIMSS + (PISA – TIMSS) <lb/>where PISA denotes a country&apos;s PISA mathematics mean score, and TIMSS denotes that country&apos;s TIMSS <lb/>mathematics mean score. Consequently, the regression models can be formulated with PISA mathematics <lb/>country mean score as the dependent variable to be predicted, and TIMSS mathematics country mean score <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>69 <lb/></page>

			<body>as the first predictor (independent variable), plus other predictor variables that have a relationship with <lb/>(PISA – TIMSS) country mean scores. <lb/>196. <lb/>Table 4.8 shows a summary of the multiple regression models used. The simplest model (model <lb/>1) is to predict PISA mathematics country mean scores using only TIMSS mathematics country mean <lb/>scores as a predictor. As discussed at the beginning of this Chapter, the correlation between PISA and <lb/>TIMSS country mean scores is 0.84. The percentage of variance of PISA mathematics country mean scores <lb/>explained by the regression model is 71%. That is, around 30% of the variance of PISA country mean <lb/>scores cannot be explained by their TIMSS country mean scores. Under this regression model, we can <lb/>compute the predicted PISA mean score for each country and compare it with the observed score as <lb/>reported in the PISA international report (OECD, 2004). With TIMSS mathematics score as the only <lb/>predictor, two out of the 22 countries have a predicted PISA mathematics score within the confidence <lb/>interval of the reported PISA score. <lb/>197. <lb/>For regression model 2, two more variables are added as predictors: TIMSS Age and Content <lb/>advantage index. Note that TIMSS Age is used as a proxy for Years of schooling in PISA. The percentage <lb/>of variance of PISA mathematics scores explained is 93%, showing a large improvement from regression <lb/>model 1. Under regression model 2, nine out of the 22 countries have a predicted PISA mathematics score <lb/>within the confidence interval of the reported PISA score. <lb/>198. <lb/>As there is more reading demand in PISA mathematics items, it is worth exploring whether PISA <lb/>reading country mean score is a useful predictor for PISA mathematics scores. Regression model 3 <lb/>explores the relationships between PISA mathematics country mean scores and PISA reading country <lb/>mean scores. Quite surprisingly, there is a very high correlation (R=0.95) between PISA mathematics <lb/>country mean scores and PISA reading country mean scores. Under this regression model, five out of 22 <lb/>countries have a predicted PISA mathematics score within the confidence interval of the reported PISA <lb/>score. <lb/>199. <lb/>Regression model 4 uses all four predictors: TIMSS Mathematics country mean score, TIMSS <lb/>Age, Content advantage index, and PISA Reading score. Under this model, 97% of the variance of PISA <lb/>mathematics scores can be explained. Further, 11 out of the 22 countries have a predicted PISA <lb/>mathematics score within the confidence interval of the reported PISA score. Table 4.9 shows the predicted <lb/>PISA mathematics scores as compared to the reported scores. It can be seen that, for most countries, the <lb/>difference between the predicted score and the reported score is less than 10. <lb/>200. <lb/>The four regression models shown in Table 4.8 are not the only models that can be fitted. PISA <lb/>science country mean scores are also highly correlated with PISA mathematics country mean scores <lb/>(R=0.97). TIMSS science scores also have a moderately high correlation with PISA mathematics scores <lb/>(R=0.89). However, the purpose of this section is not so much as to predict PISA mathematics scores per <lb/>se. The purpose is to illustrate how PISA and TIMSS mathematics scores and other factors are inter-<lb/>related. <lb/>201. <lb/>It should be noted that different regression models will show different sets of countries as having <lb/>the best predicted scores. Consequently, Table 4.9 should not be used to make judgements about specific <lb/> &quot; outlier &quot; countries. If different predictors are used, different countries will be outliers. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>70 <lb/></page>

			<body>Table 4.8 Regression models for predicting PISA mathematics country mean scores <lb/>Table 4.9 Comparisons between Reported and Predicted Country Mean Scores <lb/>Country <lb/>Reported <lb/>mean score <lb/>Predicted mean <lb/>score from <lb/>regression <lb/>model 4 <lb/>Difference <lb/>between <lb/>reported and <lb/>predicted <lb/>scores <lb/>Predicted score <lb/>is within the <lb/>confidence <lb/>interval of the <lb/>reported score <lb/>Australia <lb/>524 <lb/>526 <lb/>-2 <lb/>񮽙 <lb/>Belgium Flemish <lb/>553 <lb/>544 <lb/>9 <lb/>Canada-Ontario, <lb/>531 <lb/>539 <lb/>-8 <lb/>Canada-Quebec <lb/>541 <lb/>541 <lb/>0 <lb/>񮽙 <lb/>England <lb/>507 <lb/>506 <lb/>1 <lb/>񮽙 <lb/>Hong Kong -China <lb/>550 <lb/>546 <lb/>4 <lb/>񮽙 <lb/>Hungary <lb/>490 <lb/>497 <lb/>-7 <lb/>Indonesia <lb/>360 <lb/>371 <lb/>-11 <lb/>Italy <lb/>466 <lb/>479 <lb/>-13 <lb/>Japan <lb/>534 <lb/>533 <lb/>1 <lb/>񮽙 <lb/>Korea <lb/>542 <lb/>557 <lb/>-15 <lb/>Latvia <lb/>483 <lb/>483 <lb/>0 <lb/>񮽙 <lb/>New Zealand <lb/>523 <lb/>515 <lb/>8 <lb/>Norway <lb/>495 <lb/>494 <lb/>1 <lb/>񮽙 <lb/>Russian Federation <lb/>468 <lb/>458 <lb/>10 <lb/>Scotland <lb/>524 <lb/>521 <lb/>3 <lb/>񮽙 <lb/>Slovak Republic <lb/>498 <lb/>479 <lb/>19 <lb/>Spain-Basque country <lb/>502 <lb/>492 <lb/>10 <lb/>Sweden <lb/>509 <lb/>506 <lb/>3 <lb/>񮽙 <lb/>The Netherlands <lb/>538 <lb/>534 <lb/>4 <lb/>񮽙 <lb/>Tunisia <lb/>359 <lb/>355 <lb/>4 <lb/>񮽙 <lb/>United States <lb/>483 <lb/>502 <lb/>-19 <lb/>16 <lb/>The variable Age at time of TIMSS test is used as a proxy for Years of schooling at time of PISA test. <lb/>Regression <lb/>model <lb/>To Predict <lb/>(Dependent variable) <lb/>Predictor(s) <lb/>(Independent variables) <lb/>Percentage of <lb/>variance <lb/>explained (R 2 ) <lb/>Correlation (R) <lb/>1 <lb/>PISA mathematics <lb/>TIMSS Mathematics <lb/>71% <lb/>0.84 <lb/>2 <lb/>PISA mathematics <lb/>TIMSS Mathematics <lb/>TIMSS Age 16 <lb/>Content advantage index <lb/>93% <lb/>0.97 <lb/>3 <lb/>PISA mathematics <lb/>PISA Reading <lb/>91% <lb/>0.95 <lb/>4 <lb/>PISA mathematics <lb/>TIMSS Mathematics <lb/>TIMSS Age <lb/>Content advantage index <lb/>PISA Reading <lb/>97% <lb/>0.99 <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>71 <lb/></page>

			<body>Implications of differential performance of countries in content domains <lb/>202. <lb/>The findings from previous sections show that differential performance of countries in content <lb/>domains can explain, to a large extent, different country rankings in PISA and TIMSS. An implication of <lb/>this finding is that the reported combined mathematics score must be interpreted in relation to the <lb/>composition of the test in terms of content balance. For example, if a test consists only of TIMSS Data <lb/>items, then Sweden ranks 7th out of the 22 countries considered above. If a test consists only of TIMSS <lb/>Algebra items, then Sweden ranks 18 th out of the 22 countries. In contrast, out of 22 countries, Hungary <lb/>ranks 13th in TIMSS Data content domain, but 4 th in TIMSS Algebra content domain. Consequently, tests <lb/>consisting of different balance of content domains will likely to produce different rankings of countries. <lb/>Any statement about how a country performed in &quot; mathematics &quot; must be carefully interpreted. <lb/>203. <lb/>One may argue that the performance of countries at the level of content domains may be more <lb/>informative. As PISA and TIMSS have different content classifications, it will be difficult to cross-check <lb/>results at the content domain level. Of the five TIMSS content domains and four PISA overarching ideas, <lb/>the best match is perhaps between TIMSS Data domain and PISA Uncertainty overarching idea (see <lb/>Chapter 3 for comparisons of the PISA and TIMSS frameworks). The correlation between country mean <lb/>scores in TIMSS Data content domain and PISA Uncertainty overarching idea is 0.93 for the 22 countries. <lb/>This is much higher than the correlation between the combined mathematics scores (0.84). This suggests <lb/>that, if the contents of the tests are aligned, the results will be more similar. For other TIMSS content <lb/>domains and PISA overarching ideas, it is difficult to form a one-to-one match between TIMSS and PISA. <lb/>TIMSS Data domain and PISA Uncertainty overarching idea <lb/>204. <lb/>It is worthwhile taking a closer look at the Data/Uncertainty content domain, as PISA has <lb/>considerably more coverage of this content domain (31%) than TIMSS has (14%), and it appears that <lb/>Western countries have relatively more strengths in this content domain (as compared to the other content <lb/>domains) than Eastern European and Asian countries have. One may suggest that the Data/Uncertainly <lb/>domain has a prominence in the PISA test because the need to represent and interpret data is becoming <lb/>more and more important in everyday lives of the citizens, whether it is reading the newspaper, <lb/>advertisements, or other forms of communication. Reading graphs and interpreting charts is part of our <lb/>lives, and not just a school subject. It is therefore hypothesised that skills and knowledge in the <lb/>Data/Uncertainty domain may be closely related to those in the reading domain, as one area of the reading <lb/>domain is about document reading. Table 4.10 shows the correlations between country mean scores in <lb/>TIMSS content domains and PISA reading. <lb/>Table 4.10 Correlation between country mean scores in PISA Reading and TIMSS content domains <lb/>Correlation with PISA <lb/>Reading <lb/>TIMSS Number <lb/>0.65 <lb/>TIMSS Algebra <lb/>0.62 <lb/>TIMSS Measurement <lb/>0.79 <lb/>TIMSS Geometry <lb/>0.57 <lb/>TIMSS Data <lb/>0.91 <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>72 <lb/></page>

			<body>205. <lb/>From Table 4.10, it can be seen that the Data domain stands out as one that is highly correlated <lb/>with Reading. Since the PISA mathematics test has nearly one third of the items in the Data/Uncertainty <lb/>domain, it is not surprising that there is a high correlation between PISA mathematics and PISA reading <lb/>scores. It is also not surprising that country rankings are somewhat difference between TIMSS and PISA, <lb/>as PISA mathematics, on balance, is testing something a little different from what TIMSS tests. <lb/>Examining the spread of PISA and TIMSS achievement distributions <lb/>206. <lb/>The comparisons in the previous sections focus on the differences between country mean scores. <lb/>While the mean score of a country provides one measure of overall performance of a country, it does not <lb/>provide a complete picture of country performance. For example, it is often of interest to know how low <lb/>and high achievers differ within a country and across countries. In this chapter, we examine characteristics <lb/>of the achievement distributions other than mean scores, such as the spread of the distributions and <lb/>percentile points. <lb/>Standard Deviations <lb/>207. <lb/>Standard deviation is a measure of dispersion (or spread) of a set of data values. The larger the <lb/>standard deviation, the more spread out the data values are (See Box 4.1 for an explanation). In PISA and <lb/>TIMSS, the mean and standard deviation of mathematics achievement scores are reported by country. <lb/>Table 4.11 shows the standard deviations for the 22 countries that participated in both surveys. The last <lb/>column of Table 4.11 shows PISA standard deviation in TIMSS unit, using the transformation 17 as <lb/>described earlier in this chapter. The countries are arranged in order of their TIMSS standard deviation, <lb/>from smallest to largest. If two surveys have the same population definition and similar composition of <lb/>items, one would expect the standard deviations in the two surveys to have a high correlation. That is, if <lb/>the mathematics abilities of students in country X are more spread out than those in other countries, then <lb/>this should be reflected in the standard deviation values in both surveys. A scan down the column of <lb/>standard deviations in PISA shows that there is not a very strong relationship between TIMSS standard <lb/>deviation and PISA standard deviation. <lb/>208. <lb/>Figure 4.7 shows a plot of TIMSS standard deviation and PISA standard deviation in TIMSS <lb/>unit. The correlation between these two variables across the countries is 0.22, a somewhat weak <lb/>correlation. There are two notable outliers in this graph: Indonesia 18 and Canada–Quebec. The other <lb/>countries seem to lie somewhere in between these two outliers, with a stronger positive correlation (0.55). <lb/>The dotted line in Figure 4.7 shows the equality line where TIMSS standard deviation equals PISA <lb/>standard deviation. It can be seen that 18 out of 22 countries have larger standard deviations in PISA than <lb/>in TIMSS. A possible explanation for this is that the PISA samples contain students from multiple grades, <lb/>so there may be a wider spread of mathematics abilities. Of course there are also many other possible <lb/>reasons for differences in the magnitude of standard deviations between the two surveys. Since the content <lb/>17 <lb/>More specifically, the transformation used was: <lb/>PISA standard deviation in TIMSS unit = ( PISA standard deviation / 51.92) * 45.08, <lb/>where 51.92 was the standard deviation of the PISA country mean scores of the 22 countries participated in <lb/>both PISA and TIMSS, and 45.08 was the standard deviation of the TIMSS country mean scores. <lb/>18 <lb/>In fact, Indonesia has the highest standard deviation in TIMSS and lowest standard deviation in PISA <lb/>among the 22 countries. We have sought explanations for this, and it appears that the sampling base (types <lb/>of schools) may not be the same in the two surveys in Indonesia. A thorough investigation of this is outside <lb/>the scope of this report. But there are some indications that the PISA and TIMSS samples do not reflect the <lb/>same population groups in Indonesia, over and above the difference in grade-based and age-based sampling <lb/>in TIMSS and PISA. <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>73 <lb/></page>

			<body>balance and the items are different in the two surveys, one test may spread students out more than the <lb/>other. However, as PISA has a slightly lower reported reliability than TIMSS&apos; reported reliability, it <lb/>appears unlikely that PISA test should spread students out more than TIMSS test. <lb/>Table 4.11 PISA and TIMSS standard deviations for the 22 countries <lb/>TIMSS <lb/>standard <lb/>deviation <lb/>PISA standard deviation <lb/>as reported <lb/>PISA standard deviation <lb/>transformed to TIMSS <lb/>unit <lb/>Quebec, Canada <lb/>58 <lb/>93 <lb/>81 <lb/>Tunisia <lb/>60 <lb/>82 <lb/>71 <lb/>Basque country, Spain <lb/>64 <lb/>82 <lb/>72 <lb/>Ontario, Canada <lb/>66 <lb/>83 <lb/>72 <lb/>The Netherlands <lb/>69 <lb/>93 <lb/>80 <lb/>Norway <lb/>71 <lb/>92 <lb/>80 <lb/>Sweden <lb/>71 <lb/>95 <lb/>82 <lb/>Hong Kong -China <lb/>72 <lb/>100 <lb/>87 <lb/>Belgium Flemish <lb/>73 <lb/>105 <lb/>91 <lb/>Latvia <lb/>73 <lb/>88 <lb/>76 <lb/>Scotland <lb/>75 <lb/>84 <lb/>73 <lb/>England <lb/>77 <lb/>93 <lb/>81 <lb/>Italy <lb/>77 <lb/>96 <lb/>83 <lb/>Russian Federation <lb/>77 <lb/>92 <lb/>80 <lb/>New Zealand <lb/>78 <lb/>98 <lb/>85 <lb/>Hungary <lb/>80 <lb/>94 <lb/>81 <lb/>Japan <lb/>80 <lb/>101 <lb/>87 <lb/>United States <lb/>80 <lb/>95 <lb/>83 <lb/>Australia <lb/>82 <lb/>95 <lb/>83 <lb/>Slovak Republic <lb/>82 <lb/>93 <lb/>81 <lb/>Korea <lb/>84 <lb/>92 <lb/>80 <lb/>Indonesia <lb/>89 <lb/>81 <lb/>70 <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>74 <lb/></page>

			<body>90.00 <lb/>85.00 <lb/>80.00 <lb/>75.00 <lb/>70.00 <lb/>PISAStdevTU <lb/>90.00 <lb/>85.00 <lb/>80.00 <lb/>75.00 <lb/>70.00 <lb/>65.00 <lb/>60.00 <lb/>55.00 <lb/>TIMSSStdev <lb/>QUE <lb/>ONT <lb/>BSQ <lb/>USA <lb/>TUN <lb/>SWE <lb/>SVK <lb/>SCO <lb/>RUS <lb/>NZL <lb/>NOR <lb/>NLD <lb/>LVA <lb/>KOR <lb/>JPN <lb/>ITA <lb/>IDN <lb/>HUN <lb/>HKG <lb/>ENG <lb/>BFL <lb/>AUS <lb/>Figure 4.7 TIMSS standard deviation versus PISA standard deviation (in TIMSS unit) <lb/>Percentiles <lb/>209. <lb/>To examine the distributions more closely, the 5 th and 95 th percentile points are shown in Table <lb/>4.12. Figure 4.8 shows a country-by-country comparison of PISA and TIMSS percentile points. For each <lb/>country, two bars are shown. The first bar shows the PISA results, and the second bar shows TIMSS <lb/>results. The top end of each bar shows the 95 th percentile point, and the bottom end of each bar shows the <lb/>5 th percentile point. The circle in the middle shows country mean score. For example, the pair of bars for <lb/>Australia shows that the spread of the achievement distribution of Australian students in PISA is similar to <lb/>the spread of TIMSS achievement distribution, but the whole PISA distribution is shifted upwards as <lb/>compared to TIMSS distribution. In contrast, for Hong Kong, the spread of the PISA distribution is much <lb/>larger than the spread of the TIMSS distribution, and the whole PISA ability distribution is shifted <lb/>downwards as compare to the TIMSS distribution. <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>75 <lb/></page>

			<body>Table 4.12 PISA and TIMSS 95 <lb/>th and 5 <lb/>th percentiles for countries participating in both Surveys in 2003 <lb/>TIMSS 5 th <lb/>percentile <lb/>TIMSS <lb/>95 th <lb/>percentile <lb/>PISA 5 th <lb/>percentile <lb/>PISA 95 th <lb/>percentile <lb/>PISA 5 th <lb/>percentile <lb/>(in TIMSS <lb/>unit) <lb/>PISA 95 th <lb/>percentile <lb/>(in TIMSS <lb/>unit) <lb/></body>

			<div type="annex">Australia <lb/>368 <lb/>634 <lb/>364 <lb/>676 <lb/>391 <lb/>662 <lb/>Basque country, Spain <lb/>379 <lb/>591 <lb/>361 <lb/>631 <lb/>389 <lb/>623 <lb/>Belgium Flemish <lb/>398 <lb/>643 <lb/>360 <lb/>707 <lb/>388 <lb/>689 <lb/>England <lb/>373 <lb/>627 <lb/>354 <lb/>660 <lb/>383 <lb/>648 <lb/>Hong Kong -China <lb/>455 <lb/>691 <lb/>374 <lb/>700 <lb/>400 <lb/>682 <lb/>Hungary <lb/>398 <lb/>656 <lb/>335 <lb/>644 <lb/>366 <lb/>634 <lb/>Indonesia <lb/>266 <lb/>558 <lb/>233 <lb/>499 <lb/>278 <lb/>508 <lb/>Italy <lb/>355 <lb/>606 <lb/>307 <lb/>623 <lb/>342 <lb/>616 <lb/>Japan <lb/>433 <lb/>697 <lb/>361 <lb/>690 <lb/>388 <lb/>674 <lb/>Korea <lb/>439 <lb/>715 <lb/>388 <lb/>690 <lb/>412 <lb/>674 <lb/>Latvia <lb/>386 <lb/>625 <lb/>339 <lb/>626 <lb/>370 <lb/>619 <lb/>New Zealand <lb/>364 <lb/>623 <lb/>358 <lb/>682 <lb/>386 <lb/>667 <lb/>Norway <lb/>340 <lb/>573 <lb/>343 <lb/>645 <lb/>373 <lb/>635 <lb/>Ontario, Canada <lb/>411 <lb/>628 <lb/>390 <lb/>665 <lb/>413 <lb/>653 <lb/>Quebec, Canada <lb/>449 <lb/>640 <lb/>378 <lb/>682 <lb/>403 <lb/>667 <lb/>Russian Federation <lb/>381 <lb/>632 <lb/>319 <lb/>622 <lb/>352 <lb/>615 <lb/>Scotland <lb/>368 <lb/>615 <lb/>380 <lb/>660 <lb/>405 <lb/>648 <lb/>Slovak Republic <lb/>371 <lb/>642 <lb/>342 <lb/>648 <lb/>372 <lb/>638 <lb/>Sweden <lb/>378 <lb/>614 <lb/>353 <lb/>662 <lb/>381 <lb/>650 <lb/>The Netherlands <lb/>417 <lb/>644 <lb/>385 <lb/>683 <lb/>409 <lb/>668 <lb/>Tunisia <lb/>316 <lb/>515 <lb/>229 <lb/>501 <lb/>274 <lb/>510 <lb/>United States <lb/>369 <lb/>635 <lb/>323 <lb/>638 <lb/>355 <lb/>629 <lb/></div>

			<body>EDU/WKP(2010)5 <lb/>76 <lb/>QUE <lb/>ONT <lb/>BSQ <lb/>USA <lb/>TUN <lb/>SWE <lb/>SVK <lb/>SCO <lb/>RUS <lb/>NZL <lb/>NOR <lb/>NLD <lb/>LVA <lb/>KOR <lb/>JPN <lb/>ITA <lb/>IDN <lb/>HUN <lb/>HKG <lb/>ENG <lb/>BFL <lb/>AUS <lb/>CountryC <lb/>800 <lb/>700 <lb/>600 <lb/>500 <lb/>400 <lb/>300 <lb/>200 <lb/>TIMSSMaths <lb/>PISAMathsTU <lb/>TIMSS95thPC <lb/>TIMSS5thPC <lb/>PISA95thTU <lb/>PISA5thTU <lb/>Figure 4.8 Comparison of PISA and TIMSS 95 <lb/>th and 5 <lb/>th percentile points <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>77 <lb/></page>

			<body>210. <lb/>Factors that have an impact on the differential spread of ability distributions in PISA and TIMSS <lb/>will be difficult to identify, as there are likely between-country differences in terms of characteristics of <lb/>samples and advantages/disadvantages in content balance. One can only make some hypotheses. For <lb/>example, in Hong Kong and Tunisia, the spread of the achievement distribution is considerably larger in <lb/>PISA than in TIMSS. One possible reason for this is that, in both Hong Kong and Tunisia, the PISA <lb/>sample contains many students from grades lower than the typical grade of 15 year-olds. Figure 4.9 shows <lb/>the histograms of grade distribution for Hong Kong and Tunisia. <lb/>Hong Kong <lb/>Tunisia <lb/>12 <lb/>11 <lb/>10 <lb/>9 <lb/>8 <lb/>7 <lb/>6 <lb/>Grade Q1a <lb/>50000.0000 <lb/>40000.0000 <lb/>30000.0000 <lb/>20000.0000 <lb/>10000.0000 <lb/>0.0000 <lb/>Frequency <lb/>Mean =9.38 <lb/>Std. Dev. =0.871 <lb/>N =72,484 <lb/>Adjudicated sub-region: Hong Kong SAR <lb/>Cases weighted by Student final weight <lb/>12 <lb/>11 <lb/>10 <lb/>9 <lb/>8 <lb/>7 <lb/>6 <lb/>Grade Q1a <lb/>60000.0000 <lb/>50000.0000 <lb/>40000.0000 <lb/>30000.0000 <lb/>20000.0000 <lb/>10000.0000 <lb/>0.0000 <lb/>Frequency <lb/>Mean =8.88 <lb/>Std. Dev. =1.133 <lb/>N =150,875 <lb/>Adjudicated sub-region: Tunisia <lb/>Cases weighted by Student final weight <lb/>Figure 4.9 Grade distributions of Hong Kong and Tunisia PISA samples <lb/>211. <lb/>The results in this section show that the spread of ability distributions need to be interpreted with <lb/>regard to the population definition. If one makes a statement that students in country X are more similar in <lb/>ability than students in country Y, the underlying population definition must be referred to. The <lb/>comparisons between TIMSS and PISA show that, within a country, the variation in student abilities for a <lb/>grade level may be quite different from the variation in abilities for an age group. Further, in general, <lb/>variations of mathematics abilities for an age group tend to be larger than variations for a grade level. <lb/>The impact of calculator availability on differences in performance in TIMSS and PISA <lb/>212. <lb/>In Chapter 2, it was found that the percentages of students with access to a calculator differ <lb/>across countries, and differ between the two surveys. These differences do not have an impact on the <lb/>differential performance of countries in TIMSS and PISA. The correlation between these two variables is <lb/>0.106 (p=0.676). That is, the different policies on calculator use in TIMSS and PISA do not explain the <lb/>differences in country rankings in TIMSS and PISA. <lb/>Summary <lb/>213. <lb/>This chapter explores the factors that may have an impact on the observed differences between <lb/>country mean scores in PISA and TIMSS. It is found that the age at testing in TIMSS has a positive <lb/>relationship with the differential performance in TIMSS and PISA. That is, countries with an older cohort <lb/>of students tend to perform relatively better in TIMSS than in PISA. However, this does not mean that <lb/>better performance in TIMSS is necessarily due to older students in the TIMSS sample. In fact, there is a <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>78 <lb/></page>

			<body>strong negative correlation between age at time of testing in TIMSS and years of schooling at time of <lb/>testing in PISA. The older the students are in the TIMSS sample for a country, the fewer number of years <lb/>of schooling the students have at time of testing in PISA. Therefore, a relatively better performance in <lb/>PISA could be due to more years of schooling at time of testing in PISA. <lb/>214. <lb/>Further, a Content Advantage Index was constructed based on each country&apos;s performance in the <lb/>five content areas of mathematics in TIMSS, and the relative proportions of items in the content areas in <lb/>each survey. Given that the content balance of PISA is quite different from that of TIMSS, some countries <lb/>have more advantage in PISA and some have more advantage in TIMSS, depending on their relative <lb/>strengths and weaknesses in the content areas. It was shown that the differential performance of countries <lb/>in PISA and TIMSS was closely related to the content balance of each survey. <lb/>215. <lb/>Using three variables (TIMSS mathematics country mean score, Age at time of TIMSS testing, <lb/>Content Advantage Index) as predictors for PISA mathematics country mean scores, it was found that 93% <lb/>of the variance of PISA mathematics scores could be explained by these three predictors. Further, as there <lb/>is more reading demand in the PISA mathematics tests, if PISA reading score is used as an additional <lb/>explanatory variable, then 97% of the variance of the PISA mathematics scores can be explained. With <lb/>these four predictor variables, 11 out of the 22 countries have a predicted PISA mathematics score within <lb/>the confidence interval of the reported PISA score. <lb/>216. <lb/>The standard deviations of achievement distributions in PISA are generally larger than the <lb/>standard deviations in TIMSS. This is likely the result of different population definitions where PISA <lb/>samples contain students from multiple grades while TIMSS controls for the grade level. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>79 <lb/></page>

			<body>CHAPTER 5 -GENDER DIFFERENCE AND ATTITUDES <lb/>Introduction <lb/>217. <lb/>It will be of interest to compare the results drawn from PISA and TIMSS in terms of the <lb/>performance of subgroups of students, such as girls and boys. In addition, both PISA and TIMSS <lb/>collected data on student home background information and attitudes towards mathematics. A comparison <lb/>of the similarities or differences in the findings will be interesting. If the same results are found, then each <lb/>study provides some evidence of validity for the other study. If different results are found, it will be <lb/>interesting to investigate why there are differences, and, in doing so, one hopes to gain a better <lb/>understanding of the results of each study. <lb/>Gender differences <lb/>Gender differences for overall mathematics scale <lb/>218. <lb/>Do PISA and TIMSS arrive at the same conclusions about the performance of girls and boys? On <lb/>the surface, the answer seems to be &quot; No &quot; . <lb/>219. <lb/>TIMSS found that <lb/>On average, across all countries, there was essentially no difference in achievement between <lb/>boys and girls at either the eighth or fourth grade, although the situation varied from country to <lb/>country. (IEA, 2003, p.47) <lb/>220. <lb/>In contrast, in PISA, apart from two countries (Iceland and Thailand) where girls&apos; mean <lb/>mathematics score is higher than that for boys, in all other 38 countries, boys&apos; mean score is higher, <lb/>although not all significant statistically. Figure 5.1 shows pictorially the differences between the <lb/>performance of girls and boys, for TIMSS and PISA, where darkened bars indicate statistical significance <lb/>(extracted from TIMSS report, p48, (IEA, 2003), and PISA report, p97, (OECD, 2004)). <lb/>221. <lb/>On first impression, there appears to be a discrepancy between TIMSS and PISA results of <lb/>gender differences across countries. In TIMSS, there are about equal numbers of countries where girls <lb/>performed better and where boys performed better. On the other hand, in PISA, boys performed better in <lb/>the majority of countries. If one uses TIMSS results, one might conclude that, at Grade eight, there is no <lb/>clear gender difference in mathematics performance. If one uses PISA results, one might conclude that <lb/>15 year-old girls lag behind 15 year-old boys in mathematics performance among OECD countries 19 . <lb/>19. <lb/>In 26 out of 40 countries in PISA, boys&apos; performance is statistically significantly better than girls&apos; <lb/>performance. However, collectively for the OECD population, 38 out of 40 countries showed that the mean <lb/>score for boys is higher than the mean score for girls. This is extremely significant statistically; (Consider <lb/>tossing a coin 40 times and obtain a head 38 times. There is little doubt that the coin is biased.) <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>80 <lb/></page>

			<body>Figure 5.1 Gender Difference in TIMSS and PISA for overall mathematics scale (Extract from <lb/>TIMSS and PISA reports) <lb/>TIMSS <lb/>PISA <lb/>222. <lb/>On closer examination, all the countries where girls performed better in TIMSS have not <lb/>participated in PISA. These countries are Serbia, Macedonia, Armenia, Moldova, Singapore, Philippines, <lb/>Cyprus, Jordan and Bahrain. None of these countries is an OECD member. On the other hand, in TIMSS, <lb/>in nine countries boys performed better including the OECD member countries the Flemish Community of <lb/>Belgium, Hungary, Italy and the United States (the other countries were Chile, Ghana, the Lebanon, <lb/>Morocco and Tunisia). So it appears that gender difference varies between countries, as noted in both the <lb/>TIMSS and PISA reports, and that different compositions of participating countries could provide a <lb/>different picture of overall gender difference in a study. For valid comparisons, one needs to look at the <lb/>same group of countries. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>81 <lb/></page>

			<body>223. <lb/>Figure 5.2 shows gender differences for the 22 countries that participated in both PISA and <lb/>TIMSS. <lb/>Figure 5.2 Comparisons of gender differences in PISA and TIMSS <lb/>-10 <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>LVA <lb/>HKG <lb/>IDN <lb/>AUS <lb/>NLD <lb/>NOR <lb/>SWE <lb/>USA <lb/>ENG <lb/>SCO <lb/>QUE <lb/>HUN <lb/>JPN <lb/>BSQ <lb/>RUS <lb/>TUN <lb/>ONT <lb/>BFL <lb/>NZL <lb/>ITA <lb/>SVK <lb/>KOR <lb/>Male mean score -Female mean score (in TIMSS score <lb/>unit) <lb/>PISA gender <lb/>difference <lb/>TIMSS gender <lb/>difference <lb/>224. <lb/>In Figure 5.2, the countries are arranged in descending order of the magnitude of gender <lb/>difference in PISA (in TIMSS score unit). For each country, the first bar shows gender difference (in mean <lb/>scores) in PISA, and the second bar immediately below the first bar shows gender difference (in mean <lb/>scores) in TIMSS for that country. If there is a high correlation between PISA and TIMSS gender <lb/>differences, one would expect the second bar for each country to be longest (and positive) at the top of <lb/>the graph, and decreases in length or becoming negative, as for the bars for PISA. However, it can be <lb/>seen that TIMSS gender differences do not diminish as one scans down the graph, as PISA gender <lb/>differences diminish. The correlation between PISA and TIMSS gender differences is 0.23, indicating <lb/>that there is a weak relationship between the two variables. Consequently, one cannot conclude that there is <lb/>a strong agreement between PISA and TIMSS results in terms of gender difference for the overall <lb/>mathematics scale. <lb/>225. <lb/>It should also be noted that Figure 5.2 shows that, on average, gender difference is larger in PISA <lb/>than in TIMSS, from a visual comparison of the size and direction of the two bars for each country. That is, <lb/>on average, boys outperformed girls by a greater amount in PISA than in TIMSS. On average, gender <lb/>differences in TIMSS are not only smaller in magnitude, but girls outperformed boys in a number of <lb/>countries. <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>82 <lb/></page>

			<body>Gender differences by mathematics content areas <lb/>226. <lb/>A comparison of the composition of PISA and TIMSS tests in terms of the balance of content <lb/>domains was presented in Chapter 3. It was shown that there was a significant difference in terms of the <lb/>proportion of items from each traditional content area. How do gender differences vary across mathematics <lb/>content areas? Both TIMSS and PISA found that gender difference is not uniform across mathematics <lb/>content areas. In TIMSS, girls performed better in Algebra, while boys performed better in Measurement. <lb/>In Algebra, girls performed significantly better than boys in 23 countries/regions, while boys <lb/>performed better in only 3 countries/regions. In measurement, boys performed significantly better than <lb/>girls in 15 countries/regions, while girls performed better in only 2 countries/regions. For Number, <lb/>Geometry and Data, the difference in performance between gender groups is not as great as for Algebra <lb/>and for Measurement. However, there is some suggestion that boys performed a little better than girls <lb/>did, on average, in these three content areas. Table 5.1 provides a tally of the number of <lb/>countries/regions in terms of differential performance between boys and girls. <lb/>Table 5.1 Comparison of performance of boys and girls in TIMSS by content area <lb/>Mathematics content area <lb/>Number of <lb/>countries/regions in which <lb/>BOYS PERFORMED <lb/>BETTER <lb/>Number of <lb/>countries/regions in which <lb/>GIRLS PERFORMED <lb/>BETTER <lb/>Number of <lb/>countries/regions in which <lb/>there is NO DIFFERENCE <lb/>between performance of <lb/>boys and girls <lb/>Number <lb/>14 <lb/>10 <lb/>26 <lb/>Algebra <lb/>3 <lb/>23 <lb/>24 <lb/>Measurement <lb/>15 <lb/>2 <lb/>33 <lb/>Geometry <lb/>13 <lb/>8 <lb/>29 <lb/>Data <lb/>9 <lb/>8 <lb/>33 <lb/>227. <lb/>For PISA, gender differences are examined by Overarching Ideas. Table 5.2 shows a summary of <lb/>the results. <lb/>Table 5.2 Comparison of performance of Boys and Girls in PISA by Overarching Ideas <lb/>Mathematics content area <lb/>Number of <lb/>countries/regions in which <lb/>BOYS PERFORMED <lb/>BETTER <lb/>Number of <lb/>countries/regions in which <lb/>GIRLS PERFORMED <lb/>BETTER <lb/>Number of <lb/>countries/regions in which <lb/>there is NO DIFFERENCE <lb/>between performance of <lb/>boys and girls <lb/>Space and shape <lb/>32 <lb/>1 <lb/>7 <lb/>Change and relationship <lb/>21 <lb/>1 <lb/>18 <lb/>Quantity <lb/>16 <lb/>1 <lb/>23 <lb/>Uncertainty <lb/>29 <lb/>2 <lb/>9 <lb/>228. <lb/>Although PISA and TIMSS have different proportions of countries in which boys outperformed <lb/>girls, there is still some consistency in the results from the two studies. If PISA&apos;s overarching idea Space <lb/>and Shape can be mapped onto TIMSS&apos; Measurement and Geometry domains (see Table 3.6), then both <lb/>studies found that boys outperformed girls by the greatest amount in this content area. <lb/>229. <lb/>In PISA, differential performance between boys and girls is least in the area of the overarching <lb/>idea Quantity (although boys still performed significantly better). As PISA&apos;s overarching idea Quantity <lb/>can be regarded as a subset of TIMSS&apos; Number content domain (see the Quantity section in Chapter 3), it <lb/>is observed that the gender difference for the content domain Number in TIMSS is also less than the <lb/>difference in Measurement. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>83 <lb/></page>

			<body>230. <lb/>In TIMSS, at the international level, the only content area in which girls outperformed boys is <lb/>Algebra. In PISA, only 7 items are classified as Algebra. So one might conjecture that most PISA items <lb/> &quot; favour &quot; boys, since there are not many Algebra items in PISA. This could explain, at least in part, the <lb/>difference in the findings from the two studies for the overall mathematics scale. That is, in TIMSS, there <lb/>was little gender difference in overall mathematics performance internationally, while a large difference in <lb/>PISA was found. <lb/>231. <lb/>In PISA, there is a large gender difference for the Overarching Idea Uncertainty (as compared to <lb/>the other Overarching Ideas), but in TIMSS, the content area Data showed less gender difference than, say, <lb/>for Number. Since Uncertainty could be regarded as part of the Data content area, there seems to be some <lb/>inconsistencies in the findings here. However, Data, in general, covers topics in statistics, while <lb/>Uncertainty deals more with Chance (as in Chance and Data). So it seems that Uncertainty in PISA is a <lb/>narrower domain than Data in TIMSS. Some aspects of Data in TIMSS may be classified as Change and <lb/>Relationships (e.g., graphing data) in PISA (see Table 3.6). So one might conclude that, for topics dealing <lb/>with Chance, or, more formally, Probability, boys tend to do a great deal better than girls, as found in <lb/>PISA, but not for Data. <lb/>232. <lb/>It should be noted that, while there are considerable variations across countries in gender <lb/>differences in both PISA and TIMSS, there is a consistent pattern of gender differences in the content <lb/>areas of mathematics. That is, in general, if one content area shows large gender difference <lb/>internationally, it is usually reflected in the gender difference within a country, where that content area <lb/>shows the largest gender difference relative to the other content areas within a country. This suggests that <lb/>there may be some underlying factors, whether biological or social, that differentiates between boys and <lb/>girls, so that boys may be naturally better than girls at certain tasks, and vice versa. For example, boys tend <lb/>to perform better in certain spatial tasks (Voyer, Voyer and Bryden, 1995). Gallagher, Levin and Cahalan <lb/>(2002) also found that males performed relatively better on items requiring the use of spatial <lb/>representations, but gender difference is small on items involving the use of mathematics language and <lb/>recall of knowledge. <lb/>233. <lb/>On the other hand, the vast variation in gender differences across countries indicates that gender <lb/>difference could be addressed, and there are many countries where gender difference in mathematics <lb/>performance is negligible. Further studies looking into how some countries address gender equity may be <lb/>useful. It should be noted that, while, in PISA, significant gender differences were found in mathematics, <lb/>the magnitude of the difference is much smaller than the gender difference found for Reading (OECD, <lb/>2005, p.98). For Reading, girls outperformed boys by a great deal more. From this point of view, it is <lb/>somewhat surprising that more gender difference was observed in PISA than in TIMSS, since PISA <lb/>mathematics items require more reading than TIMSS mathematics items, as many TIMSS items are short <lb/>and context free items. If reading &quot; gets in the way &quot; of successfully responding to a mathematics item, then <lb/>one would expect the gender difference in PISA mathematics to be less than that observed in TIMSS. <lb/>Possible explanations for observed differences between PISA and TIMSS in gender gap <lb/>234. <lb/>In summary, there could be four reasons why the observed gender difference in PISA is larger <lb/>than in TIMSS: <lb/>235. <lb/>First, in TIMSS and PISA reports, conclusions were drawn from an overall picture of gender <lb/>difference across countries. Since there are different countries participating in each study, the overall <lb/>pictures are somewhat different. An observation is that countries where girls&apos; performance is higher than <lb/>boys&apos; in TIMSS are not OECD member countries. It is possible that OECD countries, being more <lb/>developed than non-OECD countries in general, may have systematic similarities between themselves <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>84 <lb/></page>

			<body>(and differences from non-OECD countries) in education systems that exacerbate the gender difference in <lb/>mathematics achievement. <lb/>236. <lb/>Second, it has been shown that gender differences are not uniform across mathematics content <lb/>areas. Since PISA and TIMSS have different content compositions for the tests, gender differences for the <lb/>overall test are likely to be different for PISA and TIMSS, depending on whether there are more items <lb/>giving advantage to boys or to girls in a test. From this point of view, gender differences should be <lb/>interpreted with regard to the composition of a test, and not just for mathematics as a whole. <lb/>237. <lb/>There may also be subtle influences from the choice of particular contexts for individual items <lb/>which have not been detected by normal vetting procedures. <lb/>238. <lb/>Third, PISA items are more application oriented, while TIMSS items are more curriculum <lb/>oriented. This difference in the emphasis of the type of the items may have different effects on the <lb/>performance of girls and boys. Friedman (1989) found that girls are better at curriculum-based items, while <lb/>boys are better at problem-solving items. That is, girls are not as good as boys in applying mathematical <lb/>knowledge for functional use in everyday life. In PISA, the correlation between the problem solving <lb/>domain and the mathematics domain is a little higher for boys than for girls, indicating that there is a <lb/>slightly more consistent behaviour for boys than for girls in successfully (or unsuccessfully) answering <lb/>problem solving items and mathematics items in PISA. <lb/>239. <lb/>Fourth, the PISA report described growing gender gap in mathematics achievement as students <lb/>get older (Box 2.3, p96, OECD, 2004). Since PISA cohort has slightly older students than TIMSS&apos; cohort, <lb/>it is likely that gender differences for 15 to 16 years are greater than for 14-year-olds, as mathematics <lb/>becomes a more demanding subject. Friedman (1989) found this result in the meta-analysis and also <lb/>commented on supporting findings from previous literature. Most commonly with young children (e,g. up <lb/>to age 10) no differences are found, or if found they favour girls. In the junior high school years, Friedman <lb/>reported that a mixed pattern develops, with either no differences or differences favouring boys, with the <lb/>exception that very gifted mathematics students are more likely to be boys. Differences favouring males <lb/>increase in older age groups. For example, the Australian report of the TIMSS Population 3 advanced <lb/>mathematics group (Lokan and Greenwood, 2001) noted that in almost all countries and on all topics, there <lb/>were differences favouring males, often statistically significant, and these differences were much stronger <lb/>than in the TIMSS studies with younger students. <lb/>240. <lb/>All of the above four points suggest that explanations are worth further investigation to try to <lb/>understand gender differences in mathematics achievement, and to put in place measures to reduce gender <lb/>differences within those countries where gender differences are large. Friedman (1989) pointed to a social, <lb/>rather than a biological, explanation when commenting on the meta-analysis result that gender differences <lb/>had closed rapidly over two decades in the USA. This means that gender differences can be rectified with <lb/>appropriate intervention. <lb/>Gender difference in the spread of achievement distributions <lb/>241. <lb/>PISA noted that <lb/>Gender differences tend to be larger at the top end of the performance distribution. (OECD, <lb/>2005, p.98) <lb/>242. <lb/>That is, on average, there are more boys than girls at the top end of the scale. This can be readily <lb/>seen from Figure 5.3 where the percentages of boys and girls in Level 6 of the mathematics scale are <lb/>shown. Apart from Iceland where there are more girls than boys in Level 6, in every country where the <lb/>percentage of students in Level 6 is not zero, there are more boys than girls in Level 6. This finding is <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>85 <lb/></page>

			<body>consistent with that of Friedman (1989) who found that often gifted mathematics students are more likely <lb/>to be boys. In addition, in only 3 of 16 countries in the TIMSS study of advanced mathematics did more <lb/>females than males do advanced mathematics. <lb/>243. <lb/>For TIMSS, the percentages of students in benchmark levels are not reported separately for boys <lb/>and girls. However, the standard deviations of performance distributions for girls and boys are reported. <lb/>These are shown in Figure 5.4. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>86 <lb/></page>

			<body>Figure 5.3 Percentages of boys and girls in Level 6 of PISA mathematics scale <lb/>0.0 <lb/>2.0 <lb/>4.0 <lb/>6.0 <lb/>8.0 <lb/>10.0 <lb/>12.0 <lb/>14.0 <lb/></body>

			<div type="acknowledgement">Hong Kong-China <lb/>Japan <lb/>Belgium <lb/>Liechtenstein <lb/>Korea <lb/>Sw itzerland <lb/>New Zealand <lb/>Finland <lb/>Netherlands <lb/>Canada <lb/>Australia <lb/>Macao-China <lb/>Czech Republic <lb/>Germany <lb/>Denmark <lb/>Sw eden <lb/>Austria <lb/>France <lb/>Slovak Republic <lb/>Iceland <lb/>Norw ay <lb/>Luxembourg <lb/>Hungary <lb/>Poland <lb/>Turkey <lb/>Ireland <lb/>United States <lb/>Italy <lb/>Russian Federation <lb/>Latvia <lb/>Spain <lb/>Portugal <lb/>Greece <lb/>Uruguay <lb/>Brazil <lb/>Serbia <lb/>Thailand <lb/>Indonesia <lb/>Mexico <lb/>Tunisia <lb/>Percentage of students <lb/>% of males performing at Level 6 % of females performing at Level 6 <lb/>EDU/WKP(2010)5 <lb/></div>

			<page>87 <lb/></page>

			<body>Figure 5.4 TIMSS standard deviations of performance distribution in mathematics for boys and girls <lb/>0 <lb/>20 <lb/>40 <lb/>60 <lb/>80 <lb/>100 <lb/>120 <lb/></body>

			<div type="acknowledgement">So uth A frica <lb/>Chinese Taipei <lb/>Egypt <lb/>Ghana <lb/>Palestinian <lb/>Romania <lb/>Serbia <lb/>Jo rdan <lb/>M acedo nia, <lb/>Indo nesia <lb/>Israel <lb/>Philippines <lb/>Armenia <lb/>Korea, <lb/>Chile <lb/>Slovak Republic <lb/>Australia <lb/>B ulgaria <lb/>Cyprus <lb/>Singapo re <lb/>Japan <lb/>Lithuania <lb/>M oldova <lb/>New Zealand <lb/>United States <lb/>Hungary <lb/>Saudi Arabia <lb/>Bahrain <lb/>Italy <lb/>Russian Federation <lb/>England <lb/>Scotland <lb/>Iran <lb/>Latvia <lb/>M alaysia <lb/>B elgium (Fl.) <lb/>Hong Kong <lb/>Slo venia <lb/>B otswana <lb/>No rway <lb/>Sweden <lb/>Indiana <lb/>Netherlands <lb/>Esto nia <lb/>M oro cco <lb/>Basque <lb/>Lebanon <lb/>Ontario <lb/>Quebec <lb/>Tunisia <lb/>Standard deviation for girls Standard deviation for boys <lb/>244. <lb/>The standard deviation of the performance distribution in mathematics for boys is larger than <lb/>that for girls in 46 out of 50 countries/regions. So it appears that there is some consistency in the <lb/>findings of PISA and TIMSS in terms of the differences in the spread of performance distributions for <lb/>boys and girls. <lb/>EDU/WKP(2010)5 <lb/></div>

			<page>88 <lb/></page>

			<body>Attitudinal scales <lb/>245. <lb/>Both PISA and TIMSS collect information on students&apos; attitudes towards mathematics. In <lb/>particular, indices on &quot; self-confidence &quot; and &quot; interest and motivation &quot; have been constructed in both PISA <lb/>and TIMSS. <lb/>Self-confidence index <lb/>246. <lb/>Figure 5.5 shows a comparison of the self-confidence index in TIMSS and the self-concept index <lb/>in PISA. Note that the vertical axis shows the percentage of students who were assigned to the high level <lb/>of the TIMSS index. Box 6.1 details how each index was constructed. <lb/>247. <lb/>The correlation between PISA and TIMSS self-confidence indices is 0.73. Indonesia appears an <lb/>outlier in Figure 5.5. Without Indonesia, the correlation is 0.83. So there is a close relationship between the <lb/>two indices. Further, in both PISA and TIMSS, it was found that the South East Asian countries (Hong <lb/>Kong-China, Japan and Korea) had a low self-confidence index, despite the fact that student achievement <lb/>was high in these countries. On the other hand, English speaking countries tend to have a high self-<lb/>confidence index in both PISA and TIMSS. Both PISA and TIMSS found that, within each country, <lb/>students with higher self-confidence index performed better than students with lower self-confidence <lb/>index. <lb/>Box 6.1 Measuring students&apos; confidence in mathematics in PISA and TIMSS <lb/>TIMSS: Index of students&apos; self-confidence in learning mathematics (SCM) <lb/>This index is based on students&apos; responses to four statements about their mathematics ability: <lb/>•  I usually do well in mathematics;<lb/>•  Mathematics is more difficult for me than for many of my classmates * ;<lb/>•  Mathematics is not one of my strengths * ;<lb/>•  I learn quickly in mathematics.<lb/>Figure 5.5 presents results for the students with a high level of self-confidence on this index. These students <lb/>agreed a little or agreed a lot with all four of the above statements on average. <lb/>PISA: Index of students&apos; self-concept in mathematics <lb/>This index is based on students&apos; level of agreement with the following statements about their mathematics ability: <lb/>•  I am just not good at mathematics * ;<lb/>•  I get good marks in mathematics;<lb/>•  I learn mathematics quickly;<lb/>•  I have always believed that mathematics is one of my best subjects;<lb/>•  In my mathematics class, I understand even the most difficult work.<lb/>EDU/WKP(2010)5 <lb/></body>

			<page>89 <lb/></page>

			<body>Students could answer that they strongly agreed, agreed, disagreed or strongly disagreed with the above <lb/>statements. Positive values on this index indicate a positive self-concept in mathematics. <lb/>* The response categories for this statement were reversed in constructing the index. <lb/>Figure 5.5 Comparison of PISA and TIMSS self-confidence indices <lb/>0.25 <lb/>0.00 <lb/>-0.25 <lb/>-0.50 <lb/>PISA_SelfConcept <lb/>80.00 <lb/>60.00 <lb/>40.00 <lb/>20.00 <lb/>0.00 <lb/>TIMSS_SelfConf <lb/>QUE <lb/>ONT <lb/>USA <lb/>TUN <lb/>SWE <lb/>SVK <lb/>SCO <lb/>RUS <lb/>NZL <lb/>NOR <lb/>NLD <lb/>LVA <lb/>KOR <lb/>JPN <lb/>ITA <lb/>IDN <lb/>HUN <lb/>HKG <lb/>ENG <lb/>BFL <lb/>AUS <lb/>Interest and motivation indices <lb/>248. <lb/>In TIMSS, an index of &quot; Students&apos; Valuing Mathematics &quot; was constructed from questions about <lb/>students&apos; interest, enjoyment, and motivation about doing mathematics. In PISA, two separate indices were <lb/>constructed. The first is &quot; Students&apos; interest in and enjoyment of mathematics &quot; . The second is &quot; Students&apos; <lb/>instrumental motivation in mathematics &quot; . For the comparison shown here, PISA&apos;s second index is used. <lb/>Box 6.2 details the components of each index. As can be seen, with the exception of the first two <lb/>statements in the TIMSS index of valuing mathematics, the two measures are very similar in the way they <lb/>capture students&apos; motivation to learn mathematics for some external motivating factor whether it be <lb/>helping them in their current studies, future studies or indeed future work. The first two statements in the <lb/>TIMSS index of valuing mathematics are similar to components of PISA&apos;s index of interest and enjoyment <lb/>of mathematics, that is, they capture students&apos; enthusiasm for learning the subject itself. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>90 <lb/></page>

			<body>Box 6.2 Measuring students motivation to learn mathematics in PISA and TIMSS <lb/>TIMSS: Index of students valuing mathematics <lb/>This index is based on students&apos; responses to seven statements about mathematics: <lb/>•  I would like to take more mathematics in school;<lb/>•  I enjoy learning mathematics;<lb/>•  I think learning mathematics will help me in my daily life;<lb/>•  I need mathematics to learn other school subjects;<lb/>•  I need to do well in mathematics to get into the university of my choice;<lb/>•  I would like a job that involved using mathematics;<lb/>•  I need to do well in mathematics to get the job I want.<lb/>Figure 5.6 presents results for the students with a high level on the valuing mathematics index. These students <lb/>agreed a lot with all seven of the above statements on average. <lb/>PISA: Index of students&apos; instrumental motivation in mathematics <lb/>This index is based on students&apos; level of agreement with the following statements about mathematics: <lb/>•  Making an effort in mathematics is worth it because it will help me in the work that I want to do later on;<lb/>•  Learning mathematics is worthwhile for me because it will improve my career prospects;<lb/>•  Mathematics is an important subject for me because I need it for what I want to study later on ;<lb/>•  I will learn many things in mathematics that will help me get a job.<lb/>Students could answer that they strongly agreed, agreed, disagreed or strongly disagreed with the above <lb/>statements. Positive values on this index indicate higher levels of instrumental motivation to learn mathematics. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>91 <lb/></page>

			<body>Figure 5.6 Comparison of indices of valuing/motivation in mathematics in TIMSS and PISA <lb/>0.50 <lb/>0.25 <lb/>0.00 <lb/>-0.25 <lb/>-0.50 <lb/>-0.75 <lb/>PISA_InstrMotiv <lb/>80.00 <lb/>70.00 <lb/>60.00 <lb/>50.00 <lb/>40.00 <lb/>30.00 <lb/>20.00 <lb/>10.00 <lb/>TIMSS_ValueMaths <lb/>QUE <lb/>ONT <lb/>USA <lb/>TUN <lb/>SWE <lb/>SVK <lb/>SCO <lb/>RUS <lb/>NZL <lb/>NOR <lb/>NLD <lb/>LVA <lb/>KOR <lb/>JPN <lb/>ITA <lb/>IDN <lb/>HUN <lb/>HKG <lb/>ENG <lb/>BFL <lb/>AUS <lb/>249. <lb/>Figure 5.6 shows a comparison of PISA and TIMSS indices on valuing/motivation in <lb/>mathematics. The correlation between TIMSS &quot; Valuing Mathematics index &quot; and PISA &quot; Instrumental <lb/>Motivation index &quot; is 0.87, showing a close relationship between these two variables. <lb/>250. <lb/>The similarities in the findings of PISA and TIMSS on student motivation and self-confidence in <lb/>doing mathematics provide some reassurance of the validity of the results. It also suggests that, regardless <lb/>whether the sample is age based or grade based, and whether the sample consists of 14 or 15-year-olds, <lb/>profiles of students&apos; attitudes towards mathematics are largely stable. <lb/>Gender differences in attitudes towards mathematics <lb/>251. <lb/>One of the important findings in PISA is that, despite only moderate differences in mathematics <lb/>performance between girls and boys, where girls lag a little behind boys on average, there is a large <lb/>difference between attitudes of girls and boys towards mathematics. <lb/>A first striking finding is that while gender differences in student performance tend to be modest, <lb/>there are marked differences between males and females in their interest in and enjoyment of <lb/>mathematics as well as in their self-related beliefs, emotions and learning strategies related to <lb/>mathematics. (p.151, OECD, 2004) <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>92 <lb/></page>

			<body>252. <lb/>PISA made the comparisons between achievement differences and attitude differences based on <lb/> &quot; effect size &quot; 20 (p.152 – 153, OECD, 2004). In TIMSS, gender differences in attitude are not reported. In <lb/>this report, the average of TIMSS index of self-confidence was computed separately for girls and <lb/>boys. These indices are compared to PISA&apos;s scale of self-confidence. The results are shown in Figure <lb/>5.7 21 . Since the metrics of attitude indices are different in the two studies, the indices have been scaled <lb/>to have the same variance. So the actual relative positions of the index between PISA and TIMSS is <lb/>not important (consequently, no vertical scale unit is shown), but the relative size of the gender <lb/>difference within countries, and the trends across countries, can be compared. <lb/>Figure 5.7 Self-confidence in mathematics, by gender <lb/>AUS <lb/>BFL <lb/>ENG <lb/>HKG <lb/>HUN <lb/>IDN <lb/>ITA <lb/>JPN <lb/>KOR <lb/>LVA <lb/>NLD <lb/>NOR <lb/>NZL <lb/>RUS <lb/>SCO <lb/>SVK <lb/>SWE <lb/>TUN <lb/>USA <lb/>TIMSS GIRLS <lb/>TIMSS BOYS <lb/>PISA GIRLS <lb/>PISA BOYS`253 BOYS` <lb/>BOYS`253. <lb/>A number of observations can be made from the results shown in Figure 5.7. First, the rise and <lb/>fall of the lines across countries is similar between PISA and TIMSS. This means that countries where <lb/>students expressed high (or low) self-confidence are consistent in PISA and TIMSS. For example, in Japan <lb/>and Korea, students rated themselves low on the self-confidence scale in both PISA and TIMSS. This has <lb/>been discussed in the section on self-confidence index where it is shown that the correlation between the <lb/>self-confidence index between PISA and TIMSS is high. <lb/>254. <lb/>Second, in both PISA and TIMSS, boys expressed higher self-confidence in mathematics in <lb/>every country, except for Russia in TIMSS. In PISA, all 19 countries 22 had a higher mean achievement <lb/>score for boys 23 , so it is expected that boys would have higher self-confidence score than girls. However, <lb/>in TIMSS, girls had a higher mean achievement score 24 in eight of the 19 countries, but girls had lower <lb/>self-confidence than boys in all countries except for the Russian Federation. This supports the finding in <lb/>PISA that the gender gap in attitudes towards mathematics is larger than the gender gap in achievement <lb/>scores. <lb/>20. <lb/>Effect size is a measure of the magnitude of the difference in relation to the variance of all the scores (see <lb/>Box 3.3, p117, OECD, 2004). <lb/>21. <lb/>Note that data for Spain-Basque country, Canada-Ontario and Canada-Quebec are not available for this <lb/>graph <lb/>22. <lb/>Data for Canada-Ontario, Canada-Quebec and Spain-Basque country are not available. <lb/>23. <lb/>Although not always statistically significant. <lb/>24. <lb/>Although not always statistically significant. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>93 <lb/></page>

			<body>255. <lb/>Third, the magnitude of the gender difference in self-confidence for each country is similar <lb/>between PISA and TIMSS results. For example, in TIMSS, the gender gap in self-confidence is small for <lb/>Indonesia, Latvia, the Russian Federation and the Slovak Republic. In PISA, these countries also have <lb/>relatively smaller gender differences. On the other hand, the Netherlands and Sweden have larger gender <lb/>differences in self-confidence in both PISA and TIMSS. In general, in both studies, Eastern European <lb/>countries tend to have smaller gender gap in self-confidence, while Western countries and Asian <lb/>countries have larger gender gap. This is an interesting observation in itself and further investigation will <lb/>be worthwhile to study how Eastern European countries reduce gender gap in attitudes towards <lb/>mathematics, or, how Western and Asian countries create gender gap. <lb/>256. <lb/>Fourth, the correlation of the magnitude of gender gap in self-confidence between PISA and <lb/>TIMSS is moderately high (0.72), as compared to the correlation of the gender gap in mathematics <lb/>achievement between PISA and TIMSS (0.25). This suggests that attitude scales are more invariant than <lb/>achievement scales. Achievement scales are more sensitive to variations such as years of schooling and <lb/>the content balance of the assessments. Importantly this moderately high correlation between the surveys <lb/>in magnitude of gender gap in self-confidence validates the findings from each survey and suggests that <lb/>this gender gap is quite established across the ages of 14 to 16 years. <lb/>257. <lb/>Fifth, the correlation between gender gap in achievement and gender gap in self-confidence <lb/>across countries is relatively weak in both PISA and TIMSS. That is, in countries where boys outperform <lb/>girls by a great deal, the difference in self-confidence is not necessarily large. However, in TIMSS, the <lb/>relationship is slightly stronger than the relationship in PISA, with a correlation of 0.25 for TIMSS, and -<lb/>0.31 for PISA! This could be due to differences in the nature of the two assessments. While, in TIMSS, <lb/>the test is more curriculum-based, so performance on the test has a closer relationship with teaching and <lb/>learning in schools, which in turn relates to students&apos; self-confidence as they work with mathematics in <lb/>schools, while, in PISA, the test is not so curricular focused. The extent to which students can solve PISA <lb/>items may not be as closely related to school-based instructions as in TIMSS. <lb/>258. <lb/>In this section, only the self-confidence index is examined. It is expected, however, that other <lb/>attitudinal indices will provide similar findings. <lb/>Socio-economic Background of Students <lb/>259. <lb/>Both PISA and TIMSS collected information on a number of student background variables. PISA <lb/>reported students&apos; performance in relation to parental occupation, parental education, possessions, single <lb/>parent family, immigrant status and language spoken at home. TIMSS reported student performance in <lb/>relation to parental education, language spoken at home, number of books in the home, availability of <lb/>study desk/table in the home, and the use of computers at home and at other places. <lb/>260. <lb/>In relation to parental education, TIMSS found that, on average, for the Grade 8 cohort, students <lb/>with university-educated parents 25 scored more than 90 points higher than students whose parents had <lb/>primary or lower education (IEA, 2003, p.127). In PISA, on average across OECD countries, students <lb/>whose fathers completed tertiary education scored around 90 points higher than students whose fathers <lb/>completed primary or lower secondary education (OECD, 2004, Table 4.2c). While the score units are not <lb/>exactly the same in PISA and TIMSS, and the composition of the countries are different in the two <lb/>surveys, it is still evident that both surveys found a significant impact of parental education on student <lb/>performance in mathematics, and the magnitude of the impact is also of similar order of magnitude. <lb/>25. <lb/>This is defined as the highest education level of either parent <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>94 <lb/></page>

			<body>261. <lb/>In relation to home possessions, TIMSS reported student performance against each type of home <lb/>possessions (the number of books at home, the provision of a desk/table, and the availability of a computer <lb/>at home) separately. In contrast, PISA constructed an index of cultural possessions from the number of <lb/>classical literature, books of poetry and works of art in students&apos; homes (OECD, 2004, p.166; OECD, <lb/>2004, Table 4.2d; OECD, 2005, p.283). A direct comparison of results is difficult because different <lb/>variables were used in PISA and TIMSS. However, both PISA and TIMSS found a strong relationship <lb/>between home possession and student performance. <lb/>262. <lb/>The PISA report (OECD, 2004) provided a more detailed analysis on the impact of socio-<lb/>economic status on students&apos; performance, both at individual student level and at the school level. <lb/>263. <lb/>Despite differences in the collection of student background variables, the message is clear from <lb/>both PISA and TIMSS that student background has a significant impact on achievement level. <lb/>Summary <lb/>264. <lb/>This chapter explores similarities and differences in the findings of PISA and TIMSS in relation <lb/>to student backgrounds and attitudes towards mathematics. <lb/>265. <lb/>On the surface, it appears the PISA found that boys performed better than girls, while TIMSS <lb/>found little gender differences for the grade 8 cohort. On a closer examination of the results, however, the <lb/>different findings in PISA and TIMSS can be attributed to different cohort of countries and different <lb/>content balance in the respective tests. In particular, girls performed better in algebra in TIMSS, but there <lb/>were very few algebra items in PISA. The pattern of gender differences is consistent between PISA and <lb/>TIMSS findings. For example, both TIMSS and PISA found that boys outperformed girls by the greatest <lb/>amount in the content areas of measurement and geometry (or space and shape as labeled in PISA). In <lb/>contrast, both TIMSS and PISA found the smallest gender difference for the number content area (or <lb/>quantity as labeled in PISA). <lb/>266. <lb/>Further, in both TIMSS and PISA, the spread of mathematics achievement distribution is larger <lb/>for boys than for girls. <lb/>267. <lb/>Both PISA and TIMSS constructed indices of students&apos; self-confidence. There is a strong <lb/>agreement in the measures of this index at the country level between PISA and TIMSS. Similarly, TIMSS&apos; <lb/>Valuing index and PISA&apos;s Motivation index have a correlation of 0.89 between country means, showing a <lb/>close relationship between these two variables. In short, there is a good agreement between PISA and <lb/>TIMSS on the measures of students&apos; attitudes towards mathematics, despite different age groups in the two <lb/>surveys. <lb/>268. <lb/>Information on students&apos; socio-economic background was also collected in both PISA and <lb/>TIMSS. Both surveys found that socio-economic background had a significant impact on students&apos; <lb/>performance in mathematics. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>95 <lb/></page>

			<body>CHAPTER 6 -CONCLUSIONS <lb/>Overview <lb/>269. <lb/>The concurrent testing of TIMSS and PISA in 2003 provided education researchers with a <lb/>unique opportunity to gain an in-depth understanding of the results of international large-scale <lb/>assessments. Comparisons of the methodologies and the results between the two surveys led to a number of <lb/>important findings that not only would help us interpret the survey results, but also would help us plan <lb/>future surveys. This chapter discusses the findings and their implications for current and future PISA and <lb/>TIMSS surveys. <lb/>270. <lb/>The most important finding in this report is that differential performances of countries in TIMSS <lb/>and PISA can be accounted for by a number of factors. In particular, test content balance is the most <lb/>significant factor. Other factors include years of schooling and reading load in items. The identification of <lb/>factors not only enables us to draw valid conclusions from country scores and rankings, it also provides us <lb/>with evidence that both PISA and TIMSS conducted the surveys with rigorous procedures, since the results <lb/>can be cross verified and validated across the two surveys. Without two concurrent surveys, it would not be <lb/>possible to examine reliability and validity to this extent. Some specific findings and their implications are <lb/>discussed below. <lb/>The impact of content balance on achievement results <lb/>271. <lb/>A re-classification of PISA items according to TIMSS content domains (number, algebra, <lb/>measurement, geometry and data) shows that PISA and TIMSS have quite different test content balance. <lb/>PISA has more number and data items, and fewer algebra items. The differences in test content balance <lb/>between the two surveys account for 66% of the variation of country mean score differences between PISA <lb/>and TIMSS. This finding suggests that mathematics curriculum coverage has a significant impact on <lb/>student performance in PISA and TIMSS, and it is consistent with TIMSS&apos; finding that different countries <lb/>have different strengths and weaknesses in the five mathematics content areas (Exhibit 3.1, IEA, 2003). <lb/>272. <lb/>A link between students&apos; performance and a country&apos;s curriculum is a reasonable conjecture, but <lb/>it is not readily established, as it involves an extensive survey of the curriculum of each country. TIMSS <lb/>conducted a survey of the mathematics curriculum in each country. However, the results of the curriculum <lb/>survey were not examined closely in relation to the relative performance of each country in each <lb/>mathematics topic. There is only one note in the TIMSS 2003 International Mathematics Report (IEA, <lb/>2003) about the link between curriculum coverage and achievement results: <lb/>Although the relationship between inclusion in the intended curriculum and student achievement was <lb/>not perfect, it was notable that several of the higher-performing countries reported high levels of <lb/>emphasis on the mathematics topics in their intended curricula and that those with the lowest levels of <lb/>curricular coverage came from the lower half of the achievement distribution. (p.182, IEA, 2003) <lb/>273. <lb/>Below we take a look at the relationship between the implemented curriculum (as represented by <lb/>instructional time) and achievement scores. <lb/>274. <lb/>A survey of the percentage of instructional time in mathematics class devoted to TIMSS content <lb/>areas shows some variations across countries (Exhibit 7.4, IEA, 2003). For example, in the Russian <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>96 <lb/></page>

			<body>Federation, only 3% of time in mathematics class is devoted to the data strand at Grade 8 level, while in <lb/>Australia, 14% of time is devoted to the data strand. One might hypothesise that such differences in <lb/>curriculum emphases of different mathematics content areas will lead to differential student performances <lb/>in the content areas. To test this hypothesis, the correlation between Percentage time in mathematics <lb/>class devoted to a content strand and Deviation of achievement score in TIMSS content strand from <lb/>TIMSS mean score is computed and presented in Table 6.1. <lb/>Table 6.1 Correlation between percentage of instructional time and achievement score in TIMSS 2003 <lb/>expressed as deviation from the TIMSS mean score over 20 <lb/>26 countries <lb/>Correlation <lb/>p <lb/>Number <lb/>-0.043 <lb/>0.87 <lb/>Algebra <lb/>0.540 <lb/>0.045 <lb/>Measurement <lb/>0.223 <lb/>0.389 <lb/>Geometry <lb/>0.708 <lb/>0.001 <lb/>275. <lb/>The figures in Table 6.1 show that, in geometry, instructional time has a high correlation with the <lb/>TIMSS achievement score. In algebra and data, the relationship between instructional time and <lb/>achievement score is moderate. In measurement, the relationship is low. In number, there appears to be no <lb/>relationship between the amount of instructional time and achievement score! These results may be <lb/>consistent with the view that number, measurement and data are topics which one encounters frequently in <lb/>everyday life, but algebra and geometry are topics that need to be studied formally to understand the <lb/>concepts and the use of special terminology, symbols and formulae. In PISA, there are few algebra and <lb/>geometry items. Consequently, it could be said that the PISA test contains items that require less direct <lb/>instruction on the specific mathematics content. From this point of view, the PISA achievement score <lb/>reflects everyday use of mathematics, which may or may not be learned at schools, while TIMSS <lb/>achievement score reflects more school mathematics. It should also be noted that instructional time in <lb/>Table 6.1 is for Grade 8. Some countries teach topics such as number and measurement mostly in primary <lb/>schools, so that instructional time at Grade 8 level could be quite little. But this does not necessarily <lb/>indicate that the topics are not taught. Rather, they are taught in earlier grades. <lb/>276. <lb/>Accordingly, a country with a high score in PISA shows that the students are good at &quot; everyday <lb/>mathematics &quot; , while a high score in TIMSS shows that the students are good at &quot; school mathematics &quot; . <lb/>For example, in terms of country mean scores, Australia performed significantly better than Hungary in <lb/>PISA, but Hungary performed significantly better than Australia in TIMSS. One might conclude that <lb/>students in Hungary are better at typical school mathematics than Australian students, but they are not as <lb/>good as Australian students in using mathematics for everyday life applications. The fact that there are <lb/>differences in country rankings between PISA and TIMSS results suggests that, at least in some countries, <lb/>school mathematics has not prepared students as well in the application of mathematics as in academic <lb/>mathematics. Conversely, there are countries that have not prepared students as well in specialist areas of <lb/>mathematics, such as algebra and geometry, as they have prepared students in solving mathematics <lb/>problems in everyday life. The question as to which approach is better or which curriculum balance is the <lb/>best will be for the education policy makers in each country to consider in their own context, and, <lb/>certainly, neither PISA nor TIMSS alone should set the directions for future mathematics curriculum <lb/>reform. <lb/>277. <lb/>As content balance has such a significant impact on country performances in mathematics, to <lb/>establish reliable trend indicators from one survey cycle to another, each survey will need to maintain a <lb/>stable frame of reference in terms of content balance. This may be more difficult to achieve in PISA than <lb/>26 <lb/>Note that there are no data for the instructional time by content domain for England and Scotland. <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>97 <lb/></page>

			<body>in TIMSS, since PISA does not examine content balance in terms of traditional mathematics curriculum <lb/>strands. Given that there is not a one-to-one mapping between the Overarching Ideas and traditional <lb/>mathematics curriculum strands, the PISA content balance in terms of traditional mathematics curriculum <lb/>strands may vary across different cycles of PISA. This issue needs to be monitored carefully. On the other <lb/>hand, the content balance of an assessment cannot be static over many years, since the world will change <lb/>over time. What is relevant now may no longer be relevant in ten years&apos; time. So test content must change <lb/>accordingly. One can already see changes in TIMSS, such as the permission to use calculators in the <lb/>tests in 2003. <lb/>278. <lb/>Even if the test contents can stay relatively stable across survey cycles, the curricula of countries <lb/>change continually (Leung, Graf &amp; Lopez-real, 2006). TIMSS (IEA, 2003, p.165) also reported that the <lb/>curricula in participating countries were being revised constantly. This in turn will affect trend estimates <lb/>for the countries. Therefore, as a note of caution, in assessing trends across survey cycles, factors such as <lb/>content changes in the tests or curriculum changes within countries must be taken into account. <lb/>279. <lb/>The impact of content balance on achievement also casts some doubts about the reliable measure <lb/>of growth between two grade levels, particularly if the grade levels are far apart, since the alignment of <lb/>content between different grade levels will be difficult, as topics and content inevitably change across <lb/>school grades. For example, algebra is usually not taught until secondary schools. Arithmetic computations <lb/>are more the focus of primary schools but not secondary schools. In primary schools, students mostly <lb/>work with concrete representations while secondary school students often work with abstract <lb/>representations. These differences in content balance between primary and secondary school mathematics <lb/>will make any measure of growth difficult when students cannot easily make use of their knowledge and <lb/>skills across different content areas. <lb/>280. <lb/>More generally, one message for developers of assessments of mathematics is that the issue of <lb/>content balance should receive careful consideration and discussion, as performance results may be <lb/>sensitive to the composition of items from different content areas. The assumption made in many <lb/>assessment programmes about the uni-dimensionality of mathematics items may need careful checking. <lb/>That is, there is some evidence that mathematics items in PISA and TIMSS measure multiple abilities than <lb/>a single ability. <lb/>The impact of reading load on mathematics achievement results <lb/>281. <lb/>Leaving aside the three South-East Asian countries (Japan, Korea and Hong Kong-China), PISA <lb/>reading country mean score is found to be a good predictor of the differences between TIMSS and PISA <lb/>scores. As there is a considerable amount of reading in PISA tests compared with TIMSS tests, it is <lb/>reasonable to assume that poor readers will not perform as well in PISA as in TIMSS. It is also possible <lb/>that, in countries where reading achievement is relatively higher, students may be exposed to an <lb/>environment which facilitates mathematics problem-solving skills in everyday life. For example, students <lb/>may read newspapers more often, and be familiar with presentations of charts and diagrams for summary <lb/>of information, or, students may have more opportunity to be intelligent consumers such as through <lb/>reading advertisements of mobile phone cost plans. Unfortunately, it is difficult to disentangle these factors <lb/>from the current datasets. <lb/>Grade-based and age-based samples <lb/>282. <lb/>It has been contentious whether a grade-based sample or an age-based sample provides the most <lb/>comparable results in international surveys. The proponents for an age-based sample (OECD, 2004, p.27) <lb/>argue that, as each country has different school systems with different number of years of pre-schools, it is <lb/>difficult to define a grade at which the number of years of schooling is aligned across countries. In <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>98 <lb/></page>

			<body>contrast, an age-based sample defines the population according to an age range, where age is always <lb/>clearly defined. However, in the case of an age-based sample, the number of years of schooling will clearly <lb/>be different across countries. Proponents for a grade-based sample (IEA, 2003, p18) argue that there is a <lb/>better chance to align the number of years of schooling with grade-based sample. Further, a grade-based <lb/>sample provides the opportunity to collect information about classroom instructions and implemented <lb/>curriculum for comparison across countries. <lb/>283. <lb/>This report shows that the age at time of testing of TIMSS can be matched with the number of <lb/>years of schooling in PISA for most countries. This finding shows that both the TIMSS sample and the <lb/>PISA sample provide the same degree of comparability across countries, in that neither provides a more <lb/>stable frame of reference than the other, since both samples can be cross-checked in age and grade. <lb/>284. <lb/>However, the number of years of schooling does have an impact on mathematics achievement. <lb/>One year of schooling could increase the average achievement by 20 to 40 score points. Therefore, in <lb/>comparing the differences between TIMSS and PISA results, the number of years of schooling in PISA <lb/>should be taken into account. A recommendation from this report is that both PISA and TIMSS should <lb/>endeavour to obtain a better measure of the number of years of schooling, as even a fraction of a year of <lb/>schooling will make some differences to the achievement scores. Since the rough measure of the number <lb/>of years of schooling constructed in this report already can provide some analytic power in explaining <lb/>achievement differences in TIMSS and PISA, any refinement of the rough estimates produced in this <lb/>report should be an improvement. <lb/>Correlated factors <lb/>285. <lb/>While we have identified a number of factors that have an impact on achievement scores, the <lb/>picture is not so clear as to the causal relationships between achievement levels and the identified factors. <lb/>It appears that many factors are correlated, sometimes for no clear and obvious reasons. For example, it <lb/>appears that students in Asian and Eastern European countries tend to start school later than Western <lb/>countries. These same countries also tend to have a curriculum that is oriented towards an emphasis on <lb/>formal mathematics. A check on the correlation between the variable TIMSS advantage index and TIMSS <lb/>age at time of testing shows a correlation significantly different from zero (correlation=0.487, p=0.035). It <lb/>is possible that cultural factors could have an impact on both the school systems (in terms of age of entry, <lb/>etc.) and the curriculum (in terms of formal mathematics versus mathematics for application, etc.), so <lb/>that groups of countries have similar profiles in a number of aspects of mathematics education. <lb/>Gender and attitudinal differences <lb/>286. <lb/>The comparisons of gender differences in PISA and TIMSS in this report (Chapter 5) highlight <lb/>at least two important points. First, gender differences are not uniform across mathematics content <lb/>domains. In particular, for spatial tasks and measurement tasks, boys outperform girls by the greatest <lb/>amount. This finding has implications for instructional measures to address gender differences. For <lb/>example, particular kinds of tasks may be designed to engage girls and boys in different ways. Second, <lb/>gender differences are not uniform across countries. Is there a hint of suggestion that boys outperform girls <lb/>in mathematics in more developed countries such as OECD member countries? If so, this would seem <lb/>contrary to expectations that more developed countries would generally have better addressed the equity <lb/>issue between girls and boys. Is there a stereo-typing promoted in developed countries? The contrast in <lb/>TIMSS and PISA results regarding gender differences prompts us to examine gender issues in further <lb/>depths. <lb/>287. <lb/>Both PISA and TIMSS reported similar findings in students&apos; attitudes towards mathematics. The <lb/>most important one is that there is a larger gender gap in attitudes towards mathematics than in <lb/></body>

			<note place="headnote">EDU/WKP(2010)5 <lb/></note>

			<page>99 <lb/></page>

			<body>mathematics achievement, with boys feeling more positive and confident towards mathematics than girls. <lb/>The combined results of TIMSS and PISA provide us with a more complete picture of students&apos; attitudes <lb/>towards mathematics. In particular, there is still much to learn by Western countries from Eastern <lb/>European countries about reducing the gender gap in students&apos; attitudes towards mathematics. <lb/>And finally… <lb/>288. <lb/>A number of important findings in this report could only be made when there are two <lb/>international surveys conducted at the same time. From this point of view, the two surveys contribute not <lb/>only to cross-national comparisons, but also to the validation of similarities and the identification of <lb/>differences in the results of the two surveys. The interpretations of the results from each survey are <lb/>enhanced by the cross validation by the other survey. It is comforting to all involved in the surveys to <lb/>know that the results between the two surveys can be quantitatively validated and crosschecked. It shows <lb/>that the surveys are conducted with rigour and sound theoretical underpinnings. <lb/>289. <lb/>However, the fact that the results from the two surveys can be corroborated quantitatively <lb/>suggests that there is some duplication in the outcomes of the surveys, at least in the results in the <lb/>international reports. The findings in this report highlight the importance of making careful interpretations <lb/>of results for each country individually, since there are country specific factors that impact on student <lb/>performance. Consequently, international reports are just the first steps in presenting data collected in these <lb/>surveys. National reports should be viewed with greater importance for examining the data in more depth <lb/>in relation to cultural and policy factors specific to each country. <lb/>290. <lb/>In making comparisons between PISA and TIMSS results at the international level, it should be <lb/>remembered that PISA results reflect relative performance of countries on a set of desirable mathematical <lb/>proficiencies for everyday life (as set through a consensus building process guided by the PISA <lb/>mathematics expert group). In contrast, TIMSS results reflect how well countries performed against <lb/>important areas of current mathematics curricula as agreed by participating countries. The fact that these <lb/>two approaches to building the mathematics frameworks do not produce the same assessment results is <lb/>food for thought for both curriculum developers and assessment practitioners. <lb/>291. <lb/>Looking ahead, for future cycles of PISA and TIMSS, many issues raised in this report should be <lb/>considered in order to provide the best strategies in producing internationally useful and valid results with <lb/>maximum efficiency for the countries. This calls for collaboration between the two surveys so that the <lb/>results from the surveys can complement each other, rather than duplicate each other. <lb/>EDU/WKP(2010)5 <lb/></body>

			<page>100 <lb/></page>

			<listBibl>References <lb/>Adams, R. J. (2003). Response to &apos;Cautions on OECD&apos;s recent educational survey (PISA)&apos;, Oxford <lb/>Review of Education, Vol. 29, No. 3, September 2003. pp.377-398. <lb/>Bonotto, C. (2003). Suspension of sense-making in mathematical word problem solving: A possible <lb/>remedy. Taken from http://math.unipa.it/~grim/Jbonotto on 16/8/2003 <lb/>Brown, G., Micklewright, J., Schnepf, S., &amp; Waldmann, R. (2005). Cross-national surveys of learning <lb/>achievement: How robust are the findings? Retrieved December 28, 2005 from <lb/>ftp://repec.iza.org/RePEc/Discussionpaper/dp1652.pdf. <lb/>Committee of Inquiry into the Teaching of Mathematics in Schools (1982). Mathematics Counts. (the <lb/>Cockcroft report), Her Majesty&apos;s Stationery Office, London, United Kingdom. <lb/>De Lange, J. (1996). Using and applying mathematics in education. In A. J. Bishop, K. Clements, C. <lb/>Keitel, J. Kilpatrick &amp; C. Laborde (Eds.), International handbook of mathematics education (pp. 49-<lb/>98). Dordrecht, The Netherlands: Kluwer Academic Publishers. <lb/>Embretson, S. E., &amp; Reise, S. P. (2000). Item response theory for psychologists. Mahwah, NJ: Lawrence <lb/>Erlbaum Associates. <lb/>Ferrini-Mundy, J. (2002). Draft commentary on PISA 2003 mathematics framework. Internal report <lb/>commissioned by OECD. <lb/>Freudenthal, H. (1973). Mathematics as Educational Task, D. Reidel, Dordrecht, Netherlands. <lb/>Friedman, Lynn (1989) Mathematics and the gender gap: A meta-analysis of recent studies on sex <lb/>difference in mathematical tasks. Review of Educational research, Summer 1989, 59(2), 185 – 213 <lb/>Gallagher, A., Levin, J., &amp; Cahalan, C. (2002). Cognitive patterns of gender differences on mathematics <lb/>admissions tests. GRE Research, September 2002, GRE Board Professional Report no. 96-17P, ETS <lb/>Research Report 02-19. Princeton, NJ: ETS. <lb/>Gee, J. (1998). Preamble to a literacy program. Department of Curriculum and Instruction. Madison, WI. <lb/>Gravemeijer, K. (1999). How emergent models may foster the constitution of formal mathematics. <lb/>Mathematical Thinking and Learning. An International Journal, 1(2), 155-177. <lb/>Grünbaum, B. (1985). Geometry Strikes Again &quot; , Mathematics Magazine, 58 (1) , pp 12-18. <lb/>IEA (2003). TIMSS 2003 International Mathematics Report. Chestnut Hill, M.A: TIMSS International <lb/>Study Centre. <lb/>IEA (2003b). TIMSS Assessment Frameworks and Specifications 2003. Chestnut Hill, M.A: TIMSS <lb/>International Study Centre. <lb/>IEA (2004). TIMSS 2003 Technical Report. Chestnut Hill, M.A: TIMSS International Study Centre. <lb/>Leung, F.K.S., Graf, K-D., &amp; Lopez-Real, F.J. (Eds.) (2006). Mathematics Education in Different Cultural <lb/>Traditions – A Comparative Study of East Asia and the West. The 13 th ICMI Study. Springer. <lb/>EDU/WKP(2010)5 <lb/>101 <lb/>LOGSE (1990). Ley de Ordenacion General del Sistema Educativo, Madrid, Spain. <lb/>Lokan, J. &amp; Greenwood, L. (2001) TIMSS Maths &amp; Science on the line. Australian Year 12 students&apos; <lb/>performance in the Third International Mathematics and Science Study. ACER, Melbourne. <lb/>Mathematical Sciences Education Board (MSEB) (1990). Reshaping School Mathematics: A Philosophy <lb/>and Framework of Curriculum. National Academy Press, Washington, DC. <lb/>Mislevy, R.J. (1991). Randomization-based inference about latent variable from complex samples. <lb/>Psychometrika, 56, Psychometric Society, Greensboro, pp. 177-196. <lb/>Mislevy, R.J., &amp; Sheehan, K.M. (1987). Marginal estimation procedures, in A.E. Beaton (Ed.), The NAEP <lb/>1983-1984 Technical Report (Report No. 15-TR-20), Educational Testing Service, Princeton, N.J. <lb/>Mullis, I., Martin, M., Smith, T. A., Garden, R. A., Gregory, K. D., Gonzales, E. J., Chrostowski, S. J., &amp; <lb/>O&apos;Connor, K. M. (2001). TIMSS assessment frameworks and specifications 2003. Chestnut Hill: <lb/>ISC, Boston College. <lb/>Nagasaki, E., &amp; Senuma, H. (2002). TIMSS mathematics results: A Japanese perspective. In D. F. <lb/>Robitaille &amp; A. E. Beaton (Eds.), Secondary analysis of the TIMSS data. (pp.81-93). Dordrecht, <lb/>The Netherlands: Kluwer Academic Publishers. <lb/>National Council of Teachers of Mathematics (NCTM) (1989). Curriculum and Evaluation Standards for <lb/>School Mathematics. NCTM, Reston, VA. <lb/>National Council of Teachers of Mathematics (NCTM) (2000). Principles and Standards for Mathematics. <lb/>NCTM, Reston, VA. <lb/>OECD (2001). Knowledge and skills for life. First results from PISA 2000. Paris: OECD. <lb/>OECD (2003). The PISA 2003 assessment framework. Paris: OECD. <lb/>OECD (2004). Learning for Tomorrow&apos;s World. Paris: OECD. <lb/>OECD (2005). PISA 2003 Technical Report. Paris: OECD. <lb/>Prais, S. J. (2003). Cautions on OECD&apos;s recent educational survey (PISA). Oxford Review of Education, <lb/>Vol. 29, No. 2, June 2003. <lb/>Robitaille, D. F., et al (1993). TIMSS Monograph No. 1: Curriculum frameworks for mathematics and <lb/>science. Vancouver, BC: Pacific Educational Press. <lb/>Romberg, T., &amp; de Lange, J. (1998). Mathematics in context. Chicago: Britannica Mathematics System. <lb/>Routitsky, A., &amp; Turner, R. (2003). Item format types and their influence on cross-national comparisons <lb/>of student performance, Paper presented at the AERA Annual Meeting, Chicago, April, 2003. <lb/>Routitsky, A., &amp; Zammit, S. (2001). What we can learn from TIMSS: Comparison of Australian and <lb/>Russian TIMSS-R results in algebra. In H. Chick, K. Stacey, Jill Vincent, &amp; John Vincent. (Eds.), <lb/>Proceedings of the 12 th ICMI study conference: The Future of the teaching and learning of algebra. <lb/>Melbourne, Australia: University of Melbourne. <lb/>EDU/WKP(2010)5 <lb/>102 <lb/>Routitsky, A., Zammit, S. A (2002) Association between intended and attained Algebra curriculum in <lb/>TIMSS 1998/1999 for ten countries. Proceedings of the 2002 annual conference of the Australian <lb/>Association for Research in Education, Brisbaine. Retrieved January 2003 from: <lb/>http://www.aare.edu.au/indexpap.htm (rou02147) <lb/>Silver, E. A. (2002). Review of the proposed OECD/PISA 2003 mathematics framework. Internal report <lb/>commissioned by OECD. <lb/>van der Linden, W. J., &amp; Hambleton, R. K. (1997). Handbook of modern item response theory. New <lb/>York: Springer-Verlag. <lb/>Verschaffel, L., Greer, B. &amp; de Corte E. (2000). Making sense of word problems. Swets &amp; Zeitlinger, <lb/>Lisse. <lb/>Voyer, D., Voyer, S., &amp; Bryden, M. P. (1995). Magnitude of sex differences in spatial abilities: A meta-<lb/>analysis and consideration of critical variables. Psychological Bulletin, 117(2), 250-270. <lb/>Wang, W C. (2000). PISA Maths analysis report. Unpublished Internal Report. Camberwell: ACER. <lb/>Wu, M. L. (2005). The Impact of PISA in Mathematics Education –Linking Mathematics and the Real <lb/>World. Education Journal (Special issue: Analyzing the quality of education in Hong Kong from an <lb/>international perspective), Volume 32, Number 1, Summer 2004, pp121-140. <lb/>Wu, M. L. (2005b). The role of plausible values in large-scale surveys. Postlethwaite (Ed.). Special Issue <lb/>of Studies in Educational Evaluation (SEE) in memory of R M Wolf. 31 (2005) 114-128. <lb/>Wu, M. L. (2006). A comparison of mathematics performance between East and West – What PISA and <lb/>TIMSS can tell us. ICMI Study 13, Springer, 239-259. <lb/>Zabulionis, A. (2001). Similarity of Mathematics and Science Achievement of Various Nations. Education <lb/>Policy Analysis Archives, 9(33). Retrieved 15 March 2003 from the World Wide Web: <lb/>http://epaa.asu.edu/epaa/v9n33/. </listBibl>


	</text>
</tei>
