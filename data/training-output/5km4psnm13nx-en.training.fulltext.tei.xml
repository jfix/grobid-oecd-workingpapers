<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<head>OECD DIRECTORATE FOR EDUCATION<lb/> OECD EDUCATION WORKING PAPERS SERIES<lb/></head>

			<p>This series is designed to make available to a wider readership selected studies drawing on the work<lb/> of the OECD Directorate for Education. Authorship is usually collective, but principal writers are named.<lb/> The papers are generally available only in their original language (English or French) with a short<lb/> summary available in the other.<lb/> Comment on the series is welcome, and should be sent to either edu.contact@oecd.org or the<lb/> Directorate for Education, 2, rue André Pascal, 75775 Paris CEDEX 16, France.<lb/></p>

			<p>The opinions expressed in these papers are the sole responsibility of the author(s) and do not<lb/> necessarily reflect those of the OECD or of the governments of its member countries.<lb/> Applications for permission to reproduce or translate all, or part of, this material should be sent to<lb/> OECD Publishing, rights@oecd.org or by fax 33 1 45 24 99 30.<lb/> ---------------------------------------------------------------------------<lb/>www.oecd.org/edu/workingpapers<lb/> ---------------------------------------------------------------------------<lb/>Applications for permission to reproduce or translate<lb/> all or part of this material should be made to:<lb/></p>

			<table>Head of Publications Service<lb/> OECD<lb/> 2, rue André-Pascal<lb/> 75775 Paris, CEDEX 16<lb/> France<lb/> Copyright OECD 2010<lb/> EDU/WKP(2010)5<lb/></table>

			<p>ABSTRACT<lb/> This paper makes an in-depth comparison of the PISA (OECD) and TIMSS (IEA) mathematics<lb/> assessments conducted in 2003. First, a comparison of survey methodologies is presented, followed by an<lb/> examination of the mathematics frameworks in the two studies. The methodologies and the frameworks in<lb/> the two studies form the basis for providing explanations for the observed differences in PISA and TIMSS<lb/> results. At the country level, it appears that Western countries perform relatively better in PISA as<lb/> compared to their performance in TIMSS. In contrast, Asian and Eastern European countries tend to do<lb/> better in TIMSS than in PISA. This paper goes beyond making mere conjectures about the observed<lb/> differences in results between PISA and TIMSS. The paper provides supporting evidence through the use<lb/> of regression analyses to explain the differences. The analyses showed that performance differences at the<lb/> country level can be attributed to the content balance of the two tests, as well as the sampling definitions –<lb/> age-based and grade-based – in PISA and TIMSS respectively. Apart from mathematics achievement, the<lb/> paper also compares results from the two studies on measures of self-confidence in mathematics. Gender<lb/> differences are also examined in the light of contrasting results from the two studies. Overall, the paper<lb/> provides a comprehensive comparison between PISA and TIMSS, and, in doing so, it throws some light on<lb/> the interpretation of results of large-scale surveys more generally.<lb/></p>

			<head>RESUME<lb/></head>

			<figure>Le présent document établit une comparaison détaillée des évaluations des mathématiques PISA<lb/> (OCDE) et TIMSS (IEA), toutes deux menées en 2003. Il présente tout d&apos;abord une comparaison des<lb/> méthodologies d&apos;enquête, puis un examen des cadres d&apos;évaluation des mathématiques. C&apos;est en effet par<lb/> une analyse des méthodologies et cadres d&apos;évaluation des deux études que l&apos;on peut expliquer les<lb/> différences constatées dans les résultats du PISA et ceux du TIMSS. Au niveau des pays, il apparaît que les<lb/> nations occidentales réussissent relativement mieux à l&apos;enquête PISA qu&apos;à l&apos;enquête TIMSS. En revanche,<lb/> les pays d&apos;Asie et d&apos;Europe de l&apos;Est ont tendance à obtenir de meilleures performances aux évaluations<lb/> TIMSS qu&apos;aux évaluations PISA. Au-delà de simples conjectures sur les différences observées, le présent<lb/> document fournit des éléments de preuves en utilisant des analyses de régression qui expliquent les<lb/> disparités de résultats. Les analyses ont démontré que les variations de performance au niveau national<lb/> peuvent être imputées à l&apos;équilibre des contenus des deux tests, ainsi qu&apos;aux définitions d&apos;échantillonnage<lb/> – fondées sur l&apos;âge ou fondées sur la classe – pour PISA et pour TIMSS, respectivement. Outre les<lb/> performances en mathématiques, le présent document compare les résultats des deux études sur les<lb/> mesures de la confiance en soi en mathématiques. Les différences entre les sexes sont également<lb/> examinées à la lumière des résultats contrastés des deux enquêtes. Dans l&apos;ensemble, ce document offre une<lb/> comparaison complète entre PISA et TIMSS et, ce faisant, éclaire plus généralement les interprétations des<lb/> résultats d&apos;enquêtes de grande envergure.<lb/> TABLE OF CONTENTS<lb/> CHAPTER 1 -INTRODUCTION .</figure>

			<head>................................................................................................................. 7<lb/></head>

			<p>Purpose of the working paper .</p>

			<head>..................................................................................................................... 7<lb/> Surveys for comparison ............................................................................................................................... 8<lb/> Similarities and differences ......................................................................................................................... 8<lb/> Focus of comparison ................................................................................................................................... 8<lb/></head>

			<p>Background information about the two surveys .</p>

			<head>......................................................................................... 9<lb/> The aims of TIMSS and PISA ................................................................................................................. 9<lb/> Organisation of the report ......................................................................................................................... 10<lb/> CHAPTER 2 -SURVEY METHODOLOGIES ........................................................................................... 12<lb/></head>

			<p>Introduction .</p>

			<head>.............................................................................................................................................. 12<lb/> Population definition ................................................................................................................................. 12<lb/></head>

			<figure>CHAPTER 1 -INTRODUCTION<lb/> Purpose of the working paper<lb/> 1.<lb/> What would help explain differences between the results in students&apos; mathematics performance<lb/> from two respected international education surveys conducted in 2003: the OECD&apos;s Programme for<lb/> International Student Assessment (PISA) and the International Association for the Evaluation of<lb/> Educational Achievement (IEA)&apos;s Trends in International Mathematics and Science Study (TIMSS)? This<lb/> was a question asked by the PISA Governing Board (PGB) and was at the heart of the development of this<lb/> report.<lb/></figure>

			<p>2.<lb/> The results of these two international assessments have often been cross-referenced and<lb/> synthesised to present an overall picture of mathematical achievements. Valid cross-references of the<lb/> results from PISA and TIMSS require, however, a clear and accurate understanding of the two assessments<lb/> in terms of their objectives, assessment frameworks and the nature of the tasks and items that were<lb/> presented to students. The objective of this report is to provide such an understanding. 1<lb/> 3.<lb/> This objective is important as comparisons are inevitably made between the results when two<lb/> international surveys are carried out concurrently to measure mathematics achievement, and superficial<lb/> comparisons can often be misleading, inaccurate or simply inadequate. For instance, it is customary to<lb/> brush aside any comparison of PISA and TIMSS by stating that (1) PISA samples are age-based while<lb/> TIMSS samples are grade-based; and (2) PISA is not curriculum-driven while TIMSS is based on<lb/> curriculum. While these statements are generally correct, these explanations of the differences between the<lb/> surveys hardly convey any useful information in gaining an understanding of the comparative results of the<lb/> two surveys. Further, uninformed comparisons can be dangerous, particularly when somewhat emotionally<lb/> charged. <ref type="biblio">Prais (2003)</ref> made a number of conjectures in a comparison of PISA and TIMSS results for the<lb/> United Kingdom. In a rejoinder to <ref type="biblio">Prais&apos; article, Adams (2003)</ref> pointed out that Prais&apos; criticisms were<lb/> based on some misunderstanding of the two surveys. Furthermore, a comparison limited to the results of<lb/> one country is unlikely to have the power to reveal patterns of similarities and differences. As a result, this<lb/> working paper attempts to provide a comprehensive comparison of PISA and TIMSS, examining both<lb/> methodological similarities and differences, as well as similarities and differences in the results. Aspects in<lb/> relation to methodology include sampling, framework development and test construction. Aspects in<lb/> relation to results include comparisons and explanations of rank ordering of countries, observed gender<lb/> differences, as well as the impact of attitude on achievement.<lb/> 4.<lb/> The major target audience of this report will be educational practitioners. The term &quot; educational<lb/> practitioners &quot; is widely defined here to include policy makers and researchers in the education field. The<lb/> report aims to help those involved in planning instructions and monitoring achievement in mathematics to<lb/> understand the differences and similarities between PISA and TIMSS, and to provide guidance on how to<lb/> relate and interpret the results from the two assessments. For example, PISA and TIMSS results can<lb/> identify relative strengths and weaknesses of students in the field of mathematics. This information, when<lb/> interpreted correctly, can in turn be used for an evaluation of current practices as well as future reforms in<lb/> curriculum and instruction.<lb/></p>

			<head>Surveys for comparison<lb/></head>

			<p>5.<lb/> This report uses results from the PISA 2003 mathematics assessment and TIMSS 2003<lb/> population 2 mathematics assessment 2 as the basis for comparison.<lb/> 6.<lb/> There are two reasons for the choice of these two surveys for comparison. First, they were both<lb/> conducted at around the same time 3 . The data collected represent a cross-sectional profile of students&apos;<lb/> mathematics achievement in 2003 in each participating country. On-going changes in educational reforms<lb/> within each country are not likely to account for the differences between the results of the two surveys, as<lb/> both surveys were conducted at about the same time. Second, although PISA assesses reading, science and<lb/> mathematics, the majority of testing time was devoted to mathematics in PISA 2003 so that the data<lb/> collected covered most mathematics content areas, as was typically the case in TIMSS studies. This makes<lb/> the results of the two surveys more comparable.<lb/></p>

			<head>Similarities and differences<lb/></head>

			<p>7.<lb/> While it is useful to identify differences between PISA and TIMSS, the identification of<lb/> similarities between the two surveys should also be valuable. First, where the findings from both surveys<lb/> agree, the results provide strong evidence for policy makers and researchers to take appropriate actions<lb/> based on the findings. For example, if gender differences in mathematics performance from both surveys<lb/> are consistent, then there is a clear message about the differential performance of girls and boys, despite<lb/> differences in grade and age in the two surveys. Second, the identification of the extent of similarities in<lb/> the survey methodologies could inform policy decisions regarding the best way to move forward so that<lb/> the two surveys complement, rather than duplicate, each other. For example, both surveys are currently<lb/> paper-and-pencil based tests with short tasks. It is possible that some extended performance tasks delivered<lb/> through computer-based testing could be included in one survey to tap into a different aspect of<lb/> mathematics performance, so that the two surveys can provide complementary information about all<lb/> aspects of mathematics literacy. Consequently, a useful comparison between the two surveys should go<lb/> beyond just looking for differences. This working paper provides a holistic comparison between the two<lb/> surveys, identifying both similarities and differences between the methodologies and the findings.<lb/></p>

			<head>Focus</head>

			<table>of comparison<lb/> 8.<lb/> More specifically, this report addresses four aspects of PISA and TIMSS:<lb/> • What findings are similar between the two surveys? Agreements between findings from the two<lb/> surveys will reinforce the underlying messages and provide some evidence of validity of the<lb/> results.<lb/> • What findings are different, or even contradictory, between the two surveys? What are possible<lb/> explanations for the differences?<lb/> 2.<lb/> TIMSS Population 2 refers to the Grade 8 cohort. TIMSS Population 1 refers to the Grade 4 cohort.<lb/> 3.<lb/> The PISA testing window was between March and August 2003 (OECD, 2005, p.46). In TIMSS, seven<lb/> Southern Hemisphere countries tested in October through December, 2002. Korea tested later in 2003. The<lb/> remaining countries tested mostly between April and June 2003 (IEA, 2003, p.18).<lb/> • Which issues are investigated by only one survey? What findings from the two surveys are<lb/> complementary?<lb/> • What lessons can we learn from a cross-comparison of the two surveys? How can we improve<lb/> each survey based on the findings of this report?<lb/></table>

			<head>Background information about the two surveys<lb/></head>

			<p>9.<lb/> PISA is conducted by the OECD while TIMSS is conducted by the IEA. To fully understand the<lb/> differences between PISA and TIMSS, it will be important to be familiar with the history and the<lb/> &quot; philosophies &quot; of IEA studies and OECD work. OECD, being a co-operative organisation between<lb/> governments, has policy-makers&apos; interests as the focus of its work. In contrast, IEA, formed as a united<lb/> body of research organisations, has the interests of researchers at the forefront of its studies. Although the<lb/> distinction between policy focus and research focus is a blurred one, as many research questions that drive<lb/> TIMSS are heavily influenced by policy considerations. Nevertheless, the different backgrounds of the two<lb/> organisations have resulted in setting different goals for the studies conducted. For example, in TIMSS it<lb/> was deemed important to link the survey results directly to instructional practices in the classrooms, while<lb/> in PISA, the measure of the outcome of schooling is deemed more important for governments in shaping<lb/> educational policies. The emphases in the main goals of each study in turn had an impact on population<lb/> definition and sampling procedures. For example, to examine instructional practices and relate these to<lb/> student achievement, one needs to sample classes. To sample classes, the population definition will need to<lb/> be grade based. In contrast, to compare outcomes of schooling, an age-based sample may place countries<lb/> on more equal footings for describing the preparedness of students for adult life. Consequently, a clear<lb/> understanding of the different objectives of each study is fundamental in subsequent analyses of the<lb/> comparisons of the surveys.<lb/></p>

			<head>The aims of TIMSS and PISA<lb/></head>

			<p>10.<lb/> In the introduction to the TIMSS 2003 International Mathematics Report (<ref type="biblio">IEA, 2003</ref>), the aim of<lb/> the study is stated as follows:<lb/></p>

			<p>The aim of TIMSS … is to improve the teaching and learning of mathematics and science by<lb/> providing data about students&apos; achievement in relation to different types of curricula, instructional<lb/> practices, and school environments. (p.13)<lb/> 11.<lb/> The aim of TIMSS places teaching and learning at the forefront, with a special mention of linking<lb/> achievement to curricula and instructional practices. In contrast, in the OECD publication Learning for<lb/> <ref type="biblio">Tomorrow&apos;s World – First Results from PISA 2003 (OECD, 2004</ref>), the aim of PISA is stated as follows:<lb/></p>

			<p>PISA seeks to assess how well 15-year-olds are prepared for life&apos;s challenges. … focusing on young<lb/> people&apos;s ability to use their knowledge and skills to meet real-life challenges, rather than merely on<lb/> the extent to which they have mastered a specific school curriculum. (p.20)<lb/> 12.<lb/> PISA&apos;s statement of aim indicates that the link between achievement and curricula is not<lb/> regarded as the main objective of the study. PISA adopts a &quot; literacy &quot; concept about the extent to which<lb/> students can apply knowledge and skills. An assessment of this literacy in various subject domains will<lb/> have direct policy relevance for governments. This is not to say that TIMSS does not produce useful<lb/> information for policy-makers, or that PISA results do not inform teaching and learning. Such a division<lb/> between policy focus and research focus is an oversimplification of the aims of the surveys. Many research<lb/> objectives in TIMSS were driven by policy considerations, and many policy objectives in PISA result in<lb/> research themes. However, it is the relative emphases of the two studies that are different. When a study is<lb/> designed for a main purpose, the results are usually not as readily useful for other purposes.<lb/> 13.<lb/> It is not surprising that PISA has a policy orientation while TIMSS has a research orientation,<lb/> since the governing body of PISA consists of governmental departments, while institutional members of<lb/> IEA are research centres that may or may not be linked to the government of each country. Consequently,<lb/> the decision making processes in the two studies differ to some extent, since the participants of decision<lb/> making meetings are not the same group of people. However, there is some overlap of participants. Around<lb/> eleven countries have the same government department taking charge of both PISA and TIMSS in<lb/> participating in the decision-making processes at the international level, and about half of these countries<lb/> have the same person in charge of both projects 4 . From this point of view, PISA and TIMSS are not<lb/> entirely separate studies, since common experiences and problems in these two surveys have often been<lb/> cross-referenced when setting directions for each survey.<lb/> 14.<lb/> Nevertheless, the total number of countries participating in each study (41 countries in PISA<lb/> 2003, and 50 countries 5 in TIMSS 2003 Grade 8 cohort) is much larger than the number of countries taking<lb/> part in both studies (22 countries), and the cohort of countries that participated in each study has an impact<lb/> on the directions taken for the development of the assessment instruments. First, in both studies, there has<lb/> been an active involvement from participating countries in shaping the assessment instruments through<lb/> contributions and reviews of items. Second, the target difficulty level and cultural balance also need to<lb/> match the group of participating countries. Characteristics of items may be influenced by the background<lb/> of the participants of the surveys. For example, specific item contexts may be selected or avoided<lb/> depending on the level of socio-economic status of students in participating countries, since students&apos;<lb/> familiarity with the contexts can have some impact on the results. The items selected also need to work<lb/> well in all countries and languages.<lb/></p>

			<head>Organisation</head>

			<table>of the report<lb/> 15.<lb/> The content of this report is organised in six chapters.<lb/> 16.<lb/> Chapter 2 compares survey methodologies used in the two surveys. The topics considered include<lb/> population definitions, sampling methods, test characteristics, scaling methods, field operations and<lb/> questionnaires administration.<lb/> 17.<lb/> Chapter 3 provides a detailed comparison of the mathematics frameworks and test specifications<lb/> used in PISA and TIMSS. An attempt is made to align the two frameworks according to both the content<lb/> domains and cognitive domains.<lb/> 18.<lb/> Chapter 4 examines the degree of similarities and differences between TIMSS and PISA<lb/> achievement results in terms of country mean scores. Various hypotheses are tested to explain the observed<lb/> differences in country performance in TIMSS and PISA. With the identification of a number of factors,<lb/> quantitative models are built to explain achievement differences.<lb/> 19.<lb/> Chapter 5 focuses on comparisons of gender differences and students&apos; attitudes towards<lb/> mathematics, as reported in TIMSS and PISA. The choice of these student level variables was based on the<lb/> availability of published results provided by the two surveys.<lb/> 4.<lb/> It is difficult to obtain these figures precisely, since there are often changes of personnel or changes of<lb/> contractors for running each study.<lb/> 5<lb/> Some of the participants in TIMSS are sub-regions of a country, for example, Basque country in Spain and<lb/> two provinces of Canada.<lb/> 20.<lb/> Chapter 6 summarises the findings, as well as draws attention to the implications of the findings<lb/> for PISA and TIMSS, and, in fact, for large-scale assessments more generally.<lb/> CHAPTER 2 -SURVEY METHODOLOGIES<lb/> Introduction<lb/> 21.<lb/> This chapter compares survey methodologies used in PISA and TIMSS. In particular, five aspects<lb/> of survey methodologies are compared: sampling, test characteristics, scaling methods, field operations and<lb/> test administration. On the whole, both PISA and TIMSS adopt similar survey methodologies typically<lb/> used for large-scale studies. Both surveys chose the methodologies to meet survey objectives while taking<lb/> account of the constraints.<lb/> 22.<lb/> As international studies, both surveys attempt to provide information on mathematics<lb/> achievement at the national level for participating countries. Since it would be impractical to test every<lb/> student in each country, both PISA and TIMSS use sampling methodology to select a representative<lb/> sample from each country. The use of samples leads to the implementation of a set of statistical procedures<lb/> appropriate for drawing inferences from samples about the population.<lb/> 23.<lb/> Both PISA and TIMSS have world experts in large-scale survey sampling methodology<lb/> providing clear directions for sampling within each country. The sampling methodologies used in both<lb/> studies are similar.<lb/> 24.<lb/> In addition, both surveys use similar methodologies for estimating student performance and the<lb/> construction of a proficiency scale. These scaling methodologies are described in more detail below. There<lb/> is also some overlap in expertise in this scaling process, as some members of the technical advisory group<lb/> for PISA have also been involved in the scaling of TIMSS and the United States National Assessment of<lb/> Educational Progress (NAEP) 6 data.<lb/> 25.<lb/> In terms of field operations, PISA and TIMSS have similar processes, with minor variations in<lb/> translation and verification procedures. As technical aspects of methodologies for international surveys<lb/> are described in detail in published reports, there is now a move towards forming standards for conducting<lb/> international, or large-scale, surveys. Consequently, it is not surprising that survey methodologies are<lb/> becoming more similar across different studies in broad terms, but there are variations at the level of<lb/> details.<lb/> 26.<lb/> The following sections compare each aspect of survey methodologies between PISA and TIMSS<lb/> in more detail.<lb/> Population definition<lb/> 27.<lb/> PISA and TIMSS have different population definitions. The target population of PISA is defined<lb/> as follows:<lb/> The desired base PISA target population in each country consisted of 15-year-old students attending<lb/> educational institutions located within the country, in grades 7 and higher. (p.46, OECD, 2005)<lb/> 6.<lb/> TIMSS uses essentially the same scaling methodology as NAEP.<lb/> EDU/WKP(2010)5<lb/> 28.<lb/> Note that while PISA has an age-based population definition, there is a reference to grade so that<lb/> only students in grade 7 and above are included in the sample.<lb/> 29.<lb/> TIMSS Grade 8 population definition for the 2003 survey is the following:<lb/></table>

			<p>All students enrolled in the upper of the two adjacent grades that contain the largest proportion of 13-<lb/>year-olds at the time of testing. This grade level was intended to represent eight years of schooling,<lb/> counting from the first year of primary or elementary schooling, and was the eighth grade in most<lb/> countries. (p.110, IEA, 2004)<lb/> 30.<lb/> The fact that TIMSS results are labelled as &quot; Grade 4 &quot; or &quot; Grade 8 &quot; has misled some to think that<lb/> Grade 8 students are selected from every country. The term &quot; Grade 8 &quot; refers to eight years of schooling,<lb/> rather than a grade labelled as Grade 8 in every country. Therefore, the TIMSS population definition aims<lb/> to control for the number of years of schooling. Note that while TIMSS has a grade-based sample, in fact,<lb/> there is a clear reference to age in selecting the appropriate grade for testing. That is, while the sample is<lb/> grade-based, the target population aims to capture 13 or 14-year-olds. This additional reference to age<lb/> provides some guidance to countries to select the correct grade. It helps to overcome some difficulties in<lb/> defining &quot; the first year of primary schooling &quot; , as there are variations across countries in formal and<lb/> informal education for very young children. In this sense, the desired population base of TIMSS is<lb/> somewhat age-based, but the operational population definition is grade-based.<lb/></p>

			<head>Implications of the different definitions<lb/></head>

			<p>31.<lb/> What are the implications of the differences in population definitions in PISA and TIMSS? In<lb/> broad terms, PISA examines the educational yield in the first 15 years of life of a child, and TIMSS<lb/> examines the educational yield of the first eight school grades in each country.<lb/> 32.<lb/> So one might say that PISA asks the question: &quot; In each country, what has the education system<lb/> been able to do to raise the mathematics achievement of a child by the time he/she reaches 15 years of<lb/> age? &quot; In contrast, TIMSS asks the question: &quot; What has eight years of school grades achieved in raising the<lb/> mathematics standard of a child? &quot; In PISA, if Country X had lower mean achievement than Country Y,<lb/> one might conclude, in simplistic terms, that Country X had not been able provide as much effective<lb/> education for a child in the first 15 years of his/her life. In TIMSS, if Country X had lower mean<lb/> achievement than Country Y, one might conclude, in simplistic terms, that the first eight grades of schools<lb/> in Country X had not been as effective as the first eight grades of schools in Country Y. Of course there are<lb/> many other variables (such as socio-economic status of students or culture traditions that are not easily<lb/> amenable to policy manipulation) that need to be taken into account to make conclusions about the<lb/> effectiveness of any educational system, and the pictures are usually not as simplistic as the above<lb/> examples illustrate. See Box 2.4 for further information on student background variables selected for<lb/> providing insight into the making of effective education systems.<lb/> 33.<lb/> The population definitions of PISA and TIMSS are consistent with the objectives of each survey.<lb/> If one wants to evaluate the effectiveness of the provision of an education system for each child when<lb/> he/she reaches the age at which he/she can leave school, then the PISA population definition will provide a<lb/> more comparable sample. On the other hand, if one wants to evaluate the effectiveness of instructional<lb/> practices in classrooms, then the TIMSS population definition will provide a more comparable sample as it<lb/> controls for the number of school grades a child attends. In some sense, PISA takes a look at a bigger<lb/> picture of education systems as a whole. In contrast, TIMSS focuses on specific issues of education,<lb/> namely, schools and classrooms.<lb/></p>

			<head>Comparison of age distributions<lb/></head>

			<table>34.<lb/> The international average age of students in TIMSS &quot; Grade 8 &quot; assessment was 14.5, with country<lb/> mean age ranging from 13.7 (Scotland) to 15.5 (Ghana) (See Exhibit 2, IEA, 2003, pp. 20-22), while the<lb/> international average age of PISA 2003 students was 15.8 7 , with country mean age ranging from 15.7 to<lb/> 15.9. Of course, since PISA samples are selected by age, there is little variation in student age across<lb/> countries.<lb/></table>

			<p>35.<lb/> Within each country, there is also a spread of ages. As expected, in TIMSS, the variation of age<lb/> within each country is greater than the variation of age in PISA. The following provides a summary of the<lb/> comparison of age distributions between PISA and TIMSS.<lb/> 36.<lb/> Students in PISA typically are aged between 15 years and 3 months and 16 years and 2 months,<lb/> within each country. That is, there is a one-year age span within each country&apos;s sample. The spread of<lb/> students&apos; age distribution is similar across all countries.<lb/> 37.<lb/> In TIMSS, there is typically a two-year age span in each country&apos;s student sample. However,<lb/> there are differences across countries. For example, in England, the age difference between students is<lb/> generally less than one-year, as shown in the histogram in <ref type="figure">Figure</ref> 2.1.<lb/> Figure 2.1<lb/> Age distribution of students in the TIMSS sample for England<lb/></figure>

			<table>38.<lb/> That is, both the TIMSS sample and PISA sample for England have similar spread of age<lb/> distribution, with a range of around one year.<lb/> 39.<lb/> In contrast, for Hong Kong-China, the age distribution for the TIMSS sample has a large spread,<lb/> as shown in Figure 2.2.<lb/> 7.<lb/> Computed across all countries in PISA 2003, with each country contributing equal weight.<lb/></table>

			<figure>Figure 2.2<lb/> Age distribution of students in the TIMSS sample for Hong Kong-China<lb/></figure>

			<p>40.<lb/> That is, the age of students in the TIMSS sample for Hong Kong-China covers around four years,<lb/> while the age differences of students in the Hong Kong-China PISA sample are within one year (but across<lb/> a number of grades).<lb/> 41.<lb/> The United Kingdom and Hong Kong-China examples are two extreme cases. For most<lb/> countries, the age distribution of TIMSS samples covers around two years.<lb/> 42.<lb/> The age distributions of sampled students may have an impact on student achievement. For<lb/> example, in TIMSS, different countries have different age cohorts due to country-specific regulations such<lb/> as age of entry into schools or policies on retention. The upper of the two adjacent grades that contain the<lb/> largest proportion of 13-year-olds could have more 14-year-olds than 13-year-olds, or have only 13-year-<lb/>olds. A country such as Norway, where children enter schools at a relatively younger age, may have<lb/> younger students at Grade 8, compared with countries where students enter schools at an older age.<lb/> 43.<lb/> In contrast, the PISA sample could draw students from predominantly one grade level, or from<lb/> two or more grades, depending on the distribution of 15-year-olds across grades. The implications of<lb/> multiple grades versus single grade will need further investigation. The following shows some examples of<lb/> grade distributions in PISA.<lb/></p>

			<head>Comparison of grade distributions<lb/></head>

			<p>44.<lb/> Since TIMSS samples are grade-based, all students in each country&apos;s sample are in the same<lb/> grade. In contrast, for PISA, 15-year-olds may be in a number of different grades in each country. There<lb/> are considerable variations across countries in terms of the number of different grades covered in the PISA<lb/> sample. For example, in Iceland and Japan, all students in the PISA sample are from the same grade. In<lb/> Korea, the majority of students are from the same grade, as shown in <ref type="figure">Figure</ref> 2.3.<lb/> Figure 2.3<lb/> Grade distribution of PISA sample in Korea<lb/></figure>

			<head>45.<lb/></head>

			<p>In some countries, the majority of the students in the PISA sample come from two grades, for<lb/> example, in Austria and the Czech Republic (see</p>

			<figure>Figure 2.4, for an example).<lb/> Figure 2.4<lb/> Grade distribution of PISA sample in Austria<lb/></figure>

			<table>46.<lb/> In some countries, there are many 15-year-olds who are in grades below the modal grade, such as<lb/> in Hong Kong-China and Portugal.<lb/> EDU/WKP(2010)5<lb/></table>

			<figure>Figure 2.5<lb/> Grade distribution of PISA sample in Hong Kong-China<lb/></figure>

			<p>47.<lb/> One might hypothesise that, if many 15-year-olds in a country are in grades well below the modal<lb/> grade, then an age-based sample will tend to lower the average performance of students compared to a<lb/> grade-based sample. On the other hand, in countries where there are many 15-year-olds who are in higher<lb/> grades than the modal grade, one might expect 15-year-olds to perform relatively better than just students<lb/> in a single grade, since school grade level reflects the number of years of schooling. In comparing country<lb/> performance in PISA and TIMSS, the grade distribution of the PISA sample in each country, and the age<lb/> distributions of the TIMSS samples, should be taken into account. These are discussed in more detail in<lb/> Chapter 4.<lb/></p>

			<head>Sampling methods<lb/></head>

			<p>48.<lb/> The sampling design for both PISA and TIMSS is a two-stage stratified sample for most<lb/> countries. The first stage of sampling is the selection of schools from a list of all schools satisfying the<lb/> target population definition for the survey. The selection of schools is carried out using probability-<lb/>proportional-to-size (PPS) method. That is, the probability that a particular school will be selected is<lb/> proportional to the number of eligible students in that school. When an equal number of students are then<lb/> selected from each of the chosen schools, the probability that a particular student will be chosen will be the<lb/> same for all students in the population. If PPS is not used, and each school has the same probability of<lb/> being chosen, then students from very small schools are more likely to be in the sample than students from<lb/> very large schools. Consequently, there will likely be larger standard errors and bias of estimates if PPS is<lb/> not used.<lb/> In PISA, the second stage of sampling is the selection of, typically, 35 students from each sampled school,<lb/> while, in TIMSS, the second stage of sampling is the selection of one intact class 8 . In PISA and TIMSS,<lb/> various standards have been set in relation to school level and student level exclusions, and both surveys<lb/> 8.<lb/> In most countries, one class per school was selected. But some countries selected two classes per school.<lb/> attempt to minimise the proportion of exclusions to ensure that the sample collected is representative of the<lb/> target population for each country.<lb/> 49.<lb/> Both PISA and TIMSS require a minimum of 150 schools to be selected in each country. PISA<lb/> also recommends that the total number of sampled students is at least 5250, while TIMSS requires a<lb/> minimum of 4000 students to be selected in each country.<lb/> 50.<lb/> The achieved degree of precision for estimates of mean student performance by country is not too<lb/> different in magnitude in PISA and TIMSS, with standard errors slightly smaller in PISA than in TIMSS<lb/> (see <ref type="table">Table 9</ref> in Chapter 4). For the 22 countries that participated in both PISA and TIMSS, the average<lb/> standard error for country means is 3.2 score points in PISA, and 3.3 score points in TIMSS, although the<lb/> score points are not on the same scale. If PISA scores are placed on the TIMSS scale (with the same<lb/> overall TIMSS standard deviation for the 22 countries), then the average standard error (over the 22<lb/> countries) in PISA is 2.7 score points on the TIMSS scale, about 0.6 score points less than the average<lb/> standard error in TIMSS.<lb/></p>

			<head>Test characteristics<lb/> Test length<lb/></head>

			<p>51.<lb/> In PISA, each student took two hours of testing, with a break after one hour. In TIMSS, the<lb/> testing time for each student was 90 minutes and was divided into two sessions, with a break in between.<lb/></p>

			<head>Test design<lb/></head>

			<p>52.<lb/> To allow for coverage of all mathematics content areas in the assessment, but at the same time<lb/> not placing too much burden on individual students, both PISA and TIMSS utilise matrix sampling test<lb/> design (see Box 2.1). That is, test items are placed in a number of test booklets with linking items across<lb/> booklets, and each student takes only one test booklet. In this way, each student only answers a fraction of<lb/> the test items, out of a large number of items being tested in the whole assessment.<lb/> 53.<lb/> In PISA, 13 test booklets were rotated among students, with each booklet containing items from<lb/> at least two subject domains from mathematics, reading, science and problem solving. In addition, every<lb/> booklet contained at least one mathematics item cluster. In TIMSS, 12 test booklets were rotated among<lb/> students, with each booklet containing both mathematics and science items.<lb/></p>

			<table>Box 2.1 Matrix sampling test design in PISA and TIMSS<lb/> Cluster rotation design used to form test booklets for PISA 2003<lb/> Booklet<lb/> Cluster 1<lb/> Cluster 2<lb/> Cluster 3<lb/> Cluster 4<lb/> 1<lb/> M1<lb/> M2<lb/> M4<lb/> R1<lb/> 2<lb/> M2<lb/> M3<lb/> M5<lb/> R2<lb/> 3<lb/> M3<lb/> M4<lb/> M6<lb/> PS1<lb/> 4<lb/> M4<lb/> M5<lb/> M7<lb/> PS2<lb/> 5<lb/> M5<lb/> M6<lb/> S1<lb/> M1<lb/> 6<lb/> M6<lb/> M7<lb/> S2<lb/> M2<lb/> 7<lb/> M7<lb/> S1<lb/> R1<lb/> M3<lb/> 8<lb/> S1<lb/> S2<lb/> R2<lb/> M4<lb/> 9<lb/> S2<lb/> R1<lb/> PS1<lb/> M5<lb/> 10<lb/> R1<lb/> R2<lb/> PS2<lb/> M6<lb/> 11<lb/> R2<lb/> PS1<lb/> M1<lb/> M7<lb/> 12<lb/> PS1<lb/> PS2<lb/> M2<lb/> S1<lb/> 13<lb/> PS2<lb/> M1<lb/> M3<lb/> S2<lb/> Note: M denotes a mathematics item cluster, R a reading item cluster, S a science item cluster and PS a problem solving item cluster.<lb/> To enable linking between booklets, each of these item clusters appears in four of the test booklets in each of the four possible<lb/> positions (i.e. in Cluster 1, Cluster 2, Cluster 3 or Cluster 4).<lb/> TIMSS 2003 booklet design<lb/> Student<lb/> Booklet<lb/> Part I<lb/> Part II<lb/> Block 1<lb/> Block 2<lb/> Block 3<lb/> Block 4<lb/> Block 5<lb/> Block 6<lb/> 1<lb/> M1<lb/> M2<lb/> S6<lb/> S7<lb/> M5<lb/> M7<lb/> 2<lb/> M2<lb/> M3<lb/> S5<lb/> S8<lb/> M6<lb/> M8<lb/> 3<lb/> M3<lb/> M4<lb/> S4<lb/> S9<lb/> M13<lb/> M11<lb/> 4<lb/> M4<lb/> M5<lb/> S3<lb/> S10<lb/> M14<lb/> M12<lb/> 5<lb/> M5<lb/> M6<lb/> S2<lb/> S11<lb/> M9<lb/> M13<lb/> 6<lb/> M6<lb/> M1<lb/> S1<lb/> S12<lb/> M10<lb/> M14<lb/> 7<lb/> S1<lb/> S2<lb/> M6<lb/> M7<lb/> S5<lb/> S7<lb/> 8<lb/> S2<lb/> S3<lb/> M5<lb/> M8<lb/> S6<lb/> S8<lb/> 9<lb/> S3<lb/> S4<lb/> M4<lb/> M9<lb/> S13<lb/> S11<lb/> 10<lb/> S4<lb/> S5<lb/> M3<lb/> M10<lb/> S14<lb/> S12<lb/> 11<lb/> S5<lb/> S6<lb/> M2<lb/> M11<lb/> S9<lb/> S13<lb/> 12<lb/> S6<lb/> S1<lb/> M1<lb/> M12<lb/> S10<lb/> S14<lb/> Note: M denotes mathematics and S science. To enable linking between booklets, all blocks will appear in at least two of the 12<lb/> booklets. Trend items are placed in Part I, and new items are placed in Part II, except for M5 and M6 which appear in both Part I and<lb/> Part II. Calculators are not allowed for Part I items, but they are allowed for Part II items.<lb/></table>

			<head>Amount of assessment material<lb/></head>

			<p>54.<lb/> Both PISA and TIMSS developed approximately 210 minutes of mathematics assessment<lb/> material, and these are placed in the test booklets with some duplication across the booklets 9 . Interestingly,<lb/> in PISA, the 210 minutes of test material were made up of 85 test items (with 94 score points in total),<lb/> while, in TIMSS, the 210 minutes of test material were made up of 194 items (with 215 score points in<lb/> 9.<lb/> The final 2003 tests contain 210 minutes of test material, although many more items were developed and<lb/> field tested, from which 210 minutes of test material were selected.<lb/> total). So, on average, each PISA item should take about 2.5 minutes to complete, while a TIMSS item<lb/> should take about 1 minute to complete. That is, on average, it is expected that each PISA item would take<lb/> more than twice the time to complete than a TIMSS item.<lb/> 55.<lb/> In PISA, each student took between 30 to 90 minutes of mathematics assessment (with an<lb/> average of 65 minutes), while, in TIMSS, half of the students took 30 minutes of mathematics assessment,<lb/> and the other half took 60 minutes of mathematics assessment (with an average of 45 minutes). While, on<lb/> average, PISA students were given more time to answer mathematics items, the average number of items<lb/> administered to PISA students was 26 (total of 29 score points), as compared to an average of 42 items<lb/> (total of 47 score points) administered to each TIMSS student.<lb/> 56.<lb/> The reported test reliability for TIMSS is 0.89, and 0.85 for PISA. This is not surprising, as test<lb/> reliability is closely related to the number of items (or total score points) administered to each student. It<lb/> appears that PISA developed more extended mathematics tasks in the assessment, but was not able to turn<lb/> the extended assessment into more score points. In some sense, PISA&apos;s attempt to build longer tasks to<lb/> reflect real-life contexts is at some expense of test reliability.<lb/> 57.<lb/> A more detailed comparison of item types in PISA and TIMSS is given in Chapter 3.<lb/></p>

			<head>Scaling methodology<lb/></head>

			<p>58.<lb/> Both PISA and TIMSS use item response theory (IRT) to model student responses to test items.<lb/> Item response theory is particularly useful for the matrix sampling test design in PISA and TIMSS to<lb/> &quot; equate &quot; student scores when students took different test booklets, since, in these cases, raw scores on the<lb/> tests were not directly comparable as different sets of test items were administered to each student. See<lb/> Box 2.2 for a brief description of IRT.<lb/></p>

			<figure>Box 2.2 Overview of Item Response Modelling<lb/> Principles of Item Response Theory (IRT)<lb/></figure>

			<p>Item response theory pertains to the use of mathematical functions to model the probability of success on an item<lb/> as a function of the characteristics of an item (e.g., difficulty) and characteristics of a person (e.g., ability). Typically,<lb/> item difficulty parameter and ability parameter are defined on the same measurement scale, so that the relationship<lb/> between the ability of a person and the difficulty of an item is well defined through a mathematical function. In the case<lb/> of the one-parameter item response model, the probability of success on an item is given by<lb/></p>

				<formula>(<lb/> )<lb/> (<lb/> )<lb/> δ<lb/> θ<lb/> δ<lb/> θ<lb/> −<lb/> +<lb/> −<lb/> =<lb/> exp<lb/> 1<lb/> exp<lb/> success<lb/> of<lb/> y<lb/> probabilit<lb/></formula>

			<p>whereθ is the person&apos;s ability on the measurement scale, and δ is the item difficulty on the same measurement<lb/> scale. When a person&apos;s ability is equal to the item difficulty, the probability of success is 0.5. Consequently, if a person<lb/> is administered a number of test items with known item difficulty parameters, one can make an estimate of the<lb/> person&apos;s ability based on the observed patterns of successes and failures on the items. The advantage of this<lb/> approach is that the estimate of a person&apos;s ability is invariant (within measurement error) of the particular set of items<lb/> administered. This property of the one-parameter item response model is particularly useful when there is a rotated<lb/> booklet test design where students take different sets of the items such as the booklets in PISA and TIMSS.<lb/> In addition, since person ability and item difficulty are defined on the same measurment scale, one can make<lb/> statements about a student (located at a point on the scale) with respect to his/her likelihood of being successful on<lb/> various items which are also located on the scale according to the item difficulty parameter. In this way, one can<lb/> provide descriptors of skills and knowledge to illustrate typical capcacities of students located at various poinsts on the<lb/> measurement scale.<lb/></p>

			<p>There are also many different item response models, where different mathematical functions are used to describe<lb/> the probability of success. In particular, the two-parameter and three-parameter item response models are also<lb/> commonly used. The two-parameter item response model has an additional parameter describing the discrimination<lb/> power of each item. For example, opend-ended items typically provide more discrimination power than multiple-choice<lb/> items in separating students on the measurement scale (<ref type="biblio">Routitsky &amp; Turner, 2003</ref>). The three-parameter model has an<lb/> additional &quot; guessing &quot; parameter for multiple-choice items.<lb/> In general, the choice of a particular item response model is often influenced by different schools of thought. All<lb/> item response models have theoretical and practical advantages and drawbacks. A detailed discussion on the<lb/> differences between item response models is beyond the scope of this report. Some references on item response<lb/> theory include Embretson and Reise, 2000, and van der <ref type="biblio">Linden and Hambleton, 1997.<lb/></ref> 59.<lb/> There is, however, a difference in the item response models used in PISA and TIMSS. In PISA,<lb/> the one-parameter item response model was used (<ref type="biblio">OECD, 2005</ref>), while in TIMSS, the three-parameter<lb/> model was used (IEA, 2004). There appears to be no clearly documented findings comparing the one-<lb/>parameter and three-parameter item response models. In TIMSS 1995, student responses were modelled<lb/> using both the one-parameter model and the three-parameter model, but IEA did not publish any direct<lb/> comparisons of the two scaling methods<lb/> 60.<lb/> From a theoretical point of view, the three-parameter item response model takes into account the<lb/> discrimination power of individual items, that is, to what extent the item can separate poor ability students<lb/> from high ability students, as well as guessing factors for multiple-choice items. Consequently, if test items<lb/> are greatly different in terms of discrimination power, it is possible that the two scaling methods could<lb/> produce a different &quot; spread &quot; of the student ability distributions. The ranking of the countries is likely to be<lb/> unchanged (up to the accuracy of the proficiency estimates), provided that the items do not exhibit large<lb/> differential item functioning (DIF) across countries. While DIF was checked after the field trial in PISA,<lb/> this is an issue that still remains to be investigated.<lb/> 61.<lb/> Both PISA and TIMSS use plausible values methodology (<ref type="biblio">Mislevy, 1991; Mislevy, et al, 1992</ref>)<lb/> for estimating student achievement distributions, as well as replication methods for computing standard<lb/> errors of estimates. TIMSS uses the jacknife replication method, and PISA uses the balanced repeated<lb/> replication method. Both methods use the same approach in estimating standard errors for complex<lb/> samples. For more detail, consult the technical reports of PISA and TIMSS. See Box 2.3 for a brief<lb/> description of the Plausible Values methodology.<lb/></p>

			<head>Box 2.3 Plausible Values<lb/></head>

			<p>There are at least two different approaches to estimate student achievement distributions. The first is an indirect<lb/> approach where an estimate of each student&apos;s ability is first made based on the student&apos;s item responses, and these<lb/> ability estimates are then aggregated to form population characteristics. The second approach is a direct estimation<lb/> method where a parametric model for the ability distribution is assumed (for example, as a normal distribution with<lb/> mean µ and variance σ<lb/> 2 ). The parameters of the distribution are then estimated directly using student item responses.<lb/> The plausible values methodology is a direct estimation method for population characteristics. Plausible values can be<lb/> regarded as computational tools for building the population ability distribution. This direct estimation method<lb/> overcomes some of the problems encountered in the indirect approach where inaccuracies in individual student ability<lb/> estimates cause biases in population estimates. Readers can refer to Wu (2005b) for a more detailed explanation on<lb/> plausible values.<lb/></p>

			<head>Field operation procedures<lb/></head>

			<p>62.<lb/> Field operation procedures in PISA and TIMSS are very similar, with provisions for translation<lb/> verification, marker training and marker reliability studies, national centre monitors, school visits, and a<lb/> whole suite of quality control procedures to ensure the standards of survey operations across all<lb/> participating countries. It can be said both PISA and TIMSS adopt similar field operation procedures with<lb/> only minor variations. For example, in PISA, source documents are prepared in both English and French,<lb/> and a double translation is carried out using English and French source documents separately. In TIMSS,<lb/> the source language is in English, and an independent double translation is carried out. Both PISA and<lb/> TIMSS have independent translation verification processes in place, where international translation<lb/> companies verify the translated instruments. The respective study centres then review and check for the<lb/> reconciliation of the various translations.<lb/> 63.<lb/> Test administration procedures are also very similar between the two studies, with some minor<lb/> differences such as lengths of testing time. However, policies on calculator use are somewhat different<lb/> between the two surveys. Since the use of calculators may have an impact on mathematics achievement,<lb/> the following section examines this issue more closely.<lb/></p>

			<head>Calculator use<lb/></head>

			<p>64.<lb/> PISA and TIMSS have different rulings on calculator use. PISA adopts an open position on the<lb/> use of calculators, as the following shows:<lb/> National centres decided whether calculators should be provided for their students on the basis of<lb/> standard national practice. No items in the pool required a calculator, but some items involved<lb/> solution steps for which the use of a calculator could facilitate computation. In developing the<lb/> mathematics items, test developers were particularly mindful to ensure that the items were as<lb/> calculator-neutral as possible.</p>

			<table>(p.16, OECD, 2005)<lb/> 65.<lb/> TIMSS did not allow the use of calculators in 1995 and 1999. However, in 2003, for Grade 8,<lb/> calculators were allowed for some items. For Part I of the TIMSS test booklets, calculators were not<lb/> allowed. For Part II, calculators were allowed. As for PISA, &quot; TIMSS mathematics items were designed so<lb/> that they could be answered readily without the use of a calculator (p.374, IEA, 2003) &quot; .<lb/> 66.<lb/> The differences between PISA and TIMSS in the policy on calculator use reflect differences in<lb/> the nature of the two assessments.<lb/></table>

			<p>The PISA assessment focuses on problem situations that arise from the real world, and the use of<lb/> calculators is very much a part of everyday life (whether at work or at home). However, it should be<lb/> stressed that intensive computation is not a key focus of the PISA test, and there will not be purely<lb/> computational items that depend solely on the use of the calculator. (p.365, OECD, 2005)<lb/> 67.<lb/> In contrast, TIMSS clearly regards computation skills without the use of calculators as one<lb/> important strand of mathematics, as reflected in the inclusion of mathematics items where calculators were<lb/> not allowed. For TIMSS 2003 Grade 4 assessment, no calculators were allowed.<lb/> 68.<lb/> The impact of calculator use on achievement results is examined in Chapter 4.<lb/></p>

			<head>Questionnaires<lb/> 69.<lb/></head>

			<p>Apart from the mathematics assessment instruments, contextual information about students and<lb/> schools is also collected in both PISA and TIMSS. This information is not only useful in its own right, it<lb/> also provides analytic power for gaining an in-depth understanding of achievement results. PISA<lb/> administers a student questionnaire and a school questionnaire, while TIMSS administers four<lb/> questionnaires: a student questionnaire, a teacher questionnaire, a school questionnaire and a curriculum<lb/> questionnaire. Since the sampling design of PISA does not involve the selection of intact classes, it is<lb/> difficult to administer a teacher questionnaire. As students come from different classes in each school, it<lb/> becomes difficult to identify all the teachers of sampled students and link students to teachers.<lb/> Consequently, in PISA, limited amount of teacher information is collected through the student and<lb/> school questionnaires. In contrast, TIMSS collects extensive teacher information including teacher<lb/> background, school and classroom climate, instructional approaches and implemented curriculum. In<lb/> addition, TIMSS administers a Curriculum questionnaire seeking information on the process of curriculum<lb/> development in each country, as well as the mathematics topics included in each country&apos;s curriculum.<lb/></p>

			<figure>Box 2.4 How PISA and TIMSS collected information from students in the 2003 surveys<lb/> Student questionnaire in TIMSS at Grade 8<lb/> Students took 30 minutes to answer 23 questions in total. There were seven main sections:<lb/></figure>

			<p>About you -questions about the students and their family, including date of birth, sex, number of books at<lb/> home, home possessions (16 items, of which 12 are country specific), language spoken at home,<lb/> educational level of parents and their own educational expectations.<lb/> Mathematics in school – students&apos; attitudes towards learning mathematics, how frequently students are<lb/> taught or learn in different suggested ways in mathematics lessons (14 different ways).<lb/> Science in school – students&apos; attitudes towards learning science, how frequently students are taught or learn<lb/> in different suggested ways in science lessons (14 different ways).<lb/> 70.<lb/> The extent to which student, teacher and school background information is collected has an<lb/> impact on the analyses that can be carried out. Box 2.3 presents an overview of the questions that students<lb/> answered in PISA and TIMSS in 2003. Both PISA and TIMSS examine students&apos; &quot; self-confidence &quot; and<lb/> &quot; interest and motivation &quot; in mathematics. A comparison of these results is given in Chapter 5. PISA also<lb/> collects information on students&apos; learning strategies and how learner characteristics influence mathematics<lb/> performance. TIMSS international report has a brief presentation of students&apos; socio-economic status (SES),<lb/> while PISA devotes a large section of the report to the analysis of SES in relation to achievement. TIMSS<lb/> presents a detailed report on the profiles of teachers, including cross-country comparisons of teacher<lb/> qualifications, gender, age, experience, and professional activities and support for teachers.<lb/> 71.<lb/> Both PISA and TIMSS report on school contexts for learning, including student-teacher<lb/> relationships, school resources, and classroom climate. TIMSS further presents details of the mathematics<lb/> classroom in terms of resources used in mathematics classes, content taught, and assessment methods. The<lb/> main difference between PISA and TIMSS in relation to contextual information is that classroom level<lb/> information is collected from teachers directly in TIMSS, while, in PISA, information is aggregated within<lb/> each school from students&apos; responses to questions about the classroom environment.<lb/></p>

			<head>Summary<lb/></head>

			<table>72.<lb/> The following is a summary of similarities and differences between PISA and TIMSS Grade 8<lb/> discussed in this Chapter.<lb/> Aspects of<lb/> Survey<lb/> Specific Point<lb/> Similarities<lb/> Differences<lb/> Sampling<lb/> Population versus<lb/> sample<lb/> Both surveys are sample-based<lb/> Population<lb/> definition<lb/> PISA is age-based. TIMSS is<lb/> grade-based<lb/> Age distribution<lb/> Typically, there is a two-year<lb/> age span in each country&apos;s<lb/> sample of students in TIMSS,<lb/> and one-year age span in<lb/> PISA.<lb/> Grade distribution<lb/> Typically, there are two grades<lb/> involved in each country in the<lb/> PISA sample, and one grade in<lb/> TIMSS.<lb/> Sampling method Both surveys use a two-stage<lb/> sampling method, where schools<lb/> are<lb/> first<lb/> selected<lb/> using<lb/> probability proportional to size<lb/> method.<lb/> In TIMSS, the second stage of<lb/> sampling selects intact classes.<lb/> In PISA, the second stage of<lb/> sampling selects students at<lb/> random within each school.<lb/> Aspects of<lb/> Survey<lb/> Specific Point<lb/> Similarities<lb/> Differences<lb/> Sample size<lb/> Both PISA and TIMSS require a<lb/> minimum of 150 schools to be<lb/> selected.<lb/> PISA recommends that the total<lb/> number of sampled students is at<lb/> least 5250, while TIMSS<lb/> requires a minimum of 4000<lb/> students to be selected in each<lb/> country.<lb/> Test<lb/> Characteristics<lb/> Test length<lb/> 73.<lb/> The testing time is<lb/> two hours in PISA and 90<lb/> minutes in TIMSS.<lb/> Test design<lb/> Both surveys use rotated test<lb/> booklet design. PISA uses 13<lb/> booklets. TIMSS uses 12<lb/> booklets.<lb/> Amount<lb/> of<lb/> assessment<lb/> material<lb/> Both<lb/> PISA<lb/> and<lb/> TIMSS<lb/> developed approximately 210<lb/> minutes<lb/> of<lb/> mathematics<lb/> assessment material<lb/> There are 94 total score points<lb/> in PISA, and 215 total score<lb/> points in TIMSS.<lb/> Scaling<lb/> Methodologies<lb/> Item<lb/> response<lb/> model<lb/> Both surveys use item response<lb/> modelling and plausible values<lb/> methodologies for estimating<lb/> student ability distributions<lb/> PISA uses the one-parameter<lb/> item response model. TIMSS<lb/> used the three-parameter item<lb/> response model.<lb/> Field<lb/> Operations<lb/> Translation<lb/> Both PISA and TIMSS have<lb/> translation verification process<lb/> in place.<lb/> In PISA, source documents are<lb/> prepared in both English and<lb/> French,<lb/> and<lb/> a<lb/> double<lb/> translation is carried out using<lb/> English and French source<lb/> documents<lb/> separately.<lb/> In<lb/> TIMSS, the source language is<lb/> in English, and an independent<lb/> double translation is carried<lb/> out.<lb/> Quality monitor<lb/> Both PISA and TIMSS have<lb/> similar procedures for marker<lb/> training and marker reliability<lb/> studies, national centre monitors<lb/> and school visits.<lb/> Aspects of<lb/> Survey<lb/> Specific Point<lb/> Similarities<lb/> Differences<lb/> Test<lb/> administration<lb/> Calculator use<lb/> In PISA, calculators are<lb/> allowed. In TIMSS, calculators<lb/> are allowed for only Part II of<lb/> the test.<lb/> More students have access to<lb/> calculators in PISA than in<lb/> TIMSS.<lb/> EDU/WKP(2010)5<lb/> CHAPTER 3 -COMPARISON OF PISA AND TIMSS MATHEMATICS FRAMEWORKS AND<lb/> ITEM FEATURES<lb/> Approaches to the development of the mathematics assessment frameworks<lb/> 74.<lb/> PISA and TIMSS adopted different approaches to the development of the assessment<lb/> frameworks. Each survey</table>

			<p>developed the assessment framework to meet their objectives which are<lb/> somewhat different. In PISA, the aim is to assess the extent to which education systems have prepared 15-<lb/>year-olds &quot; to play constructive roles as citizens in society &quot; (p.24, OECD, 2003), so the assessment focuses<lb/> on &quot; what are the skills citizens require to play constructive roles in society? &quot; . In TIMSS, the assessment is<lb/> to improve teaching and learning of mathematics, so the assessment provides information about student<lb/> achievement levels in relation to what students have learned in schools. This difference in the orientation<lb/> of the purposes of the two assessments led to the different approaches to the development of the<lb/> frameworks. A review of each framework is given below, followed by a comparison between the two<lb/> frameworks.<lb/></p>

			<head>TIMSS mathematics assessment framework<lb/></head>

			<p>75.<lb/> The overall design of TIMSS evolves around the TIMSS Curriculum Model where three levels of<lb/> curriculum, the intended curriculum, the implemented curriculum and the attained curriculum, form the<lb/> major organising principle of the TIMSS study (p.3. IEA, 2003b). Questionnaires designed for students,<lb/> teachers and school principals aim to capture the first two levels of curriculum structure, namely, the<lb/> intended curriculum and the implemented curriculum, while the assessment attempts to capture the attained<lb/> curriculum. TIMSS stresses that the usefulness of the TIMSS results to policy makers &quot; depends on<lb/> achievement measures being based, as closely as possible, on what students in their systems have actually<lb/> been taught &quot; (p.5. IEA, 2003b).<lb/> 76.<lb/> To ensure that the test contents are aligned with what students were taught in the participating<lb/> countries, a survey was conducted to collect information on the curricula in participating countries.<lb/> Mathematics topics that were regarded as important in a significant number of countries were included in<lb/> the framework. However, TIMSS stresses that &quot; the frameworks do not consist solely of content and<lb/> behaviours included in the curricula of all participating countries &quot; (p.5., IEA, 2003b). The following six<lb/> factors underlie the principles of the inclusion of mathematics content domains in the assessment<lb/> framework (p.5, IEA, 2003b):<lb/></p>

			<figure>• Inclusion of the content in the curricula of a significant number of participating countries;<lb/> • Alignment of the content domains with the reporting categories of TIMSS 1995 and TIMSS<lb/> 1999;<lb/> • The likely importance of the content to future developments in mathematics and science<lb/> education;<lb/> • Appropriateness for the populations of students being assessed;<lb/> • Suitability for being assessed in a large-scale international study;<lb/> • Contribution to overall test balance and coverage of content and cognitive domains.<lb/> EDU/WKP(2010)5<lb/></figure>

			<p>77.<lb/> Of the six factors listed above, the third one does not quite fit in with the principle that the<lb/> achievement measures reflect what students have actually been taught, since there is an implication that, if<lb/> a topic is deemed important for future developments in mathematics, then it may be included whether or<lb/> not students have been taught the topic. From this point of view, the TIMSS framework is not completely<lb/> driven by national curricula. The framework also seeks to set some directions for future directions in<lb/> mathematics education.<lb/> 78.<lb/> There are two organising dimensions underlying TIMSS 2003 framework: Mathematics content<lb/> domains and mathematics cognitive domains. These are discussed separately below.<lb/></p>

			<head>TIMSS mathematics content domains<lb/></head>

			<p>79.<lb/> Box 3.1 lists the five mathematics content domains in the TIMSS framework. Number, algebra,<lb/> measurement, geometry and data are familiar labels to mathematics educators, as they mirror closely the<lb/> content domains that are often found in the mathematics curricula of most countries. For more detail, refer<lb/> to the TIMSS 2003 Frameworks document (IEA, 2003b).<lb/></p>

			<head>Box 3.1 The TIMSS mathematics content domains<lb/></head>

			<p>There are five main content domains in the TIMSS 2003 mathematics assessment:<lb/></p>

			<table>Number<lb/> Whole numbers<lb/> Fractions and decimals<lb/> Integers<lb/> Ratio, proportion and percent<lb/> Algebra<lb/> Patterns<lb/> Algebraic expressions<lb/> Equations and formulas<lb/> Relationships<lb/> Measurement<lb/> Attributes and units<lb/> Tools, techniques and formula<lb/> Geometry<lb/> Lines and angles<lb/> Two-and three-dimensional shapes<lb/> Congruence and similarity<lb/> Locations and spatial relationships<lb/> Symmetry and transformations<lb/> Data<lb/> Data collection and organisation<lb/> Data representation<lb/> Data interpretation<lb/> Uncertainty and probability<lb/> 80.<lb/> The TIMSS framework document includes a list of topics covered by each content domain (see<lb/> Box 3.1), and, within each topic, a set of assessment outcomes to illustrate the specific tasks that students<lb/> will typically be assessed on.<lb/> 81.<lb/> For example, Box 3.1 lists four topics for the content area, number: whole numbers, fractions and<lb/> decimals, integers, and ratio, proportion, and percent. Within the topic area of ratio, proportion, and<lb/> percent, the assessment outcomes (topic bullets) are:<lb/> • Identify and find equivalent ratios.<lb/></table>

			<figure>• Divide a quantity in a given ratio.<lb/> • Convert percents to fractions or decimals, and vice versa.<lb/> • Solve problems involving percents.<lb/> • Solve problems involving proportions.<lb/> 82.<lb/> The TIMSS framework appears very comprehensive in its lists of the main topics and assessment<lb/> outcomes within each content area. A check of the actual items in the TIMSS 2003 tests showed that, out<lb/> of a total of 19 topics, only one topic (Data collection and organisation in the data content domain) was not<lb/> assessed in the TIMSS 2003 tests. Out of a total of 87 assessment outcomes (topic bullets), 67 were<lb/> covered by items in the actual tests 10 . However, some topic bullets were assessed by numerous items, while<lb/> others were assessed by only one item. The proportions of items in different content domains are given in<lb/> Table 3.1.<lb/></figure>

			<table>Table 3.1 Number and proportions of items in TIMSS by content domain<lb/> No. of<lb/> Items in<lb/> TIMSS<lb/> 2003<lb/> tests<lb/> Proportion<lb/> of items<lb/> TIMSS<lb/> Target<lb/> proportion<lb/> of items<lb/> International<lb/> average of<lb/> % of time<lb/> taught in<lb/> schools<lb/> Number<lb/> 57<lb/> 30%<lb/> 30%<lb/> 21 %<lb/> Algebra<lb/> 47<lb/> 24%<lb/> 25%<lb/> 27%<lb/> Measurement<lb/> 31<lb/> 16%<lb/> 15%<lb/> 10%<lb/> Geometry<lb/> 31<lb/> 16%<lb/> 15%<lb/> 26%<lb/> Data<lb/> 28<lb/> 14%<lb/> 15%<lb/> 10%<lb/> Total<lb/> 194<lb/> 100%<lb/> 100%<lb/> 94%<lb/> 1<lb/> 1.<lb/> The total does not equal 100%, as 6% of the content domains reported by teachers are not covered by the five areas listed in<lb/> the table.<lb/> 83.<lb/> The last column in Table 3.1 shows the international average of the percentage of time in<lb/> mathematics class devoted to each TIMSS content area during the school year, as reported by teachers 11 .<lb/> There is some discrepancy between the proportions of items in TIMSS tests and the average percentages<lb/> of time the content domains are taught across all TIMSS participating countries. The TIMSS mathematics<lb/> framework does not use content coverage as the sole criterion for determining the relative weights of the<lb/> 10 .<lb/> We obtained slightly different figures depending on whether we used the item list provided by the<lb/> Australian TIMSS National Research Coordinator, or the item list in the released data set.<lb/> 11 .<lb/> Figures are obtained from Exhibit 7.4 of the TIMSS 2003 International Mathematics Report (IEA, 2003).<lb/> content domains. Rather, the TIMSS framework &quot; represents a consensus among the countries participating<lb/> in TIMSS 2003 about the mathematics students at these grades should be expected to have learned. &quot; (IEA,<lb/> 2003, p.180)<lb/> TIMSS mathematics cognitive domains<lb/> 84.<lb/> TIMSS mathematics cognitive domains relate to the types of cognitive skills required in doing<lb/> mathematics, generic across all mathematics content domains. To achieve a balanced test, it is desirable to<lb/> ensure that each type of skills and abilities is covered by a sufficient number of items. There are four<lb/> cognitive domains in TIMSS 2003:<lb/> • Knowing facts and procedures<lb/> • Using concepts<lb/> • Solving routine problems<lb/> • Reasoning<lb/> 85.<lb/> These four cognitive domains are listed in order</table>

			<p>of the complexity of the tasks, from<lb/> straightforward problems to complex tasks. However, the TIMSS framework stresses that cognitive<lb/> complexity should not be confused with item difficulty, in that there is a range of item difficulties<lb/> associated with each cognitive domain (IEA, 2003b, p.25. That is, within each cognitive domain, there are<lb/> easy items as well as difficult items. As these labels of cognitive domains are not necessarily familiar to<lb/> the reader, a brief description of each cognitive domain is given below.<lb/></p>

			<head>Knowing facts and procedures<lb/></head>

			<p>86.<lb/> This cognitive domain covers basic language of mathematics and essential mathematical facts<lb/> and properties, as well as the use of mathematics for solving routine problems typically encountered in<lb/> everyday life. There are four categories of skills covered by this cognitive domain:<lb/></p>

			<figure>• Recall – e.g., knowing number facts, mathematical conventions/notations.<lb/> • Recognise/identify – e.g., recognising different representations of the number system.<lb/> • Compute – e.g., carrying out arithmetic computation, expanding algebraic expressions.<lb/></figure>

			<p>• Use tools – e.g., reading scales, using straightedge and compass.<lb/></p>

			<head>Using concepts<lb/></head>

			<p>87.<lb/> This cognitive domain is about the ability to make connections of knowledge, judge the validity<lb/> of mathematical statements and create mathematical representations. There are five categories of skills<lb/> under this cognitive domain:<lb/></p>

			<table>• Know – e.g., knowing concepts such as inclusion and exclusion, generality, mathematical<lb/> relationships.<lb/> • Classify – e.g., grouping objects, shapes, numbers according to common properties.<lb/> • Represent – e.g., presenting information using tables, diagrams, graphs; moving between<lb/> equivalent representations of mathematical relationships.<lb/> • Formulate – e.g., modelling problems or situations with equations or expressions.<lb/> EDU/WKP(2010)5<lb/> • Distinguish – e.g., identifying valid and invalid inferences from questions and answers.<lb/></table>

			<head>Solving routine problems<lb/></head>

			<p>88.<lb/> This cognitive domain relates to problem solving, where the problems are routine in that they are<lb/> typically encountered as classroom exercises or in textbooks. There are five categories of skills identified<lb/> under this cognitive domain:<lb/> • Select – e.g., choosing an appropriate algorithm or strategy to solve a problem<lb/> • Model – e.g., generating an appropriate model using equations or diagrams.<lb/></p>

			<figure>• Interpret – e.g., understanding a given model presented as equations or diagrams.<lb/> • Apply – e.g., using knowledge of facts, procedures, and concepts to solve routine problems.<lb/> • Verify/Check – e.g., Checking and evaluating the correctness and reasonableness of the solution<lb/> Reasoning<lb/> 89.<lb/> This cognitive domain is about solving non-routine problems using logical, systematic thinking<lb/> and various forms of reasoning. Eight categories of skills have been identified in relation to this cognitive<lb/> dimension:<lb/> • Hypothesize/Conjecture/Predict – e.g. discussing ideas, specifying an outcome resulting from an<lb/> unperformed operation<lb/> • Analyze – e.g. making valid inferences from given information, decomposing geometric figures<lb/> • Evaluate – e.g. critically evaluating mathematical ideas, methods, etc.<lb/> • Generalize – e.g. restating results in more widely applicable terms<lb/> • Connect – e.g. linking related mathematical ideas or objects<lb/> • Synthesize/Integrate – e.g. combining results to solve a problem<lb/> • Solve non-routine problems – e.g. applying mathematical procedures in unfamiliar contexts<lb/></figure>

			<p>• Justify/Prove – e.g. providing evidence for the validity of a statement using mathematical results<lb/> 90.<lb/> The proportions of items classified by the TIMSS cognitive domains are given in <ref type="table">Table 3</ref>.2.<lb/></p>

			<table>Table 3.2 Proportions of items in TIMSS by cognitive domains<lb/> Proportion of items in TIMSS tests<lb/> Target proportion of items<lb/> Knowing facts and procedures<lb/> 23%<lb/> 15%<lb/> Using concepts<lb/> 19%<lb/> 20%<lb/> Solving routine problems<lb/> 36%<lb/> 40%<lb/> Reasoning<lb/> 22%<lb/> 25%<lb/> Total<lb/> 100%<lb/> 100%<lb/> PISA mathematics assessment framework<lb/> 91.<lb/> The PISA mathematics framework begins with a formal definition of mathematical literacy for<lb/> OECD/PISA:<lb/></table>

			<p>Mathematical literacy is an individual&apos;s capacity to identify and understand the role that mathematics<lb/> plays in the world, to make well-founded judgements and to use and engage with mathematics in<lb/> ways that meet the needs of that individual&apos;s life as a constructive, concerned and reflective<lb/> citizen. <ref type="biblio">(p. 24, OECD, 2003)<lb/></ref> 92.<lb/> This definition, when stated alone, does not show how it differs from the mathematics<lb/> construct of TIMSS. In fact, the TIMSS framework states the following:<lb/> Prime reasons for inclusion of mathematics (in school curricula) are the increasing awareness that<lb/> effectiveness as citizens and success in the workplace are greatly enhanced by knowing and, more<lb/> important, being able to use mathematics. The number of vocations that demand a high level of<lb/> proficiency in the use of mathematics, or mathematical modes of thinking, has burgeoned with the<lb/> advance of technology, and with modern management methods. (p.30, IEA 2003b)<lb/> 93.<lb/> What makes the PISA mathematics framework different from the TIMSS framework, and<lb/> different from typical mathematics curricula in most countries, is the fact that PISA does not make the<lb/> assumption that school mathematics will necessarily prepare students to be mathematically literate in their<lb/> future lives as effective citizens. This is evident in the following:<lb/></p>

			<p>Rather than being limited to the curriculum content students have learned, the assessments focus on<lb/> determining if students can use what they have learned in the situations they are likely to encounter<lb/> in their daily lives. <ref type="biblio">(p.24, OECD, 2003)<lb/></ref> 94.<lb/> That is, PISA sets out to establish the mathematical knowledge and skills required to be<lb/> mathematically literate citizens, and assesses students on these. This is a more direct way to obtain<lb/> measures of whether students can meet future challenges in life, rather than via a proxy that school<lb/> achievement in mathematics is an indicator of students&apos; capacity to use mathematics to solve everyday<lb/> problems. This orientation may have come about in recent years when mathematics educators<lb/> observed that many students regarded school mathematics as an academic discipline divorced from<lb/> real life (e.g., <ref type="biblio">Bonotto, 2003; Verschaffel, Greer &amp; de Corte, 2000</ref>). Further, the theory of Realistic<lb/> Mathematics Education (RME) developed in the Netherlands (<ref type="biblio">de Lange, 1996; Gravemeijer, 1999</ref>) over<lb/> the past 30 years has gathered support from around the world (<ref type="biblio">de Lange, 1996; Romberg &amp; de Lange,<lb/> 1998</ref>). Two principles underlie RME: (1) Mathematics must be connected to the real-world; (2)<lb/></p>

			<p>Mathematics should be seen as a human activity.<lb/> 95.<lb/> PISA has not gone so far as claiming that schools are not preparing students adequately to be<lb/> mathematically literate citizens. But the approach PISA has adopted does not make the assumption that<lb/> schools do prepare students well. One would hope that the different approaches to organising the<lb/> mathematics assessment frameworks in PISA and TIMSS would not lead to different results, since one<lb/> of the main aims of schooling must be to prepare students for their wellbeing in their future lives. If the<lb/> two surveys yield different results that cannot be explained by methodological differences, a close<lb/> examination would be called for to understand the differences.<lb/> 96.<lb/> The PISA approach to defining mathematical literacy stems from the definition of &quot; literacy &quot;<lb/> in James Gee&apos;s &quot; Preamble to a literacy program &quot; (1998) where literacy refers to the use of language.<lb/> Each human language has words and rules, but to use a language effectively, one needs to know how<lb/> to combine words and rules in complex ways to convey a vast array of ideas. Analogous to language,<lb/> mathematics also consists of building blocks such as symbols, terms and rules. But to use mathematics<lb/> effectively, one needs to know how to combine the building blocks of mathematics to solve specific<lb/> problems. That is, mathematics literacy is much more than just knowing mathematical symbols and<lb/> rules. Mathematics literacy is about how people use their mathematical knowledge to actually solve<lb/> real-world problems.<lb/> 97.<lb/> PISA identifies three dimensions for the organisation of the mathematics framework. These<lb/> dimensions are: (1) situation or context; (2) mathematical content; and (3) mathematical processes. These<lb/> three dimensions are described below.<lb/></p>

			<head>Situation/context dimension<lb/></head>

			<p>98.<lb/> The following paragraph from the PISA framework describes the situation/context dimension:<lb/></p>

			<p>The situation is the part of the student&apos;s world in which the tasks are placed. It is located at a certain<lb/> distance from the students. For OECD/PISA, the closest situation is the students&apos; personal life; next<lb/> is school life, work life and leisure, followed by the local community and society as encountered in<lb/> daily life. Furthest away are scientific situations. Four situation-types will be defined and used for<lb/> problems to be solved: personal, educational/occupational, public, and scientific. (p. 32, OECD,<lb/> 2003).<lb/> 99.<lb/> The PISA framework specifies that, as far as possible, the target proportions of each<lb/> situation/context type should be about equal. The actual proportions of items by situation/context in the<lb/> PISA assessment are given in <ref type="table">Table 3</ref>			<table>.3.<lb/> Table 3.3 Number and proportions of mathematics items in PISA 2003 by situation/context dimension<lb/> Number of items<lb/> Proportion of items<lb/> Personal<lb/> 18<lb/> 21%<lb/> Educational/occupational<lb/> 20<lb/> 24%<lb/> Public<lb/> 29<lb/> 34%<lb/> Scientific<lb/> 18<lb/> 21%<lb/> Total<lb/> 85<lb/> 100%<lb/></table>

			<p>100.<lb/> The PISA framework also discusses situation/context from the point of view of the distance<lb/> between the problem and the mathematics involved. Tasks involving only mathematical objects without<lb/> any reference to matters outside the mathematical world are termed &quot; intra-mathematical &quot; , while tasks<lb/> involving real-world objects are termed &quot; extra-mathematical &quot; . PISA places an emphasis on extra-<lb/>mathematical tasks. Of the 85 items, only one was classified as intra-mathematical (and this one was also<lb/> classified as &quot; scientific &quot; in <ref type="table">Table 3</ref>.3), while all other items were extra-mathematical. That is, while<lb/> PISA does not preclude intra-mathematical tasks in the framework, the items in the actual assessment are<lb/> essentially all extra-mathematical.<lb/></p>

			<head>PISA mathematical content dimension – The overarching ideas<lb/></head>

			<p>101.<lb/> PISA adopts a phenomenological organisation for mathematical content as described below:<lb/> Since the goal of OECD/PISA is to assess students&apos; capacity to solve real problems, our strategy has<lb/> been to define the range of content that will be assessed using a phenomenological approach to<lb/> describing the mathematical concepts, structures or ideas. This means describing content in relation to<lb/> the phenomena and the kinds of problems for which it was created. This approach ensures a focus in<lb/> the assessment that is consistent with the domain definition, yet covers a range of content that<lb/> includes what is typically found in other mathematics assessments and in national mathematics<lb/> curricula. (p.34, OECD, 2003).<lb/> 102.<lb/> In other words, PISA considers the world around us, and categorises the tasks that are typically<lb/> encountered in everyday life, and uses these categories as the basis for organising the mathematics content.<lb/> This approach possibly explains why there is only one intra-mathematical item in the PISA test, since<lb/> intra-mathematical tasks are not often encountered in most people&apos;s everyday life, outside the school<lb/> environment.<lb/> 103.<lb/> Interestingly, PISA appears to suggest that the phenomenological approach is more inclusive than<lb/> school curriculum, as in the last sentence of the above quote, &quot; This approach … covers a range of content<lb/> that includes what is typically found … in national mathematics curricula. &quot; But from the point of view of<lb/> intra-mathematical and extra-mathematical tasks, and that PISA focuses on individual&apos;s &quot; everyday life &quot;<lb/> instead of focusing on mathematics as in &quot; how mathematics is used in the world &quot; , it would appear that<lb/> PISA&apos;s approach results in a subset of the tasks found in national mathematics curricula. This point<lb/> will be further discussed later in this report.<lb/> 104.<lb/> On the other hand, PISA&apos;s approach could be viewed as more inclusive from the point of view<lb/> that the tasks often involved skills from multiple (traditional) content domains, and no isolated knowledge<lb/> or skill is tested without checking whether these skills can be applied to real-life situations.<lb/> 105.<lb/> It should be noted that PISA makes a distinction between approaches to assessment and teaching,<lb/> as the following paragraph shows:<lb/> Mathematical concepts, structures and ideas have been invented as tools to organise the phenomena of<lb/> the natural, social and mental world. In schools, the mathematics curriculum has been logically<lb/> organised around content strands (e.g., arithmetic, algebra, geometry) and their detailed topics that<lb/> reflect historically well-established branches of mathematical thinking, and that facilitate the<lb/> development of a structured teaching syllabus. <ref type="biblio">(p. 34, OECD, 2003)<lb/></ref> 106.<lb/> It is important to recognise that, while PISA organises the mathematic content differently from<lb/> typical school mathematics curriculum, PISA does not suggest that the organisation based on<lb/> phenomenological approach is necessarily appropriate for organising a structured teaching syllabus.<lb/> Clearly, students cannot be taught tasks involving skills from multiple content domains without having<lb/> been taught basic building blocks of mathematics knowledge and procedures in each content domain. This<lb/> distinction between assessment and teaching is an important one, in that the comparison between PISA and<lb/> TIMSS is focused on assessment, and not on teaching, although there is a strong relationship between the<lb/> two.<lb/> 107.<lb/> PISA&apos;s phenomenological approach to organising mathematics content identifies four areas,<lb/> called overarching ideas. The four overarching ideas are quantity, space and shape, change and<lb/> relationships, uncertainty. The PISA mathematics framework provides the following descriptions for<lb/> each of the four overarching ideas (OECD, 2003, pp. 36-37).<lb/></p>

			<head>Quantity<lb/></head>

			<p>108.<lb/> This overarching idea focuses on the need for quantification in order to organise the world.<lb/> Important aspects include an understanding of relative size, the recognition of numerical patterns, and<lb/> the use of numbers to represent quantities and quantifiable attributes of real-world objects (counts and<lb/> measures). Furthermore, quantity deals with the processing and understanding of numbers that are<lb/> represented to us in various ways.<lb/> 109.<lb/> An important aspect of dealing with quantity is quantitative reasoning, Essential components<lb/> of quantitative reasoning are number sense, representing numbers in various ways, understanding the<lb/> meaning of operations, having a feel for the magnitude of numbers, mathematically elegant<lb/> computations, mental arithmetic and estimating.<lb/></p>

			<head>Space and shape<lb/></head>

			<p>110.<lb/> Patterns are encountered everywhere: in spoken words, music, video, traffic, building<lb/> constructions and art. Shapes can be regarded as patterns: houses, office buildings, bridges, starfish,<lb/> snowflakes, town plans, cloverleaves, crystals and shadows. Geometric patterns can serve as relatively<lb/> simple models of many kinds of phenomena, and their study is possible and desirable at all levels<lb/> (<ref type="biblio">Grünbaum, 1985</ref>).<lb/> 111.<lb/> The study of shape and constructions requires looking for similarities and differences when<lb/> analysing the components of form and recognising shapes in different representations and different<lb/> dimensions. The study of shapes is closely connected to the concept of &quot; grasping space &quot; . This means<lb/> learning to know, explore and conquer, in order to live, breathe and move with more understanding in the<lb/> space in which we live (<ref type="biblio">Freudenthal, 1973</ref>).<lb/> 112.<lb/> To achieve this requires understanding the properties of objects and their relative positions. We<lb/> must be aware of how we see things and why we see them as we do. We must learn to navigate through<lb/> space and through constructions and shapes. This means understanding the relationship between shapes<lb/> and images or visual representations, such as that between a real city and photographs and maps of the<lb/> same city. It includes also understanding how three-dimensional objects can be represented in two<lb/> dimensions, how shadows are formed and must be interpreted, what perspective is and how it functions.<lb/></p>

			<head>Change and relationships<lb/></head>

			<p>113.<lb/> Every natural phenomenon is a manifestation of change, and the world around us displays a<lb/> multitude of temporary and permanent relationships among phenomena. Examples are organisms changing<lb/> as they grow, the cycle of seasons, the ebb and flow of tides, cycles of unemployment, weather changes<lb/> and stock exchange indices. Some of these change processes involve and can be described or modelled by<lb/> straightforward mathematical functions: linear, exponential, periodic or logistic, either discrete or<lb/> continuous. But many relationships fall into different categories, and data analysis is often essential to<lb/> determine the kind of relationship that is present. Mathematical relationships often take the shape of<lb/> equations or inequalities, but relations of a more general nature (e.g., equivalence, divisibility, inclusion, to<lb/> mention but a few) may appear as well.<lb/> 114.<lb/> Functional thinking – that is, thinking in terms of and about relationships – is one of the most<lb/> fundamental disciplinary aims of the teaching of mathematics (<ref type="biblio">MAA, 1923</ref>). Relationships may be given a<lb/> variety of different representations, including symbolic, algebraic, graphical, tabular and geometrical.<lb/> Different representations may serve different purposes and have different properties. Hence translation<lb/> between representations often is of key importance in dealing with situations and tasks.<lb/></p>

			<head>Uncertainty<lb/></head>

			<p>115.<lb/> The present &quot; information society &quot; offers an abundance of information, often presented as<lb/> accurate, scientific and with a degree of certainty. However, in daily life we are confronted with uncertain<lb/> election results, collapsing bridges, stock market crashes, unreliable weather forecasts, poor predictions for<lb/> population growth, economic models that don&apos;t align, and many other demonstrations of the uncertainty of<lb/> our world.<lb/> 116.<lb/> Uncertainty is intended to suggest two related topics: data and chance. These phenomena are<lb/> respectively the subject of mathematical study in statistics and probability. Relatively recent<lb/> recommendations concerning school curricula are unanimous in suggesting that statistics and probability<lb/> should occupy a much more prominent place than has been the case in the past (Committee of Inquiry into<lb/> the Teaching of Mathematics in <ref type="biblio">Schools, 1982; LOGSE, 1990; MSEB, 1990; NCTM, 1989</ref>; NCTM,<lb/> 2000)..<lb/> 117.<lb/> Specific mathematical concepts and activities that are important in this area are collecting data,<lb/> data analysis and display/visualisation, probability and inference.<lb/> 118.<lb/> Table 3.4 shows the number and proportion of PISA items classified according to the<lb/> overarching ideas.<lb/> Table 3.4 Number and proportions of mathematics items in PISA 2003 by overarching ideas<lb/> Number of items<lb/> Proportion of items<lb/> Quantity<lb/> 23<lb/> 27.0%<lb/> Space and shape<lb/> 20<lb/> 23.5%<lb/> Change and relationships<lb/> 22<lb/> 26.0%<lb/> Uncertainty<lb/> 20<lb/> 23.5%<lb/> Total<lb/> 85<lb/> 100.0%<lb/></table>

			<head>Mathematical processes dimension<lb/></head>

			<p>119.<lb/> The PISA mathematics framework deems the mathematical processes dimension to be the most<lb/> important one, and devotes lengthy discussions to it. As an introduction to describing the mathematical<lb/> processes dimension, the PISA framework begins with a description of the process of mathematisation<lb/> which characterises the way mathematics problems are solved in the real world (OECD, 2003, p.27).<lb/></p>

			<figure>120.<lb/> There are five steps that characterise the process of mathematisation (OECD, 2003, p.38):<lb/> 1. Starting with a problem situated in reality<lb/> 2. Organising it according to mathematical concepts<lb/> 3. Gradually trimming away the reality through processes such as making assumptions about which<lb/> features of the problem are important, generalising and formalising (which promote the<lb/> mathematical features of the situation and transform the real problem into a mathematical<lb/> problem that faithfully represents the situation<lb/> 4. Solving the mathematical problem<lb/> 5. Making sense of the mathematical solution in terms of the real situation.<lb/> 121.<lb/> The cycle of mathematisation involves an iterative process of moving between a real-world<lb/> problem to a mathematical problem, and then moving from a mathematical problem to a mathematical<lb/> solution and then to a real solution. The process ends with making reflections of the solutions in terms of<lb/> the real-world problem.<lb/> The Competencies<lb/> 122.<lb/> To be able to successfully carry out the mathematisation process, an individual will need to draw<lb/> upon a number of competencies. PISA mathematics framework identifies eight competencies in relation to<lb/> the mathematisation process:<lb/> • Thinking and reasoning<lb/> • Argumentation<lb/> • Communication<lb/> • Modelling<lb/> • Problem posing and solving<lb/> • Representation<lb/> • Using symbolic, formal and technical language and operations<lb/> • Use of aids and tools<lb/> 123.<lb/> Note that each of the above competencies can be described at different levels. The<lb/> mathematisation process required to solve a particular problem may draw upon different competencies at<lb/> different levels.<lb/></figure>

			<p>124.<lb/> While an explication of these competencies is useful in teaching and learning, it is difficult to<lb/> assess these competencies individually, since problem-solving tasks typically involve a combination of the<lb/> competencies, and, in a large-scale assessment, the behaviours of students in relation to each competency<lb/> would be difficult to observe. Consequently, PISA summarises the competencies into three broad clusters:<lb/> the reproduction cluster, the connections cluster, and the reflection cluster. Each cluster involves all eight<lb/> competencies, but at different levels.<lb/> 125.<lb/> The PISA framework provides the following definitions for the three competency clusters.<lb/></p>

			<head>The reproduction cluster<lb/></head>

			<p>The competencies in this cluster essentially involve reproduction of practised knowledge. They<lb/> include those most commonly used in standardised assessments and classroom tests. These<lb/> competencies are knowledge of facts and of common problem representations, recognition of<lb/> equivalents, recollection of familiar mathematical objects and properties, performance of routine<lb/> procedures, application of standard algorithms and technical skills, manipulation of expressions<lb/> containing symbols and formulae in standard form, and carrying out computations. (p.42, OECD,<lb/> 2003)<lb/></p>

			<head>The connections cluster<lb/></head>

			<p>The connections cluster competencies build on the reproduction cluster competencies in taking<lb/> problem solving to situations that are not simply routine, but still involved familiar, or quasi-familiar,<lb/> settings. (p.43, OECD, 2003)<lb/></p>

			<head>The reflection cluster<lb/></head>

			<p>The competencies in this cluster include an element of reflectiveness on the part of the student about<lb/> the processes needed or used to solve a problem. They relate to students&apos; abilities to plan solution<lb/> strategies and implement them in problem settings that contain more elements and may be more<lb/> &quot; original &quot; (or unfamiliar) than those in the connections cluster. (p.46, OECD, 2003)<lb/></p>

			<head>Classifying items according to competency clusters<lb/></head>

			<p>126.<lb/> Each PISA item was classified into one of the three competency clusters. The classification<lb/> process involved an examination of the levels of the eight competencies required to answer an item. An<lb/> item is assigned to a competency cluster according to the highest level of the competencies required. The<lb/> following table shows the number of PISA items in each of the competency clusters.<lb/></p>

			<table>Table 3.5 Number and proportions of mathematics items in PISA 2003 by competency clusters<lb/> Number of items<lb/> Proportion of items<lb/> Reproduction<lb/> 26<lb/> 31%<lb/> Connection<lb/> 40<lb/> 47%<lb/> Reflection<lb/> 19<lb/> 22%<lb/> Total<lb/> 85<lb/> 100%<lb/></table>

			<head>A Comparison of PISA and TIMSS Mathematics Frameworks<lb/></head>

			<p>127.<lb/> TIMSS mathematics framework identifies two dimensions to ensure coverage of mathematics<lb/> assessment tasks: content domains and cognitive domains. PISA mathematics framework identifies three<lb/> dimensions for coverage of mathematics assessment tasks: situation/context dimension, content dimension<lb/> and processes dimension. Of these three dimensions, PISA&apos;s content dimension can be related to TIMSS&apos;<lb/> content domains, while PISA&apos;s processes dimension can be related to TIMSS&apos; cognitive domains.<lb/> 128.<lb/> TIMSS does not explicitly state a situation/context dimension as PISA does. This is not<lb/> surprising, since PISA stresses on assessing students&apos; capacity to solve problems encountered in life, PISA<lb/> items are typically embedded within some situation/context. In contrast, to test knowledge and skills based<lb/> on curriculum topics, many TIMSS items do not involve matters outside the mathematical world ( &quot; intra-<lb/>mathematical &quot; ). This is an important distinction between the assessments of PISA and TIMSS.<lb/></p>

			<head>A comparison of PISA&apos;s content dimension with TIMSS&apos; content dimension<lb/></head>

			<p>129.<lb/> TIMSS&apos; content dimension identifies five content domains: number, algebra, measurement,<lb/> geometry and data. These content domains are familiar to most mathematics educators as many school<lb/> mathematics curricula and textbooks are organised around these content domains. In contrast, PISA&apos;s<lb/> content dimension consists of four overarching ideas: quantity, space and shape, change and<lb/> relationships, and uncertainty. This classification is less familiar to mathematics educators. To facilitate<lb/> comparisons between PISA overarching ideas and TIMSS content domains, PISA items were classified<lb/> according to TIMSS content domains, as shown in <ref type="table">Table 3</ref>.6. Discussions about the relationships between<lb/> each PISA overarching idea and the TIMSS content domains are given following <ref type="table">Table 3</ref>.6.<lb/></p>

			<head>Number of PISA items by TIMSS content domains<lb/></head>

			<table>Table 3.6 Tally of PISA items classified by PISA overarching ideas and TIMSS content domains<lb/> PISA overarching ideas<lb/> Quantity<lb/> Space and<lb/> shape<lb/> Change and<lb/> relationships<lb/> Uncertainty<lb/> Total<lb/> TIMSS<lb/> content<lb/> domains<lb/> Number<lb/> 23<lb/> 1<lb/> 3<lb/> 5<lb/> 32<lb/> Algebra<lb/> 7<lb/> 7<lb/> Measurement<lb/> 6<lb/> 2<lb/> 8<lb/> Geometry<lb/> 12<lb/> 12<lb/> Data<lb/> 1<lb/> 10<lb/> 15<lb/> 26<lb/> Total<lb/> 23<lb/> 20<lb/> 22<lb/> 20<lb/> 85<lb/></table>

			<p>Quantity<lb/> 130.<lb/> This overarching idea is not dissimilar to the number strand in TIMSS. It can be seen from <ref type="table">Table<lb/> 3</ref>.6 that all 23 quantity items were classified as TIMSS number content domain. However, there are quite a<lb/> few non quantity items also classified as TIMSS number strand. That is, it appears that the PISA quantity<lb/> domain is a subset of TIMSS number domain. Box 3.2 shows a PISA quantity item that has been classified<lb/> as number against TIMSS content domains.<lb/></p>

			<figure>Box 3.2 An example PISA quantity item classified as number against TIMSS content domains<lb/> EXCHANGE RATE<lb/> Mei-Ling from Singapore was preparing to go to South Africa for 3 months as an exchange<lb/> student. She needed to change some Singapore dollars (SGD) into South African Rand (ZAR).<lb/></figure>

			<head>Question 1: EXCHANGE RATE<lb/> M413Q01 -0 1 9<lb/></head>

			<p>Mei-Ling found out that the exchange rate between Singapore dollars and South African Rand<lb/> was:<lb/> 1 SGD = 4.2 ZAR<lb/></p>

			<figure>Mei-Ling changed 3000 Singapore dollars into South African Rand at this exchange rate.<lb/> How much money in South African Rand did Mei-Ling get?<lb/> 131.<lb/> Box 3.3 shows a PISA item that has been classified as TIMSS number content domain, but as<lb/> PISA space and shape overarching idea. For this item, while the underlying operation to be carried out is<lb/> division (which is part of the number content domain), but the mathematisation process relates to dealing<lb/> with space and shape to identify the relevant information before the division operation could be carried out.<lb/> EDU/WKP(2010)5<lb/> Box 3.3 An example PISA quantity item classified as number against TIMSS content domains<lb/> STAIRCASE<lb/> The diagram below illustrates a staircase with 14 steps and a total height of 252 cm:<lb/> Question 1: STAIRCASE<lb/> M547Q01<lb/> What is the height of each of the 14 steps?<lb/> Height = .</figure>

			<head>................................................ cm.<lb/></head>

			<p>Space and shape<lb/> 132.<lb/> Judging from the PISA items that have been classified as space and shape, it appears that this<lb/> overarching idea covers some topics of geometry and measurement as defined by the TIMSS<lb/> framework, for example, two-and three-dimensional shapes and estimates of length, circumference,<lb/> area and volume. However, checking through the list of TIMSS topics under measurement and<lb/> geometry, it appears that many topics are not cover by PISA space and shape domain, such as those<lb/> listed under lines and angles, or under congruence and similarity. In TIMSS, measurement and geometry<lb/> cover a significant number of topics of formal definitions and operations involving lines, angles,<lb/> polygons, Euclidean and Coordinate Geometry. Since these are often intra-mathematical, PISA does<lb/> not seem to cover these kinds of knowledge and skills. A check in</p>

			<table>Table 3.6 shows that most of the<lb/> PISA space and shape items are classified as geometry or measurement under TIMSS classification<lb/> scheme. But not all TIMSS geometry and measurement items are included under PISA space and<lb/> shape overarching idea.<lb/> 133.<lb/> Box 3.4 shows a PISA item classified as space and shape in PISA, and as measurement<lb/> against TIMSS content domains.<lb/> Step-height<lb/> Step-depth<lb/> Total height 252 cm<lb/> Total depth 400 cm<lb/> Plateau<lb/> EDU/WKP(2010)5<lb/> 42<lb/> Box 3.4 An example PISA space and shape item classified as measurement against TIMSS content domains<lb/> CARPENTER<lb/> Question 1: CARPENTER<lb/> M266Q01<lb/> A carpenter has 32 metres of timber and wants to make a border around a garden bed. He is<lb/> considering the following designs for the garden bed.<lb/> Circle either Yes or No for each design to indicate whether the garden bed can be made with 32<lb/> metres of timber.<lb/> Garden bed design<lb/> Using this design, can the garden bed be made with 32<lb/> metres of timber?<lb/> Design A<lb/> Yes / No<lb/> Design B<lb/> Yes / No<lb/> Design C<lb/> Yes / No<lb/> Design D<lb/> Yes / No<lb/> Change and relationships<lb/> 134.<lb/> From the definitions for this overarching idea, it seems reasonable to map the overarching idea<lb/> of change and relationships to the traditional curriculum strand algebra. Indeed, the four PISA items<lb/> classified as TIMSS content domain algebra are change and relationships items in PISA (see Table 3.6).<lb/> 10 m<lb/> 6 m<lb/> 10 m<lb/> 10 m<lb/> 10 m<lb/> 6 m<lb/> 6 m<lb/> 6 m<lb/> However, a large number of items classified as change and relationships in PISA are classified by other<lb/> traditional strands, with the most number in the data strand. This may not be surprising as the<lb/> descriptions linking change and relationships to natural phenomenon include statistical data such as for<lb/> unemployment or the stock exchange. Looking down the column of change and relationships in Table<lb/> 3.6, the items appear as number, algebra, measurement, and data items by TIMSS content domains. This<lb/> shows that the overarching idea, change and relationships, plays a part in most of the traditional<lb/> mathematics strands. From this point of view, change and relationships is probably the least well<lb/> matched overarching idea among the four to traditional curriculum strands.<lb/> 135.<lb/> Box 3.5 shows two PISA change and relationships items classified as TIMSS measurement<lb/> content domain<lb/></table>

			<figure>Box 3.5 An example PISA change and relationships item classified as measurement against TIMSS content<lb/> domains<lb/> INTERNET RELAY CHAT<lb/></figure>

			<p>Mark (from Sydney, Australia) and Hans (from Berlin, Germany) often communicate with each<lb/> other using &quot; chat &quot; on the Internet. They have to log on to the Internet at the same time to be able<lb/> to &quot; chat &quot; .<lb/></p>

			<p>To find a suitable time to &quot; chat &quot; , Mark looked up a chart of world times and found the following:<lb/></p>

			<head>Question 1: INTERNET RELAY CHAT<lb/> M402Q01 -0 1 9<lb/></head>

			<p>At 7:00 PM in Sydney, what time is it in Berlin?<lb/></p>

			<head>Question 2: INTERNET RELAY CHAT<lb/> M402Q02 -0 1 9<lb/></head>

			<p>Mark and Hans are not able to chat between 9:00 AM and 4:30 PM their local time, as they have<lb/> to go to school. Also, from 11:00 PM till 7:00 AM their local time they won&apos;t be able to chat<lb/> because they will be sleeping.<lb/></p>

			<p>When would be a good time for Mark and Hans to chat? Write the local times in the table.<lb/></p>

			<head>Place<lb/> Time<lb/></head>

			<p>Sydney<lb/></p>

			<table>Berlin<lb/> Greenwich 12 Midnight<lb/> Berlin 1:00 AM<lb/> Sydney 10:00 AM<lb/> Uncertainty<lb/> 136.<lb/> This overarching idea can be linked to TIMSS&apos; data strand. Interestingly, while this strand<lb/> covers chance and data, TIMSS chooses to label it data, and PISA chooses to label it uncertainty (chance).<lb/> It might be less confusing if both surveys just use the label chance and data as for traditional curriculum<lb/> mathematics strand. Of the 20 items classified as uncertainty in PISA, 15 are classified as data, but five are<lb/> classified as number. These five items involve computing averages and percentages, which could be<lb/> regarded as number or data (statistics). Box 3.6 shows a PISA uncertainty item classified as number (topic<lb/> percentages) against TIMSS content domains.<lb/></table>

			<figure>Box 3.6 An example PISA uncertainty item classified as number against TIMSS content domains<lb/> EXPORTS<lb/> The graphics below show information about exports from Zedland, a country that uses zeds as its<lb/> currency.<lb/> Question 2: EXPORTS<lb/> M438Q02<lb/> What was the value of fruit juice exported from Zedland in 2000?<lb/> 137.<lb/> The last column in Table 3.6 shows the number of PISA items as classified by TIMSS content<lb/> domains. Notably there are few PISA items in the algebra domain, and a relatively large number of<lb/> 20.4<lb/> 25.4<lb/> 27.1<lb/> 37.9<lb/> 42.6<lb/> 0<lb/> 5<lb/> 10<lb/> 15<lb/> 20<lb/> 25<lb/> 30<lb/> 35<lb/> 40<lb/> 45<lb/> 1996<lb/> 1997<lb/> 1998<lb/> 1999<lb/> 2000<lb/></figure>

			<table>Total annual exports from Zedland in<lb/> millions of zeds, 1996-2000<lb/> Distribution of exports from<lb/> Zedland in 2000<lb/> Year<lb/> Tobacco<lb/> 7%<lb/> Wool<lb/> 5%<lb/> Cotton fabric<lb/> 26%<lb/> Fruit juice<lb/> 9%<lb/> Rice<lb/> 13%<lb/> Tea<lb/> 5%<lb/> Meat<lb/> 14%<lb/> Other<lb/> 21%<lb/></table>

			<p>items in the number and data domains. This is not surprising, given that PISA defines the assessment<lb/> through an examination of the range of quantitative reasoning used by citizens in everyday life in<lb/> situations such as &quot; shopping, travelling, cooking, dealing with personal finances, judging political<lb/> issues, etc. &quot; (p.24, OECD, 2003). Such a list of situations/contexts appears to preclude applications of<lb/> advanced mathematics in very specialised fields, such as the use of transformational geometry in<lb/> animation, solving differential equations in engineering, forecasting trends and building mathematical<lb/> models using calculus. While these applications may well be beyond that expected of 15-year-olds and<lb/> do not appear in either PISA or TIMSS, the underlying mathematical concepts stem from algebra. That<lb/> is, while ordinary citizens are not required to know a great deal of algebra to become mathematically<lb/> literate citizens, specialists in mathematics do need to have knowledge in algebra. Algebra is needed for<lb/> the advance of the modern world, but needed only by a few specialists who can work with technological<lb/> developments This seems the key distinction between the orientations of the PISA and TIMSS<lb/> frameworks, where PISA focuses on everyday needs of citizens in terms of using mathematics, while<lb/> TIMSS focuses on mathematics as a discipline to be used for potential applications in all fields.<lb/> 138.<lb/> Finally, with regard to mathematics content classifications, PISA does not provide further<lb/> breakdown beyond the overarching ideas. This is a mechanism to avoid testing fragments of skills. While<lb/> this reflects the emphasis on literacy in PISA, for the purposes of this report such a broad classification<lb/> scheme poses a potential problem. The classification of a PISA item into one traditional mathematics<lb/> content domain is a matter of judgement, as the framework is not designed with this in mind. In<lb/> particular it is challenging to map items in the change and relationships overarching idea to traditional<lb/> content domains. Consequently, it is acknowledged that the classification of PISA test items by traditional<lb/> mathematics content domains could vary considerably.<lb/></p>

			<head>Number of TIMSS items by PISA overarching ideas<lb/></head>

			<p>139.<lb/> A further comparison between PISA and TIMSS frameworks is illustrated by a re-classification<lb/> of TIMSS items by PISA overarching ideas. <ref type="table">Table 3</ref>.7 shows the distribution of TIMSS items cross-<lb/>classified by TIMSS content domains and PISA overarching ideas.<lb/></p>

			<table>Table 3.7 Tally of TIMSS items classified by PISA overarching ideas and TIMSS content domains<lb/> PISA overarching ideas<lb/> Quantity<lb/> Space and<lb/> shape<lb/> Change and<lb/> relationships<lb/> Uncertainty<lb/> Total<lb/> TIMSS<lb/> content<lb/> domains<lb/> Number<lb/> 51<lb/> 3<lb/> 3<lb/> 57<lb/> Algebra<lb/> 13<lb/> 1<lb/> 33<lb/> 47<lb/> Measurement<lb/> 15<lb/> 15<lb/> 1<lb/> 31<lb/> Geometry<lb/> 30<lb/> 1<lb/> 31<lb/> Data<lb/> 1<lb/> 10<lb/> 17<lb/> 28<lb/> Total<lb/> 80<lb/> 49<lb/> 48<lb/> 17<lb/> 194<lb/> Table 3.7 shows that PISA overarching idea Quantity is closely related to TIMSS Number content domain,<lb/> Space and Shape to TIMSS Geometry and Measurement content domains, Change and relationships to<lb/> TIMSS Algebra content domain, and Uncertainty to TIMSS Data content domain. In addition, the TIMSS<lb/> test does not have many items classified as PISA Uncertainty overarching idea.<lb/></table>

			<head>A comparison of PISA&apos;s processes dimension with TIMSS&apos; cognitive domains<lb/></head>

			<p>140.<lb/> The descriptions for PISA&apos;s processes dimension are not dissimilar to those for TIMSS cognitive<lb/> domains. Both are concerned with cognitive demands (other than mathematics content) in the process of<lb/> solving a mathematics problem. Below are some comparisons between the three PISA competency clusters<lb/> and the four TIMSS cognitive domains.<lb/> 141.<lb/> The PISA reproduction cluster can be linked to TIMSS cognitive domain knowing facts and<lb/> procedures, and perhaps also covers some of the skills listed in TIMSS using concepts domain. The PISA<lb/> connections cluster can be related to TIMSS using concepts and solving routine problems domains. The<lb/> PISA reflection cluster can be linked to TIMSS reasoning domain, where non-routine problems are<lb/> presented to students.<lb/> 142.<lb/> To facilitate comparisons between PISA competency clusters and TIMSS cognitive domains,<lb/> PISA items were classified according to TIMSS cognitive domains.</p>

			<table>Table 3.8 shows a tally of the PISA<lb/> items cross-classified according to PISA competency clusters and TIMSS cognitive domains.<lb/> Table 3.8 Tally of PISA items classified by PISA competency clusters and TIMSS cognitive domains<lb/> PISA competency clusters<lb/> Reproduction<lb/> Connections<lb/> Reflection<lb/> Total<lb/> TIMSS cognitive<lb/> domains<lb/> Knowing facts<lb/> and procedures<lb/> 15<lb/> 8<lb/> 0<lb/> 23<lb/> Using concepts<lb/> 5<lb/> 8<lb/> 1<lb/> 14<lb/> Solving routine<lb/> problems<lb/> 4<lb/> 8<lb/> 3<lb/> 15<lb/> Reasoning<lb/> 2<lb/> 16<lb/> 15<lb/> 33<lb/> Total<lb/> 26<lb/> 40<lb/> 19<lb/> 85<lb/> 143.<lb/> As expected, most of the items classified as PISA reproduction items are classified as TIMSS<lb/> knowing facts and procedures items. And most</table>

			<p>of the items classified as PISA reflection items are<lb/> classified as TIMSS reasoning items. However, for items classified as PISA connections items, there is a<lb/> spread across the TIMSS classifications, but with more items in the TIMSS reasoning domain. Overall,<lb/> the proportions of PISA items classified by TIMSS cognitive domains are quite different from the<lb/> proportions of TIMSS items in the cognitive domains (compare with <ref type="table">Table 3</ref>.2) where the most number of<lb/> TIMSS items are in the solving routine problems domain, and the most number of PISA items are in the<lb/> reasoning domain. It appears that PISA has succeeded in moving a little away from routine problem<lb/> solving, and moving towards assessing students&apos; ability to solve non-routine problems. However, about<lb/> one quarter of the items are still in the knowing facts and procedures domain, where students are assessed<lb/> on recall and applications of basic procedures. The inclusion of these lower level items is necessary,<lb/> otherwise students at lower levels on the mathematical proficiency scale will not find PISA tests<lb/> accessible. Nevertheless, the differences between PISA and TIMSS in the distributions of items across the<lb/> cognitive domains reflect the different approaches to the development of the frameworks.<lb/></p>

			<head>Characteristics of tests and items<lb/></head>

			<p>144.<lb/> In this section, item features, such as item format, unit structure, and amount of reading involved,<lb/> are compared between PISA and TIMSS.<lb/></p>

			<head>Item Format<lb/></head>

			<p>145.<lb/> Both PISA and TIMSS use multiple-choice and constructed-response item formats, as both<lb/> surveys recognise that a multiple-choice format is not suited for students to demonstrate their abilities to<lb/> communicate solutions, make interpretations, construct models and perform other more complex tasks.<lb/> On the other hand, the cost of scoring constructed-response items can become prohibitively expensive in<lb/> large-scale assessments, so the objectively scored multiple-choice item format is also used. <ref type="table">Table 3.9<lb/></ref> shows the proportions of items in multiple-choice or constructed-response format for the two surveys.<lb/></p>

			<table>Table 3.9 Proportions of items by item format<lb/> PISA<lb/> TIMSS<lb/> Multiple-choice<lb/> 33%<lb/> 66%<lb/> Constructed-response<lb/> 67%<lb/> 34%<lb/> 146.<lb/> <ref type="table">Table 3</ref>.9 shows that PISA has far more items in constructed-response format than TIMSS. In<lb/> fact, two thirds of the items in PISA are in constructed-response format, while only one third of the items<lb/> in TIMSS are in constructed-response format.<lb/> 147.<lb/> In general, constructed-response items are more discriminating than multiple-choice items (e.g.,<lb/> <ref type="biblio">Routitsky and Turner, 2003</ref>), so one would expect PISA tests to show higher test reliability than TIMSS if<lb/> the same number of items is administered. However, TIMSS tests have a higher test reliability than PISA<lb/> tests (see the section Amount of assessment material in Chapter 2), as more items are administered in<lb/> TIMSS, on average, to each student, even though the administration time is shorter. That is, TIMSS items<lb/> tend to be shorter items, while PISA items require more time to answer. For example, PISA items have<lb/> more words in the questions, and will require more time to process the information. The following section<lb/> compares the amount of reading in PISA and TIMSS.<lb/></p>

			<head>Amount of reading<lb/></head>

			<p>148.<lb/> PISA items involve more reading than TIMSS items. To convey real-world problems situations,<lb/> more words are required to explain about the problem setting, the constraints, and other parameters that<lb/> will need to be assumed. A sample of items was randomly selected from PISA and TIMSS, and the number<lb/> of words in the stem of each item is recorded, as shown in</p>

			<table>Table 3.10.<lb/> Table 3.10 Number of words in item stem in randomly selected PISA and TIMSS items<lb/> TIMSS cluster M02<lb/> Question number<lb/> Number of words in<lb/> item stem<lb/> PISA booklet 3<lb/> Question number<lb/> Number of words in<lb/> item stem<lb/> 1<lb/> 19<lb/> 1<lb/> 55<lb/> 2<lb/> 25<lb/> 2<lb/> 59<lb/> 3<lb/> 32<lb/> 3<lb/> 33<lb/> 4<lb/> 20<lb/> 4<lb/> 72<lb/> 5<lb/> 23<lb/> 5<lb/> 30<lb/> 6<lb/> 34<lb/> 6<lb/> 130<lb/> 7<lb/> 5<lb/> 7<lb/> 78<lb/> 8<lb/> 9<lb/> 8<lb/> 33<lb/> 9<lb/> 55<lb/> 9<lb/> 78<lb/> 10<lb/> 35<lb/> 10<lb/> 12<lb/> 11<lb/> 18<lb/> 11<lb/> 34<lb/> 12<lb/> 18<lb/> 12<lb/> 50<lb/> 13<lb/> 11<lb/> 13<lb/> 53<lb/> 14<lb/> 6<lb/> 14<lb/> 23<lb/> 15<lb/> 22<lb/> 15<lb/> 101<lb/> Mean<lb/> 22<lb/> Mean<lb/> 56<lb/> Standard deviation<lb/> 13<lb/> Standard deviation<lb/> 32<lb/> Standard error<lb/> 3.4<lb/> Standard error<lb/> 7.9<lb/> 149.<lb/> From a random sample of 15 items in TIMSS and 15 items in PISA, the average number of<lb/> words in a PISA item stem is about twice as many as the average number of words in a TIMSS item stem.<lb/> EDU/WKP(2010)5<lb/></table>

			<p>That is, the reading load is considerably heavier in PISA than in TIMSS. In addition, PISA items require<lb/> more interpretation of the problem statements over and above the straightforward reading of words.<lb/></p>

			<head>Unit structure<lb/></head>

			<p>150.<lb/> Given the amount of texts required in PISA to explain problem settings and contexts, it is often<lb/> more efficient to ask more than one question within a problem setting, to reduce the amount of reading for<lb/> each individual task. In cases where more than one question is asked in relation to one stimulus material,<lb/> the group of questions is referred to as a unit. In PISA, 41% of the items are stand-alone items, while the<lb/> rest are grouped in units. In TIMSS, 85% of the items are stand-alone items. The effect of having unit<lb/> structures instead of stand-alone items is that items within a unit are typically more similar to each other,<lb/> thus violating the local independence assumption of items under item response modelling. That is, items<lb/> within a unit could potentially collect the same piece of information, rather than independent pieces of<lb/> information, about a student&apos;s proficiency being assessed. This, of course, will be the extreme case. In fact,<lb/> in PISA, care was taken to ensure that the correctness of a response for an item would not depend on the<lb/> correctness of the answer on another item. But it is possible that students may have some familiarity (or<lb/> unfamiliarity) with particular contexts and situations, and the correctness of the responses to items within a<lb/> unit could be a little more similar than they would have been, had the contexts/situations been completely<lb/> different. Therefore, instead of having collected, say, 20 independent pieces of information, one had only<lb/> collected 18 pieces of information, Consequently, the reported test reliability could be a little inflated. The<lb/> achievement scores, however, are not expected to be biased due to the use of units in the assessment, since<lb/> duplicate information would still be &quot; correct &quot; information.<lb/></p>

			<head>Summary<lb/> 151.<lb/> This chapter compares PISA and TIMSS frameworks and notes similarities</head>

			<p>and differences. The<lb/> extent of the impact of these differences on achievement results is discussed in Chapter 4. The following is<lb/> a summary of similarities and differences between the PISA and TIMSS frameworks.<lb/></p>

			<head>Aspects of the<lb/> Frameworks<lb/></head>

			<table>Similarities<lb/> Differences<lb/> Approach<lb/> Both PISA and TIMSS framework<lb/> development<lb/> involved<lb/> extensive<lb/> consultative<lb/> processes<lb/> with<lb/> participating<lb/> countries<lb/> and<lb/> mathematics education experts.<lb/> PISA framework is primarily expert driven<lb/> with endorsements by countries. TIMSS<lb/> framework is primarily country driven with<lb/> endorsements by experts.<lb/> Organising<lb/> principles<lb/> Both PISA and TIMSS organise the<lb/> framework with content and cognitive<lb/> dimensions<lb/> Content<lb/> organisation<lb/> TIMSS adopts traditional mathematics<lb/> content domains: Number, algebra,<lb/> measurement, geometry and data.<lb/> PISA uses phenomenological approach to<lb/> categorise problems based on the kinds of<lb/> applications<lb/> of<lb/> mathematics.<lb/> Four<lb/> overarching ideas are identified: quantity,<lb/> space<lb/> and<lb/> shape,<lb/> change<lb/> and<lb/> relationships, uncertainty.<lb/> Content balance<lb/> TIMSS covers a wider range of curriculum<lb/> contents than PISA. PISA has few items in<lb/> algebra, measurement and geometry, but<lb/> more items in number and data.<lb/> Cognitive<lb/> dimension<lb/> While the labels are different for areas<lb/> of the cognitive dimension, both PISA<lb/> and<lb/> TIMSS<lb/> describe<lb/> cognitive<lb/> processes (or competencies) in terms<lb/> of progressions from simple to complex<lb/> tasks, with knowing facts/reproduction<lb/> at<lb/> the<lb/> lowest<lb/> level,<lb/> and<lb/> reasoning/reflection at the highest<lb/> level.<lb/> Item format<lb/> Two-thirds of the items in TIMSS are of<lb/> multiple-choice format, while only one-third<lb/> of the items in PISA are multipl-choice<lb/> items<lb/> Amount of reading<lb/> On average, PISA items have around twice<lb/> as many words as TIMSS items.<lb/></table>

			<head>CHAPTER 4 -COMPARISON OF PISA AND TIMSS ACHIEVEMENT RESULTS<lb/></head>

			<p>152.<lb/> This chapter compares PISA and TIMSS mathematics achievement scores for countries<lb/> participating in both surveys, and identifies factors that are associated with the observed differences in<lb/> results.<lb/></p>

			<head>Comparisons of country mean scores<lb/></head>

			<p>153.<lb/> In reporting student scores, both PISA and TIMSS transformed students&apos; IRT scores into a metric<lb/> that had a mean of 500 and a standard deviation of 100 based on a specified reference population. In PISA,<lb/> for mathematics the reference population was the group of OECD countries in the 2003 survey, while in<lb/> TIMSS, the reference population was the group of participating countries in 1995 Grade 8 TIMSS<lb/> mathematics survey (this was to ensure comparability of results to the 1995 data). Consequently, PISA and<lb/> TIMSS do not have strictly aligned definitions for central location and spread for the distributions of<lb/> reported scores, given that the reference populations are different (See Box 4.1 for explanations of central<lb/> location and spread of distributions). That is, a value of 500 in PISA is not directly comparable to a value<lb/> of 500 in TIMSS, since 500 is the mean for a different set of countries in PISA than in TIMSS. Further,<lb/> one PISA score point is not the same as one TIMSS score point in representing achievement differences,<lb/> since the scores have been multiplied by a factor proportional to the standard deviation of the respective<lb/> scores distribution. Consequently, a direct comparison of the reported scores of countries in TIMSS and<lb/> PISA is only valid for comparing rankings of countries, but not how far apart the countries are from each<lb/> other.<lb/> 154.<lb/> The reported PISA and TIMSS country mean scores for the 22 countries/regions that participated<lb/> in both PISA and TIMSS are shown in <ref type="table">Table 4</ref>.1 in columns 2 and 5, with associated standard errors for<lb/> the means in columns 3 and 6. It is important to note that data for England are included in this report for<lb/> illustration, but that England did not meet the required response rates and therefore did not satisfy the<lb/> technical requirements to confidently say that results were comparable internationally in both the TIMSS<lb/> and PISA 2003 surveys. For example, the country average for England was not published in the release of<lb/> the PISA 2003 initial results (OECD, 2004). All achievement results for England should therefore be<lb/> interpreted with caution.<lb/></p>

			<head>Box 4.1 Central Location and Spread of Distributions<lb/></head>

			<p>The following are two example distributions of mathematics scores for two groups of students. It can be seen that<lb/> the performance of the first group is lower than that of the second group, in that the distribution of the first group is<lb/> further to the left of the scale (i.e., lower scores). To describe the general &quot; location &quot; of a distribution, statistics such as<lb/> mean and median are useful. These statistics are referred to as statistics for central tendency. They provide<lb/> information about where the &quot; centre &quot; of the distribution is located.<lb/> Further, it can be seen that the distribution for the first group is more spread out than that for the second group.<lb/> That is, the range of scores for the first group is wider. Statistics such as range and variance are useful to describe the<lb/> spread of a distribution.<lb/></p>

			<figure>800<lb/> 700<lb/> 600<lb/> 500<lb/> 400<lb/> 300<lb/> 200<lb/> 100<lb/> 400<lb/> 300<lb/> 200<lb/> 100<lb/> 0<lb/> Frequency<lb/> Mean =413.349494<lb/> Std. Dev. =102.8566933<lb/> N =5,835<lb/> 800<lb/> 700<lb/> 600<lb/> 500<lb/> 400<lb/> 300<lb/> 200<lb/> 100<lb/> 400<lb/> 300<lb/> 200<lb/> 100<lb/> 0<lb/> Frequency<lb/> Mean =542.508653<lb/> Std. Dev. =83.2732641<lb/> N =5,796<lb/></figure>

			<p>155.<lb/> The countries in <ref type="table">Table 4</ref>.1 are arranged in decreasing order of PISA country means (column 2). A<lb/> glance down the column of TIMSS country means (column 5) shows that these are also in approximately<lb/> decreasing order, with some disordering. For example, while the Flemish Community of Belgium has a<lb/> similar mean score to Hong Kong-China in PISA, there is a large difference in TIMSS scores between the<lb/> two regions. The correlation between PISA and TIMSS country mean scores is 0.84, showing that, in<lb/> general, there is a reasonable agreement between the results of the two surveys. Note that Indonesia and<lb/> Tunisia have mean scores much lower than those of the other countries. If these two countries are omitted<lb/> in the table, the correlation between the remaining 20 countries is 0.66, which still indicates an association<lb/> between the PISA and TIMSS scores, although the relationship is not extremely strong.<lb/> 156.<lb/> To facilitate comparisons between PISA and TIMSS scores, two sets of standardised scores were<lb/> computed. The PISA country mean scores were standardised to have a mean of zero and a standard<lb/> deviation of one (column 4) for the 22 countries. Similarly, TIMSS country mean scores were standardised<lb/> to have a mean of zero and a standard deviation of one (column 7) for the same set of countries. That is, a<lb/> value of 1.01 for Hong Kong-China in PISA indicates that the mean PISA score for Hong Kong-China is<lb/> 1.01 PISA standard deviations away from the average of the 22 country means. Similarly, a value of 1.66<lb/> for Hong Kong-China in TIMSS indicates that the mean TIMSS score for Hong Kong-China is 1.66<lb/> TIMSS standard deviations away from the mean of the 22 countries. That is, the TIMSS score for Hong<lb/> Kong-China is actually relatively higher than the PISA score when measured in terms of the &quot; distances &quot;<lb/> from the other countries under comparison.<lb/> 157.<lb/> These standardised scores are now comparable between PISA and TIMSS. Had countries<lb/> performed in the same way in PISA and TIMSS, one would expect the standardised PISA and TIMSS<lb/> scores to be very similar for each country. If a country has very different standardised scores in PISA and<lb/> in TIMSS, then one might conclude that the country performed differently in PISA and in TIMSS.<lb/></p>

			<table>Table 4.1 PISA and TIMSS country mean scores for countries participating in both Surveys in 2003<lb/> PISA<lb/> TIMSS<lb/> Country<lb/> mean<lb/> Standard<lb/> error<lb/> Standardised<lb/> score<lb/> Country<lb/> mean<lb/> Standard<lb/> error<lb/> Standardised<lb/> score<lb/> Belgium (Fl.)<lb/> 553<lb/> (2.1)<lb/> 1.04<lb/> 537<lb/> (2.8)<lb/> 0.63<lb/> Hong Kong-China<lb/> 550<lb/> (4.5)<lb/> 0.99<lb/> 586<lb/> (3.3)<lb/> 1.72<lb/> Korea<lb/> 542<lb/> (3.2)<lb/> 0.83<lb/> 589<lb/> (2.2)<lb/> 1.79<lb/> Quebec, Canada<lb/> 541<lb/> (5.0)<lb/> 0.81<lb/> 543<lb/> (3.0)<lb/> 0.77<lb/> Netherlands<lb/> 538<lb/> (3.1)<lb/> 0.74<lb/> 536<lb/> (3.8)<lb/> 0.61<lb/> Japan<lb/> 534<lb/> (4.0)<lb/> 0.67<lb/> 570<lb/> (2.1)<lb/> 1.37<lb/> Ontario, Canada<lb/> 531<lb/> (3.5)<lb/> 0.61<lb/> 521<lb/> (3.1)<lb/> 0.28<lb/> Australia<lb/> 524<lb/> (2.1)<lb/> 0.48<lb/> 505<lb/> (4.6)<lb/> -0.08<lb/> Scotland<lb/> 524<lb/> (2.3)<lb/> 0.47<lb/> 498<lb/> (3.7)<lb/> -0.23<lb/> New Zealand<lb/> 523<lb/> (2.3)<lb/> 0.47<lb/> 494<lb/> (5.3)<lb/> -0.32<lb/> Sweden<lb/> 509<lb/> (2.6)<lb/> 0.19<lb/> 499<lb/> (2.6)<lb/> -0.21<lb/> England 1<lb/> 507<lb/> (2.9)<lb/> 0.15<lb/> 498<lb/> (4.7)<lb/> -0.23<lb/> Basque country, Spain<lb/> 502<lb/> (2.8)<lb/> 0.05<lb/> 487<lb/> (2.7)<lb/> -0.48<lb/> Slovak Republic<lb/> 498<lb/> (3.3)<lb/> -0.02<lb/> 508<lb/> (3.3)<lb/> -0.01<lb/> Norway<lb/> 495<lb/> (2.4)<lb/> -0.08<lb/> 461<lb/> (2.5)<lb/> -1.05<lb/> Hungary<lb/> 490<lb/> (2.8)<lb/> -0.18<lb/> 529<lb/> (3.2)<lb/> 0.46<lb/> Latvia<lb/> 483<lb/> (3.7)<lb/> -0.30<lb/> 508<lb/> (3.2)<lb/> -0.01<lb/> United States<lb/> 483<lb/> (2.9)<lb/> -0.31<lb/> 504<lb/> (3.3)<lb/> -0.10<lb/> Russian Federation<lb/> 468<lb/> (4.2)<lb/> -0.59<lb/> 508<lb/> (3.7)<lb/> -0.01<lb/> Italy<lb/> 466<lb/> (3.1)<lb/> -0.65<lb/> 484<lb/> (3.2)<lb/> -0.54<lb/> Indonesia<lb/> 360<lb/> (3.9)<lb/> -2.68<lb/> 411<lb/> (4.8)<lb/> -2.16<lb/> Tunisia<lb/> 359<lb/> (2.5)<lb/> -2.70<lb/> 410<lb/> (2.2)<lb/> -2.18<lb/> Average<lb/> 499<lb/> 508<lb/> 0.00<lb/> Standard deviation<lb/> 51.9<lb/> 45.1<lb/> 1.00<lb/> 1. England did not achieve the required response rate set by PISA and set by TIMSS for ensuring that a representative sample is<lb/> drawn for the country. The reliability of the mean scores therefore should be treated with some reservation.<lb/> 158.<lb/> The distances between countries in standardised PISA and TIMSS scores for the 22 countries<lb/> are shown graphically in Figure 4.1<lb/> EDU/WKP(2010)5<lb/> 54<lb/></table>

			<figure>Figure 4.1 Plot of PISA and TIMSS standardised country mean scores<lb/> BFL<lb/> BFL<lb/> HKG<lb/> HKG<lb/> KOR<lb/> KOR<lb/> NLD<lb/> NLD<lb/> JPN<lb/> JPN<lb/> AUS<lb/> AUS<lb/> SCO<lb/> SWE<lb/> ENG<lb/> ENG<lb/> SVK<lb/> NOR<lb/> NOR<lb/> USA<lb/> USA<lb/> RUS<lb/> RUS<lb/> ITA<lb/> ITA<lb/> IDN<lb/> IDN<lb/> TUN<lb/> TUN<lb/> BSQ<lb/> BSQ<lb/> ONT<lb/> ONT<lb/> QUE<lb/> QUE<lb/> SCO<lb/> NZL<lb/> NZL<lb/> SWE<lb/> SVK<lb/> LVA<lb/> LVA<lb/> -3<lb/> -2.5<lb/> -2<lb/> -1.5<lb/> -1<lb/> -0.5<lb/> 0<lb/> 0.5<lb/> 1<lb/> 1.5<lb/> 2<lb/> 2.5<lb/> BFL<lb/> HKG<lb/> KOR<lb/> NLD<lb/> JPN<lb/> AUS<lb/> SCO<lb/> NZL<lb/> SWE<lb/> ENG<lb/> SVK<lb/> NOR<lb/> HUN<lb/> LVA<lb/> USA<lb/> RUS<lb/> ITA<lb/> IDN<lb/> TUN<lb/> BSQ<lb/> ONT<lb/> QUE<lb/> PISA<lb/> TIMSS<lb/> EDU/WKP(2010)5<lb/> 55<lb/> 159.<lb/> A number of observations can be made about Figure 4.1. First</figure>

			<p>, for TIMSS scores, there is a large<lb/> &quot; gap &quot; between Asian countries and other countries, while, for PISA scores, the &quot; distance &quot; between Asian<lb/> countries and other countries narrows. This means that there is a greater difference between the group of<lb/> Asian countries and other countries in performance on the TIMSS test. This difference is not so marked<lb/> in PISA. Second, there appears to be more separation between countries in achievement levels in PISA,<lb/> while, in TIMSS, there is a clustering of countries around the overall mean score. Third, the countries that<lb/> &quot; moved up &quot; from PISA to TIMSS are mostly Asian countries and Eastern European countries, and the<lb/> countries that moved &quot; down &quot; from PISA to TIMSS tend to be Western countries. As a consequence, most<lb/> English-speaking countries are ahead of Eastern European countries in PISA, but many of them are behind<lb/> Eastern Europeans countries in TIMSS. This difference is more clearly shown in <ref type="figure">Figure 4</ref>.2. The countries<lb/> above the line are those that performed relatively better in TIMSS while those below the line performed<lb/> relatively better in PISA within the comparison of the 22 countries.<lb/></p>

			<figure>Figure 4.2 Standardised country mean scores: PISA 2003 versus TIMSS 2003<lb/> 1.00<lb/> 0.00<lb/> -1.00<lb/> -2.00<lb/> -3.00<lb/> 2.00<lb/> 1.00<lb/> 0.00<lb/> -1.00<lb/> -2.00<lb/> TIMSSStdScr<lb/> QUE<lb/> ONT<lb/> BSQ<lb/> USA<lb/> TUN<lb/> SWE<lb/> SVK<lb/> SCO<lb/> RUS<lb/> NZL<lb/> NOR<lb/> NLD<lb/> LVA<lb/> KOR<lb/> JPN<lb/> ITA<lb/> IDN<lb/> HUN<lb/> HKG<lb/> BFL<lb/> AUS<lb/> 1. England did not meet participation requirements either in TIMSS 2003 or PISA 2003. Mean scores therefore are not comparable to<lb/> those of other participating countries.<lb/></figure>

			<p>160.<lb/> Interestingly, the same pattern of differences was observed between PISA 2000 and TIMSS 1999<lb/> results (<ref type="biblio">Wu, 2005</ref>), where countries with higher standardised PISA mean scores were Australia, Canada,<lb/> Finland, New Zealand, the United Kingdom and the United States, and countries with higher standardised<lb/> TIMSS scores were the Czech Republic, Hungary, Italy, Japan, Korea, Hong Kong-China, Latvia and the<lb/> Russian Federation. The consistent findings for two PISA and TIMSS cycles indicate that the observed<lb/> pattern is unlikely to be due to chance. There is likely to be systematic differences between the PISA and<lb/> TIMSS tests in relation to systematic differences in education systems in countries. The following sections<lb/> present an examination of possible factors to explain the observed performance differences.<lb/></p>

			<head>Explaining the Differences between PISA and TIMSS Results<lb/></head>

			<p>161.<lb/> To identify factors that may explain the observed differences between PISA and TIMSS country<lb/> rankings as shown in <ref type="figure">Figure 4</ref>.2, test characteristics of PISA and TIMSS are examined, with a focus on<lb/> those characteristics where PISA and TIMSS differ. These include age/grade sampling method,<lb/> mathematics content differences and reading demand of test items (see Chapters 2 and 3).<lb/></p>

			<head>Years of Schooling and Age at time of testing<lb/></head>

			<p>162.<lb/> In Chapter 2, age and grade differences between PISA and TIMSS were discussed. TIMSS<lb/> population definition controls for the number of years of schooling, but the age of students varies more<lb/> across countries than in PISA. In PISA, the age of sampled students is controlled, but the number of years<lb/> of schooling varies across countries. Could these explain, at least in part, the observed differences in results<lb/> between the two surveys? To answer this question, we first examine the inter-relationship between the two<lb/> variables: age at time of TIMSS testing, and years of schooling at time of PISA testing. We then relate these<lb/> variables to achievement results.<lb/> 163.<lb/> For each country, the average age at time of TIMSS 2003 testing is given in the TIMSS<lb/> mathematics report (Exhibit 2, IEA, 2003). <ref type="table">Table 4</ref>.2 (column 2) shows the average age for Korea and<lb/> Norway, as an example.<lb/></p>

			<table>Table</table>

			<figure>4.2 Relationship between age at time of testing in TIMSS and years of schooling at time of testing in<lb/> PISA<lb/> Average age at time of<lb/> TIMSS test<lb/> (at 8 years of schooling)<lb/> Number of years of schooling<lb/> at time of PISA test<lb/> (at 15.7 years old)<lb/> Korea<lb/> 14.6<lb/> around 9<lb/> Norway<lb/> 13.8<lb/> around 10<lb/> 164.<lb/> Korea&apos;s sample in TIMSS has an average age of 14.6. That is, students with 8 years of schooling<lb/> in Korea are around 14.6 years of age. Therefore, when students in Korea reach 15.7 years old (the average<lb/> of the PISA sample), one would expect the students to have 9 years of schooling. Similarly, for Norway,<lb/> students are 13.8 years of age when they are in Grade 8. One would then expect 15.7 year-olds (the PISA<lb/> sample) to be in Grade 10. More generally, the number of years of schooling at time of PISA testing can be<lb/> approximately given by<lb/></figure>

			<table>number of years of schooling at time of PISA testing = (15.7 – age at time of TIMSS testing) + 8,<lb/> as (15.7 – age at time of TIMSS testing) gives the additional number of years of schooling between TIMSS<lb/> and PISA population definitions, to the 8 years of schooling controlled for in the TIMSS samples. So, in<lb/> fact, the two variables, age at time of TIMSS testing, and years of schooling at time of PISA testing are<lb/> essentially the same variable, as one is a linear transformation of the other:<lb/> number of years of schooling at time of PISA testing = 23.7 – age at time of TIMSS testing<lb/> (1)<lb/> 165.<lb/> To verify this relationship between age at time of TIMSS testing, and years of schooling at time of<lb/> PISA testing, an attempt was made to compute the average years of schooling at the time of testing in PISA<lb/> EDU/WKP(2010)5<lb/> using information from the PISA survey. An approximate estimate was made based on the following three<lb/> pieces of information:<lb/> 6. The grade variable from the PISA student questionnaire. This variable is meant to provide the<lb/> number of years of schooling. However, it turned out that this variable alone was too &quot; coarse &quot; for<lb/> the purpose of estimating years of schooling to the accuracy of fractions of a year.<lb/> 7. The start of the academic year in each country.<lb/> 8. The actual testing date of PISA in each country.<lb/> 166.<lb/> Combining (2) and (3)</table>

			<p>, one is able to estimate fractions of a year of schooling of the Grade the<lb/> students were placed at the time of PISA testing. It should be noted that there is not a great deal of<lb/> confidence that the grade variable (point 1, above) actually captures the number of years of schooling that<lb/> is comparable across countries. The estimated number of years of schooling for each country, derived as<lb/> described, is shown in <ref type="table">Table 4</ref>.3, as well as the age at the time of testing for TIMSS, for comparison. The<lb/> entries are arranged in increasing order of the number of years of schooling at the time of testing of PISA,<lb/> as estimated from PISA data.<lb/></p>

			<table>Table 4.3 Number of years of schooling at the time of PISA testing (estimated from PISA data) versus Average<lb/> age at time of TIMSS testing<lb/> Number of years<lb/> of schooling at the<lb/> time<lb/> of<lb/> PISA<lb/> testing (estimated<lb/> from PISA data)<lb/> Average age at time<lb/> of TIMSS testing<lb/> Tunisia<lb/> 8.52<lb/> 14.8<lb/> Latvia<lb/> 8.52<lb/> 15<lb/> Sweden<lb/> 8.67<lb/> 14.9<lb/> Hungary<lb/> 8.79<lb/> 14.5<lb/> Indonesia<lb/> 8.96<lb/> 14.5<lb/> Netherlands<lb/> 9.18<lb/> 14.3<lb/> Hong Kong-China<lb/> 9.20<lb/> 14.4<lb/> Slovak Republic<lb/> 9.21<lb/> 14.3<lb/> Korea<lb/> 9.25<lb/> 14.6<lb/> Japan<lb/> 9.29<lb/> 14.4<lb/> Belgium (Fl.)<lb/> 9.29<lb/> 14.1<lb/> Russian<lb/> F d ti<lb/> 9.32<lb/> 14.2<lb/> Italy<lb/> 9.40<lb/> 13.9<lb/> Spain-Basque<lb/> 9.40<lb/> 14.1<lb/> Canada-Ontario<lb/> 9.48<lb/> 13.8<lb/> Canada-Quebec<lb/> 9.48<lb/> 14.2<lb/> New Zealand<lb/> 9.51<lb/> 14.1<lb/> United States<lb/> 9.55<lb/> 14.2<lb/> Australia<lb/> 9.63<lb/> 13.9<lb/> Norway<lb/> 9.66<lb/> 13.8<lb/> England<lb/> 10.31<lb/> 14.3<lb/> Scotland<lb/> 10.69<lb/> 13.7<lb/> 167.<lb/> Table 4.3 shows that, by and large, PISA students in the Western countries tend to have had a<lb/> higher number of years of schooling than the PISA students in Asian and Eastern European countries. At<lb/></table>

			<p>the same time, there appears to be a negative relationship between the number of years of schooling at the<lb/> time of testing of PISA and the age at the time of testing of TIMSS, as one would expect from equation (1).<lb/> This relationship can be seen more readily in a scatter plot of the two variables, as shown in <ref type="figure">Figure</ref> 4.3.<lb/> EDU/WKP(2010)5<lb/> Figure 4.3 PISA years of schooling versus TIMSS age of testing<lb/> 15.00<lb/> 14.75<lb/> 14.50<lb/> 14.25<lb/> 14.00<lb/> 13.75<lb/> Age<lb/> 11.00<lb/> 10.50<lb/> 10.00<lb/> 9.50<lb/> 9.00<lb/> 8.50<lb/> YrsSchooling<lb/> QUE<lb/> ONT<lb/> BSQ<lb/> USA<lb/> TUN<lb/> SWE<lb/> SVK<lb/> SCO<lb/> RUS<lb/> NZL<lb/> NOR<lb/> NLD<lb/> LVA<lb/> KOR<lb/> JPN<lb/> ITA<lb/> IDN<lb/> HUN<lb/> HKG<lb/> ENG<lb/> BFL<lb/> AUS<lb/> 168.<lb/></figure>

			<p>The correlation between PISA years of schooling (estimated from PISA data) and TIMSS age of<lb/> testing is -0.77 (R 2 = 0.59), showing a strong relationship between these two variables. This is particularly<lb/> striking given that the variable, number of years of schooling in PISA, was an approximation constructed<lb/> for this report. Consequently, the age at time of TIMSS testing could be regarded as a proxy variable for<lb/> years of schooling in PISA, given the theoretical relationship between these two variables as shown in<lb/> Equation (1), as well as the empirical validation of the relationship as shown in <ref type="figure">Figure 4</ref>.3. England and<lb/> Scotland appear to be outliers in <ref type="figure">Figure 4</ref>.3. If England and Scotland are removed from <ref type="figure">Figure 4</ref>.3, the<lb/> correlation between the two variables is -0.9. This strong relationship between the two variables in <ref type="figure">Figure<lb/> 4</ref>.3 suggests that Grade-based samples in TIMSS have been well controlled for the number of years of<lb/> schooling.<lb/></p>

			<head>Box 4.2 The interpretations of R and R<lb/> 2 in regression models<lb/></head>

			<p>Consider a regression analysis where a dependent variable Y is to be explained, or predicted, by an independent<lb/> variable, X. The regression equation can be written as<lb/></p>

			<head>Y a bX<lb/> = +<lb/></head>

			<p>where a is called a regression constant and b is a regression coefficient. a and b are to be estimated in the<lb/> regression analysis from the ( X and Y ) data pairs. Typically, regression analysis also reports R and R 2 . In the case<lb/> where there is only one explanatory variable ( X in the above example), R is the correlation coefficient between X and<lb/> Y . The correlation coefficient, R, is a measure of association between two variables, ranging between -1 and 1. If a<lb/> plot of ( X and Y ) pairs fall exactly on a straight line with a positive gradient, then the correlation coefficient will be 1.<lb/> If the line has a negative gradient, then the correlation coefficient will be -1. If there is no association between X and<lb/> Y , then the correlation coefficient will be zero. The square of the correlation coefficient (R 2 ) is called the coefficient of<lb/> determination. R<lb/> 2 can be shown to be a measure of the proportion of sample variation in the dependent variable Y<lb/> which is explained by the values of the independent variable X :<lb/></p>

				<formula>2<lb/> explained variation in<lb/> total variation in<lb/> Y<lb/> R<lb/> Y<lb/> =<lb/></formula>

			<p>When there are more than one explanatory (or independent) variable, as shown below:<lb/></p>

				<formula>0<lb/> 1 1<lb/> 2 2<lb/> Y b b X b X<lb/> = +<lb/> +<lb/> +L<lb/> R<lb/> 2</formula>

			<p>shows the proportion of variation in Y explained by the combined set of the independent variables. In this case, R<lb/> is called the coefficient of multiple correlation, and R<lb/> 2 is called the coefficient of multiple determination.<lb/></p>

			<head>Impact of Years of Schooling on Student Performance in Mathematics<lb/></head>

			<p>169.<lb/> <ref type="table">Table 4</ref>.4 shows the list of countries arranged in order of how much better the countries<lb/> performed in TIMSS than in PISA. In this table, the metric for expressing score differences is in &quot; TIMSS<lb/> score &quot; unit, and not in standardised mean scores as computed in <ref type="table">Table 4</ref>.1. That is, PISA scores have been<lb/> converted to have the same mean and standard deviation of the TIMSS scores for the 22 countries, and<lb/> then the difference between TIMSS and PISA scores is computed. In this way, the magnitude of the<lb/> differences can be more easily interpreted than standardised scores, since one can discuss the average gain<lb/> in TIMSS score unit, with one additional year of schooling, for example. The countries at the top of <ref type="table">Table<lb/> 4</ref>			<figure>.4 performed better in TIMSS than in PISA; the countries at the bottom of the table performed better in<lb/> PISA. In addition, for each country, the average age of students in the TIMSS 12 assessment is also shown.<lb/> 12.<lb/> The average age at time of testing in TIMSS is taken from the TIMSS 2003 International Report (IEA,<lb/> 2003).<lb/> Table 4.4 Comparative performance in PISA and TIMSS, versus age of testing in TIMSS<lb/> Difference in<lb/> country mean<lb/> scores (TIMSS -<lb/>PISA), in &quot;TIMSS<lb/> score&quot; unit<lb/> Average age at time of<lb/> testing in TIMSS<lb/> Number of years of<lb/> schooling at time of PISA<lb/> testing (estimated from<lb/> PISA data)<lb/> Better in TIMSS<lb/> Korea<lb/> 43.16<lb/> 14.6<lb/> 9.25<lb/> Hong Kong-China<lb/> 33.08<lb/> 14.4<lb/> 9.20<lb/> Japan<lb/> 31.19<lb/> 14.4<lb/> 9.29<lb/> Hungary<lb/> 28.50<lb/> 14.5<lb/> 8.79<lb/> Russian Federation<lb/> 26.26<lb/> 14.2<lb/> 9.32<lb/> Tunisia<lb/> 23.48<lb/> 14.8<lb/> 8.52<lb/> Indonesia<lb/> 23.25<lb/> 14.5<lb/> 8.96<lb/> Latvia<lb/> 13.26<lb/> 15.0<lb/> 8.52<lb/> United States<lb/> 9.69<lb/> 14.2<lb/> 9.55<lb/> Italy<lb/> 4.64<lb/> 13.9<lb/> 9.40<lb/> Slovak Republic<lb/> 0.40<lb/> 14.3<lb/> 9.21<lb/> Better in PISA<lb/> Quebec, Canada<lb/> -1.77<lb/> 14.2<lb/> 9.48<lb/> Netherlands<lb/> -6.01<lb/> 14.3<lb/> 9.18<lb/> Ontario, Canada<lb/> -15.09<lb/> 13.8<lb/> 9.48<lb/> England<lb/> 1<lb/> -17.21<lb/> 14.3<lb/> 10.31<lb/> Sweden<lb/> -18.03<lb/> 14.9<lb/> 8.67<lb/> Belgium (Fl.)<lb/> -18.53<lb/> 14.1<lb/> 9.29<lb/> Basque country, Spain<lb/> -23.59<lb/> 14.1<lb/> 9.40<lb/> Australia<lb/> -25.24<lb/> 13.9<lb/> 9.63<lb/> Scotland<lb/> -31.86<lb/> 13.7<lb/> 10.69<lb/> New Zealand<lb/> -35.57<lb/> 14.1<lb/> 9.51<lb/> Norway<lb/> -43.99<lb/> 13.8<lb/> 9.66<lb/> 1.<lb/> PISA results for England are not comparable and therefore this score point difference should be interpreted with caution.<lb/> 170.<lb/> <ref type="table">Table 4</ref>.4 shows that the Asian and Eastern European countries tend to perform better in TIMSS<lb/> than in PISA. But it also appears that the Asian and Eastern European countries have a slightly older cohort<lb/> in the TIMSS assessment. The relationship between differential performance in TIMSS and PISA, and age<lb/> at time of testing in TIMSS can be better seen from a scatter plot of the two variables, as shown in <ref type="figure">Figure<lb/> 4</ref>.4, where the vertical scale shows the difference between TIMSS and PISA score (Column 3 of <ref type="table">Table<lb/> 4</ref>.4).<lb/></p>

			<figure>Figure 4.4 Relationship between performance in TIMSS and PISA and age at time of testing in TIMSS<lb/> 15.00<lb/> 14.75<lb/> 14.50<lb/> 14.25<lb/> 14.00<lb/> 13.75<lb/> Age at time of testing in TIMSS<lb/> 40.00<lb/> 20.00<lb/> 0.00<lb/> -20.00<lb/> -40.00<lb/> TIMSS -PISA (in TIMSS Score Unit)<lb/> QUE<lb/> ONT<lb/> BSQ<lb/> USA<lb/> TUN<lb/> SWE<lb/> SVK<lb/> SCO<lb/> RUS<lb/> NZL<lb/> NOR<lb/> NLD<lb/> LVA<lb/> KOR<lb/> JPN<lb/> ITA<lb/> IDN<lb/> HUN<lb/> HKG<lb/> ENG<lb/> BFL<lb/> AUS<lb/> 171.<lb/> Apart from Sweden, Figure 4.4 shows a positive relationship between (TIMSS – PISA) and the<lb/> age at time of testing in TIMSS. The correlation between the two variables is 0.58 (R 2 =0.34</figure>

			<p>). This is<lb/> significantly different from zero with p=0.004. If Sweden is removed from the data set, the correlation<lb/> between the two variables is 0.713 (R 2 =0.51), significantly different from zero with p=0.0003. That is,<lb/> countries with an older cohort at the time of testing in TIMSS tend to perform better in TIMSS than in<lb/> PISA. In contrast, Norway and Scotland performed a great deal better in PISA than in TIMSS (see <ref type="figure">Figure<lb/></ref> 4.1), and it appears that these two countries/regions have the youngest cohorts (13.7 and 13.8 years old<lb/> respectively) among the 22 TIMSS countries.<lb/> 172.<lb/> Of all 50 participating countries in TIMSS 2003 Grade 8 cohort 13 , Sweden is the only Western<lb/> country with an older cohort (14.9 years old), while all other Western countries have an average age less<lb/> than 14.3 years. The case for Sweden is discussed further in this chapter when both TIMSS and PISA<lb/> advantage indices are presented and an explanation is provided for why Sweden appears as an outlier in<lb/> Figure 4.4.<lb/> 173.<lb/> Overall, there is a relationship between the differential performance in TIMSS and PISA, and the<lb/> age of the cohort in the TIMSS sample. It should be remembered, however, that the age of the cohort in the<lb/> 13<lb/> There 46 countries and four benchmarking participants in the TIMSS 2003 Grade 8 cohort.<lb/></figure>

			<p>TIMSS sample is also a proxy for the number of years of schooling at time of PISA testing, as discussed in<lb/> the previous sections of this chapter.<lb/> 174.<lb/> A plot of years of schooling in PISA (as estimated from the PISA data) and differential<lb/> performance between TIMSS and PISA is shown in <ref type="figure">Figure</ref> 4.5.<lb/> Figure 4.5 Relationship between years of schooling in PISA and differential performance between TIMSS and<lb/> PISA<lb/> 11.00<lb/> 10.50<lb/> 10.00<lb/> 9.50<lb/> 9.00<lb/> 8.50<lb/> YrsSchooling<lb/> 40.00<lb/> 20.00<lb/> 0.00<lb/> -20.00<lb/> -40.00<lb/> Difference<lb/> QUE<lb/> ONT<lb/> BSQ<lb/> USA<lb/> TUN<lb/> SWE<lb/> SVK<lb/> SCO<lb/> RUS<lb/> NZL<lb/> NOR<lb/> NLD<lb/> LVA<lb/> KOR<lb/> JPN<lb/> ITA<lb/> IDN<lb/> HUN<lb/> HKG<lb/> ENG<lb/> BFL<lb/> AUS<lb/> 175.<lb/> <ref type="figure">Figure 4</ref>.5 shows a negative relationship between the number of years of schooling in PISA and<lb/> the differential performance between TIMSS and PISA, where the correlation between the two variables<lb/> is -0.52 (R 2 =0.27). This relationship is not as strong as that between the age at the time of testing in TIMSS<lb/> and the differential performance between TIMSS and PISA, possibly due to the fact that the estimated<lb/> number of years of schooling in PISA had to be constructed indirectly from a number of data sources.<lb/> Nevertheless, <ref type="figure">Figure 4</ref>.5 still shows an association between the two variables.<lb/></p>

			<head>176.<lb/> The relationships shown in Figures 4.3, 4.4 and 4.5 suggest</head>

			<p>that the difference between<lb/> performance in TIMSS and PISA could be related to the age of testing in TIMSS or the number of years of<lb/> schooling in PISA. For future cycles of testing, it will be helpful to capture more reliable information on<lb/> the number of years of schooling at the time of testing, to help to understand and interpret the achievement<lb/> results.<lb/></p>

			<head>The impact of differences in content balance between PISA and TIMSS<lb/></head>

			<p>177.<lb/> From a number of studies (e.g., <ref type="biblio">Routitsky and Zammit, 2002; Zabulionis, 2001</ref>) carried out on<lb/> the comparisons of mathematics education in the international context, it is not surprising to hypothesise<lb/> that differences in content balance in PISA and TIMSS may lead to the observed differences in country<lb/> performances as shown in Figures 4.1 and 4.2. In particular, there are different mathematics traditions<lb/> between countries. The Asian and Eastern European countries stress formal mathematics, while the<lb/> Western countries place an emphasis on problem solving and application skills (e.g., Leung, <ref type="biblio">Graf &amp;<lb/> Lopez-Real, 2006</ref>). Since the TIMSS assessment is more curricular focused and the PISA assessment is<lb/> more problem oriented, this could explain the observed differences in the results of the two assessments.<lb/> 178.<lb/> In making comparisons between the content of the PISA and TIMSS assessments, PISA items<lb/> were classified according to the TIMSS content domain classifications (see <ref type="table">Table 3</ref>.6). The decision to re-<lb/>classify PISA items according to TIMSS classifications rather than the other way round is because national<lb/> curricula are mostly structured by traditional mathematics content areas. It will be easier to compare<lb/> proportions of items in different traditional mathematics content areas in PISA and TIMSS to those in the<lb/> national curricula.<lb/> 179.<lb/> A comparison of the proportions of PISA and TIMSS items by TIMSS content domain<lb/> classifications is given in <ref type="table">Table 4</ref>.5.<lb/></p>

			<table>Table 4.5 Number and proportions of items in PISA and TIMSS by content domains<lb/> PISA<lb/> TIMSS<lb/> Differences in<lb/> percentages<lb/> between PISA<lb/> and TIMSS<lb/> Number<lb/> 32<lb/> 38%<lb/> 57<lb/> 30%<lb/> 8%<lb/> Algebra<lb/> 7<lb/> 8%<lb/> 47<lb/> 24%<lb/> -16%<lb/> Measurement<lb/> 8<lb/> 9%<lb/> 31<lb/> 16%<lb/> -7%<lb/> Geometry<lb/> 12<lb/> 14%<lb/> 31<lb/> 16%<lb/> -2%<lb/> Data<lb/> 26<lb/> 31%<lb/> 28<lb/> 14%<lb/> 17%<lb/> Total<lb/> 85<lb/> 100%<lb/> 194<lb/> 100%<lb/> 180.<lb/> It can be seen from <ref type="table">Table 4</ref>.5 that, in PISA, there are more items in the content domains of<lb/> number and data, while, in TIMSS, the proportions of items in each content domain are more evenly<lb/> spread. There are fewer algebra and measurement items in PISA when compared to item proportions in<lb/> TIMSS. Given the &quot; literacy &quot; orientation of the PISA assessment, the distribution of PISA items across<lb/> the content domains is not surprising. If one surveys the mathematics that most people have to deal with in<lb/> everyday life, one does not come across solving equations very often. Instead, in everyday life, one needs<lb/> to interpret information in tables or graphs, or calculate prices/discounts, so that there is a predominance<lb/> of number and data applications that one has to deal with. This is not to say that algebra is not an<lb/> important part of mathematics. Algebra is very important for many applications in the technological world.<lb/> But, by and large, only a small proportion of people specialise in these applications.<lb/></p>

			<figure>Achievement by content domains<lb/> 181.<lb/> It will be of interest to examine whether there are differences across countries in achievement<lb/> scores by content domains. TIMSS 2003 International Mathematics Report published averaged scaled<lb/> scores by mathematics content areas by country (see IEA, 2003, Exhibit 3.1). The following is an extract<lb/> from the TIMSS report for the 22 countries that participated in both PISA and TIMSS.<lb/></figure>

			<table>Table 4.6 TIMSS achievement scores by content areas<lb/> Number Algebra Measurement Geometry Data<lb/> Australia<lb/> 498<lb/> 499<lb/> 511<lb/> 491<lb/> 531<lb/> Belgium (Fl.)<lb/> 539<lb/> 523<lb/> 535<lb/> 527<lb/> 546<lb/> England<lb/> 1<lb/> 485<lb/> 492<lb/> 505<lb/> 492<lb/> 535<lb/> Canada-Ontario<lb/> 516<lb/> 515<lb/> 520<lb/> 513<lb/> 538<lb/> Canada-Quebec<lb/> 546<lb/> 529<lb/> 541<lb/> 542<lb/> 544<lb/> Hong Kong-<lb/>China<lb/> 586<lb/> 580<lb/> 584<lb/> 588<lb/> 566<lb/> Hungary<lb/> 529<lb/> 534<lb/> 525<lb/> 515<lb/> 526<lb/> Indonesia<lb/> 421<lb/> 418<lb/> 394<lb/> 413<lb/> 418<lb/> Italy<lb/> 480<lb/> 477<lb/> 500<lb/> 469<lb/> 490<lb/> Japan<lb/> 557<lb/> 568<lb/> 559<lb/> 587<lb/> 573<lb/> Korea<lb/> 586<lb/> 597<lb/> 577<lb/> 598<lb/> 569<lb/> Latvia<lb/> 507<lb/> 508<lb/> 500<lb/> 515<lb/> 506<lb/> Netherlands<lb/> 539<lb/> 514<lb/> 549<lb/> 513<lb/> 560<lb/> New Zealand<lb/> 481<lb/> 490<lb/> 500<lb/> 488<lb/> 526<lb/> Norway<lb/> 456<lb/> 428<lb/> 481<lb/> 461<lb/> 498<lb/> Russian<lb/> Federation<lb/> 505<lb/> 516<lb/> 507<lb/> 515<lb/> 484<lb/> Scotland<lb/> 484<lb/> 488<lb/> 508<lb/> 491<lb/> 531<lb/> Slovak Republic<lb/> 514<lb/> 505<lb/> 508<lb/> 501<lb/> 495<lb/> Spain-Basque<lb/> 490<lb/> 490<lb/> 488<lb/> 456<lb/> 499<lb/> Sweden<lb/> 496<lb/> 480<lb/> 512<lb/> 467<lb/> 539<lb/> Tunisia<lb/> 419<lb/> 405<lb/> 407<lb/> 427<lb/> 387<lb/> United States<lb/> 508<lb/> 510<lb/> 495<lb/> 472<lb/> 527<lb/> 1. England did not meet participation requirements in TIMSS 2003. Mean scores are therefore not comparable to other countries.<lb/> 182.<lb/> <ref type="table">Table 4</ref>.6 shows that the patterns of performance across mathematics content areas are quite<lb/> different between countries. For example, in Australia, the average score for data is 32 points higher than<lb/> for algebra 14 . In contrast, in the Russian Federation, it is just the opposite, where the score for algebra is<lb/> 32 points higher than the score for data. Such differential patterns of content area achievement scores<lb/> across countries will lead to different aggregate scores if proportions of items from different content areas<lb/> change. As described in the previous section, PISA and TIMSS have quite different proportions of items<lb/> from each content area, as shown in <ref type="table">Table 4</ref>.5.<lb/> 183.<lb/> To assess the impact on mathematics achievement score when proportions of items from different<lb/> content areas change, indices of &quot; PISA advantage &quot; and &quot; TIMSS advantage &quot; were constructed. The<lb/> methodology is presented below and is based only on TIMSS achievement scores and not PISA<lb/> achievement scores. Also, the mapping of PISA items to TIMSS content domains that was presented in<lb/> Chapter 3 is used and the reader is reminded that no definitive categorisation of PISA items into TIMSS<lb/> content domains is possible. With these caveats in mind the methodology, using Australia as an example, is<lb/> as follows:<lb/></p>

			<table>184.<lb/> The TIMSS average scores by mathematics content areas for Australia are given below:<lb/> 14<lb/> The standard errors of these estimates are around 2 to 5, so a mean difference of 32 is certainly significant.<lb/> TIMSS mean score by content area<lb/> Mean of the<lb/> five content<lb/> areas<lb/> 15<lb/> Number<lb/> Algebra<lb/> Measurement<lb/> Geometry<lb/> Data<lb/> Australia<lb/> 498<lb/> 499<lb/> 511<lb/> 491<lb/> 531<lb/> 506<lb/> 185.<lb/> The final column shows the mean of the five content area scores (506). The difference between<lb/> each content area score and the mean of the five content areas is then calculated, as shown below:<lb/> Deviation of content area score from the mean of the five content areas (506)<lb/> Number<lb/> Algebra<lb/> Measurement<lb/> Geometry<lb/> Data<lb/> Australia<lb/> -8<lb/> -7<lb/> 5<lb/> -15<lb/> 25<lb/> 186.<lb/> The deviation from each content area is then weighted by the proportion of items in each<lb/> assessment. For example, for PISA, the proportions of items are as follows:<lb/> PISA item distribution by content areas<lb/> Number<lb/> Algebra<lb/> Measurement<lb/> Geometry<lb/> Data<lb/> 38%<lb/> 8%<lb/> 9%<lb/> 14%<lb/> 31%<lb/></table>

			<p>187.<lb/> The PISA advantage index is computed as the weighted sum of the deviations of content area<lb/> scores from the mean of the five content areas, weighted by item proportions in PISA. For example, the<lb/> PISA advantage index for Australia is:<lb/></p>

				<formula>(</formula>

			<figure>-8) × 0.38 + (-7) × 0.08 + 5 × 0.09 + (-15) × 0.14 + 25 × 0.31 = 2.42<lb/> 188.<lb/> The same method is applied to calculate the TIMSS advantage index, using the proportion of<lb/> TIMSS items listed below:<lb/> TIMSS item distribution by content areas<lb/> Number<lb/> Algebra<lb/> Measurement<lb/> Geometry<lb/> Data<lb/> 30%<lb/> 24%<lb/> 16%<lb/> 16%<lb/> 14%<lb/> 189.<lb/> The TIMSS advantage index for Australia is:<lb/> (-8) × 0.30 + (-7) × 0.24 + 5 × 0.16 + (-15) × 0.16 + 25 × 0.14 = -2.18<lb/> 190.<lb/> An overall Content Advantage Index is computed as the difference between PISA advantage<lb/> index and TIMSS advantage index. So, for Australia, the Content Advantage Index is given by:<lb/> 2.42 – (-2.18) = 4.6<lb/> 15<lb/> Note that this is the mean of the scores for the five content areas. This mean will be close to the average<lb/> scaled score for overall mathematics achievement, but not necessarily exactly the same.<lb/></figure>

			<p>191.<lb/> The Content Advantage Index shows how much more advantage a country has in PISA as<lb/> compared to in TIMSS. Thus, from the above computations, the PISA assessment gives Australia more<lb/> &quot; advantage &quot; (4.6 units), than the TIMSS assessment in that Australia is likely to score higher in PISA than<lb/> in TIMSS, given the composition of items from different content areas in PISA and in TIMSS.<lb/> 192.<lb/> The PISA and TIMSS advantage indices, and the Content Advantage Indices were computed for<lb/> all 22 countries, and these are shown in <ref type="table">Table 4</ref>.7, together with the difference in TIMSS and PISA<lb/> achievement mean scores (in TIMSS score unit), arranged in order of the difference in performance in<lb/> TIMSS and PISA.<lb/></p>

			<table>Table 4.7 Indices of PISA and TIMSS advantage by country<lb/> Difference in<lb/> country mean<lb/> scores (TIMSS<lb/> -PISA), in<lb/> &quot;TIMSS score&quot;<lb/> unit<lb/> PISA<lb/> advantage<lb/> index<lb/> TIMSS<lb/> advantage<lb/> index<lb/> Content<lb/> advantage<lb/> index (PISA<lb/> adv – TIMSS<lb/> adv)<lb/> Better in TIMSS<lb/> Korea<lb/> 43.16<lb/> -2.85<lb/> 1.29<lb/> -4.14<lb/> Hong Kong<lb/> 33.08<lb/> -1.32<lb/> 0.86<lb/> -2.18<lb/> Japan<lb/> 31.19<lb/> -1.58<lb/> -1.71<lb/> 0.14<lb/> Hungary<lb/> 28.50<lb/> 0.34<lb/> 1.10<lb/> -0.76<lb/> Russian<lb/> F d ti<lb/> 26.26<lb/> -4.32<lb/> 1.15<lb/> -5.47<lb/> Tunisia<lb/> 23.48<lb/> -0.94<lb/> 1.35<lb/> -2.29<lb/> Indonesia<lb/> 23.25<lb/> 3.36<lb/> 1.45<lb/> 1.92<lb/> Latvia<lb/> 13.26<lb/> 0.05<lb/> 0.06<lb/> -0.01<lb/> United States<lb/> 9.69<lb/> 5.27<lb/> 1.00<lb/> 4.27<lb/> Italy<lb/> 4.64<lb/> -0.06<lb/> -1.05<lb/> 0.99<lb/> Slovak Republic<lb/> 0.40<lb/> 0.45<lb/> 1.44<lb/> -0.99<lb/> Better in PISA<lb/> Canada-Quebec<lb/> -1.77<lb/> 2.55<lb/> -0.25<lb/> 2.80<lb/> Netherlands<lb/></table>

			<figure>Figure 4.6 Relationship between difference in TIMSS and PISA scores and the Content Advantage Index<lb/> 10.00<lb/> 7.50<lb/> 5.00<lb/> 2.50<lb/> 0.00<lb/> -2.50<lb/> -5.00<lb/> Content Advantage Index<lb/> 40.00<lb/> 20.00<lb/> 0.00<lb/> -20.00<lb/> -40.00<lb/> TIMSS -PISA<lb/> QUE<lb/> ONT<lb/> BSQ<lb/> USA<lb/> TUN<lb/> SWE<lb/> SVK<lb/> SCO<lb/> RUS<lb/> NZL<lb/> NOR<lb/> NLD<lb/> LVA<lb/> KOR<lb/> JPN<lb/> ITA<lb/> IDN<lb/> HUN<lb/> HKG<lb/> ENG<lb/> BFL<lb/> AUS<lb/> 194.<lb/> <ref type="figure">Figure 4</ref>.6 shows a clear relationship between the difference in TIMSS and PISA mean scores<lb/> and the content advantage index. When the relative content advantage in PISA is lower than in TIMSS,<lb/> (TIMSS score – PISA score) tends to be positive, and when the relative content advantage is higher in<lb/> PISA than in TIMSS, (TIMSS score – PISA score) tends to be negative. The correlation between the two<lb/> variables in <ref type="figure">Figure 4</ref>.6 is -0.81 (R 2 =0.66) with p&lt;0.001. Note that, among the 22 countries, Sweden has the<lb/> second highest content advantage index in favour of PISA. This offsets the fact that students in Sweden<lb/> have fewer years of schooling at the time of PISA testing (and therefore a higher age cohort in TIMSS).<lb/></p>

			<head>Predicting PISA Mathematics Country Mean Scores<lb/></head>

			<p>195.<lb/> In the previous sections, two factors have been separately identified to have an association with<lb/> the observed differences between countries&apos; TIMSS and PISA mean scores: Content Balance and Age at<lb/> time of TIMSS testing (or Years of schooling at time of PISA testing). In this section, we explore the<lb/> combined impact of these two factors on the differences between PISA and TIMSS mean scores. To do<lb/> this, multiple regressions were carried out. To formulate the regression models, first consider a theoretical<lb/> relationship:<lb/></p>

			<head>PISA = TIMSS + (PISA – TIMSS)<lb/></head>

			<p>where PISA denotes a country&apos;s PISA mathematics mean score, and TIMSS denotes that country&apos;s TIMSS<lb/> mathematics mean score. Consequently, the regression models can be formulated with PISA mathematics<lb/> country mean score as the dependent variable to be predicted, and TIMSS mathematics country mean score<lb/> as the first predictor (independent variable), plus other predictor variables that have a relationship with<lb/> (PISA – TIMSS) country mean scores.<lb/> 196.<lb/> <ref type="table">Table 4</ref>.8 shows a summary of the multiple regression models used. The simplest model (model<lb/> 1) is to predict PISA mathematics country mean scores using only TIMSS mathematics country mean<lb/> scores as a predictor. As discussed at the beginning of this Chapter, the correlation between PISA and<lb/> TIMSS country mean scores is 0.84. The percentage of variance of PISA mathematics country mean scores<lb/> explained by the regression model is 71%. That is, around 30% of the variance of PISA country mean<lb/> scores cannot be explained by their TIMSS country mean scores. Under this regression model, we can<lb/> compute the predicted PISA mean score for each country and compare it with the observed score as<lb/> reported in the PISA international report (OECD, 2004). With TIMSS mathematics score as the only<lb/> predictor, two out of the 22 countries have a predicted PISA mathematics score within the confidence<lb/> interval of the reported PISA score.<lb/> 197.<lb/> For regression model 2, two more variables are added as predictors:</p>

			<table>TIMSS Age and Content<lb/> advantage index. Note that TIMSS Age is used as a proxy for Years of schooling in PISA. The percentage<lb/> of variance of PISA mathematics scores explained is 93%, showing a large improvement from regression<lb/> model 1. Under regression model 2, nine out of the 22 countries have a predicted PISA mathematics score<lb/> within the confidence interval of the reported PISA score.<lb/> 198.<lb/> As there is more reading demand in PISA mathematics items, it is worth exploring whether PISA<lb/> reading country mean score is a useful predictor for PISA mathematics scores. Regression model 3<lb/> explores the relationships between PISA mathematics country mean scores and PISA reading country<lb/> mean scores. Quite surprisingly, there is a very high correlation (R=0.95) between PISA mathematics<lb/> country mean scores and PISA reading country mean scores. Under this regression model, five out of 22<lb/> countries have a predicted PISA mathematics score within the confidence interval of the reported PISA<lb/> score.<lb/> 199.<lb/> Regression model 4 uses all four predictors: TIMSS Mathematics country mean score, TIMSS<lb/> Age, Content advantage index, and PISA Reading score. Under this model, 97%</table>

			<p>of the variance of PISA<lb/> mathematics scores can be explained. Further, 11 out of the 22 countries have a predicted PISA<lb/> mathematics score within the confidence interval of the reported PISA score. <ref type="table">Table 4</ref>.9 shows the predicted<lb/> PISA mathematics scores as compared to the reported scores. It can be seen that, for most countries, the<lb/> difference between the predicted score and the reported score is less than 10.<lb/> 200.<lb/> The four regression models shown in <ref type="table">Table 4</ref>.8 are not the only models that can be fitted. PISA<lb/> science country mean scores are also highly correlated with PISA mathematics country mean scores<lb/> (R=0.97). TIMSS science scores also have a moderately high correlation with PISA mathematics scores<lb/> (R=0.89). However, the purpose of this section is not so much as to predict PISA mathematics scores per<lb/> se. The purpose is to illustrate how PISA and TIMSS mathematics scores and other factors are inter-<lb/>related.<lb/> 201.<lb/> It should be noted that different regression models will show different sets of countries as having<lb/> the best predicted scores. Consequently, <ref type="table">Table 4</ref>.9 should not be used to make judgements about specific<lb/> &quot; outlier &quot; countries. If different predictors are used, different countries will be outliers.<lb/></p>

			<table>Table 4.8 Regression models for predicting PISA mathematics country mean scores<lb/> Table 4.9 Comparisons between Reported and Predicted Country Mean Scores<lb/> Country<lb/> Reported<lb/> mean score<lb/> Predicted mean<lb/> score from<lb/> regression<lb/> model 4<lb/> Difference<lb/> between<lb/> reported and<lb/> predicted<lb/> scores<lb/> Predicted score<lb/> is within the<lb/> confidence<lb/> interval of the<lb/> reported score<lb/> Australia<lb/> 524<lb/> 526<lb/> -2<lb/> 񮽙<lb/> Belgium Flemish<lb/> 553<lb/> 544<lb/> 9<lb/> Canada-Ontario,<lb/> 531<lb/> 539<lb/> -8<lb/> Canada-Quebec<lb/> 541<lb/> 541<lb/> 0<lb/> 񮽙<lb/> England<lb/> 507<lb/> 506<lb/> 1<lb/> 񮽙<lb/> Hong Kong -China<lb/> 550<lb/> 546<lb/> 4<lb/> 񮽙<lb/> Hungary<lb/> 490<lb/> 497<lb/> -7<lb/> Indonesia<lb/> 360<lb/> 371<lb/> -11<lb/> Italy<lb/> 466<lb/> 479<lb/> -13<lb/> Japan<lb/> 534<lb/> 533<lb/> 1<lb/> 񮽙<lb/> Korea<lb/> 542<lb/> 557<lb/> -15<lb/> Latvia<lb/> 483<lb/> 483<lb/> 0<lb/> 񮽙<lb/> New Zealand<lb/> 523<lb/> 515<lb/> 8<lb/> Norway<lb/> 495<lb/> 494<lb/> 1<lb/> 񮽙<lb/> Russian Federation<lb/> 468<lb/> 458<lb/> 10<lb/> Scotland<lb/> 524<lb/> 521<lb/> 3<lb/> 񮽙<lb/> Slovak Republic<lb/> 498<lb/> 479<lb/> 19<lb/> Spain-Basque country<lb/> 502<lb/> 492<lb/> 10<lb/> Sweden<lb/> 509<lb/> 506<lb/> 3<lb/> 񮽙<lb/> The Netherlands<lb/> 538<lb/> 534<lb/> 4<lb/> 񮽙<lb/> Tunisia<lb/> 359<lb/> 355<lb/> 4<lb/> 񮽙<lb/> United States<lb/> 483<lb/> 502<lb/> -19<lb/> 16<lb/> The variable Age at time of TIMSS test is used as a proxy for Years of schooling at time of PISA test.<lb/> Regression<lb/> model<lb/> To Predict<lb/> (Dependent variable)<lb/> Predictor(s)<lb/> (Independent variables)<lb/> Percentage of<lb/> variance<lb/> explained (R 2 )<lb/> Correlation (R)<lb/> 1<lb/> PISA mathematics<lb/> TIMSS Mathematics<lb/> 71%<lb/> 0.84<lb/> 2<lb/> PISA mathematics<lb/> TIMSS Mathematics<lb/> TIMSS Age 16<lb/> Content advantage index<lb/> 93%<lb/> 0.97<lb/> 3<lb/> PISA mathematics<lb/> PISA Reading<lb/> 91%<lb/> 0.95<lb/> 4<lb/> PISA mathematics<lb/> TIMSS Mathematics<lb/> TIMSS Age<lb/> Content advantage index<lb/> PISA Reading<lb/> 97%<lb/> 0.99<lb/> EDU/WKP(2010)5<lb/> Implications of differential performance of countries in content domains<lb/> 202.<lb/> The findings from previous sections show</table>

			<p>that differential performance of countries in content<lb/> domains can explain, to a large extent, different country rankings in PISA and TIMSS. An implication of<lb/> this finding is that the reported combined mathematics score must be interpreted in relation to the<lb/> composition of the test in terms of content balance. For example, if a test consists only of TIMSS Data<lb/> items, then Sweden ranks 7th out of the 22 countries considered above. If a test consists only of TIMSS<lb/> Algebra items, then Sweden ranks 18 th out of the 22 countries. In contrast, out of 22 countries, Hungary<lb/> ranks 13th in TIMSS Data content domain, but 4 th in TIMSS Algebra content domain. Consequently, tests<lb/> consisting of different balance of content domains will likely to produce different rankings of countries.<lb/> Any statement about how a country performed in &quot; mathematics &quot; must be carefully interpreted.<lb/> 203.<lb/> One may argue that the performance of countries at the level of content domains may be more<lb/> informative. As PISA and TIMSS have different content classifications, it will be difficult to cross-check<lb/> results at the content domain level. Of the five TIMSS content domains and four PISA overarching ideas,<lb/> the best match is perhaps between TIMSS Data domain and PISA Uncertainty overarching idea (see<lb/> Chapter 3 for comparisons of the PISA and TIMSS frameworks). The correlation between country mean<lb/> scores in TIMSS Data content domain and PISA Uncertainty overarching idea is 0.93 for the 22 countries.<lb/> This is much higher than the correlation between the combined mathematics scores (0.84). This suggests<lb/> that, if the contents of the tests are aligned, the results will be more similar. For other TIMSS content<lb/> domains and PISA overarching ideas, it is difficult to form a one-to-one match between TIMSS and PISA.<lb/></p>

			<head>TIMSS Data domain and PISA Uncertainty overarching idea<lb/></head>

			<p>204.<lb/> It is worthwhile taking a closer look at the Data/Uncertainty content domain, as PISA has<lb/> considerably more coverage of this content domain (31%) than TIMSS has (14%), and it appears that<lb/> Western countries have relatively more strengths in this content domain (as compared to the other content<lb/> domains) than Eastern European and Asian countries have. One may suggest that the Data/Uncertainly<lb/> domain has a prominence in the PISA test because the need to represent and interpret data is becoming<lb/> more and more important in everyday lives of the citizens, whether it is reading the newspaper,<lb/> advertisements, or other forms of communication. Reading graphs and interpreting charts is part of our<lb/> lives, and not just a school subject. It is therefore hypothesised that skills and knowledge in the<lb/> Data/Uncertainty domain may be closely related to those in the reading domain, as one area of the reading<lb/> domain is about document reading. <ref type="table">Table 4</ref>.10 shows the correlations between country mean scores in<lb/> TIMSS content domains and PISA reading.<lb/></p>

			<table>Table 4.10 Correlation between country mean scores in PISA Reading and TIMSS content domains<lb/> Correlation with PISA<lb/> Reading<lb/> TIMSS Number<lb/> 0.65<lb/> TIMSS Algebra<lb/> 0.62<lb/> TIMSS Measurement<lb/> 0.79<lb/> TIMSS Geometry<lb/> 0.57<lb/> TIMSS Data<lb/> 0.91<lb/> EDU/WKP(2010)5<lb/> 205.<lb/> From <ref type="table">Table 4</ref>.10, it can be seen that the Data domain stands out as one that is highly correlated<lb/> with Reading. Since the PISA mathematics test has nearly one third of the items in the Data/Uncertainty<lb/> domain, it is not surprising that there is a high correlation between PISA mathematics and PISA reading<lb/> scores. It is also not surprising that country rankings are somewhat difference between TIMSS and PISA,<lb/> as PISA mathematics, on balance, is testing something a little different from what TIMSS tests.<lb/></p>

			<head>Examining the spread of PISA and TIMSS achievement distributions<lb/></head>

			<p>206.<lb/> The comparisons in the previous sections focus on the differences between country mean scores.<lb/> While the mean score of a country provides one measure of overall performance of a country, it does not<lb/> provide a complete picture of country performance. For example, it is often of interest to know how low<lb/> and high achievers differ within a country and across countries. In this chapter, we examine characteristics<lb/> of the achievement distributions other than mean scores, such as the spread of the distributions and<lb/> percentile points.<lb/></p>

			<head>Standard Deviations<lb/></head>

			<p>207.<lb/> Standard deviation is a measure of dispersion (or spread) of a set of data values. The larger the<lb/> standard deviation, the more spread out the data values are (See Box 4.1 for an explanation). In PISA and<lb/> TIMSS, the mean and standard deviation of mathematics achievement scores are reported by country.<lb/> <ref type="table">Table 4</ref>.11 shows the standard deviations for the 22 countries that participated in both surveys. The last<lb/> column of <ref type="table">Table 4</ref>.11 shows PISA standard deviation in TIMSS unit, using the transformation 17 as<lb/> described earlier in this chapter. The countries are arranged in order of their TIMSS standard deviation,<lb/> from smallest to largest. If two surveys have the same population definition and similar composition of<lb/> items, one would expect the standard deviations in the two surveys to have a high correlation. That is, if<lb/> the mathematics abilities of students in country X are more spread out than those in other countries, then<lb/> this should be reflected in the standard deviation values in both surveys. A scan down the column of<lb/> standard deviations in PISA shows that there is not a very strong relationship between TIMSS standard<lb/> deviation and PISA standard deviation.<lb/> 208.<lb/> <ref type="figure">Figure 4</ref>.7 shows a plot of TIMSS standard deviation and PISA standard deviation in TIMSS<lb/> unit. The correlation between these two variables across the countries is 0.22, a somewhat weak<lb/> correlation. There are two notable outliers in this graph: Indonesia 18 and Canada–Quebec. The other<lb/> countries seem to lie somewhere in between these two outliers, with a stronger positive correlation (0.55).<lb/> The dotted line in <ref type="figure">Figure 4</ref>.7 shows the equality line where TIMSS standard deviation equals PISA<lb/> standard deviation. It can be seen that 18 out of 22 countries have larger standard deviations in PISA than<lb/> in TIMSS. A possible explanation for this is that the PISA samples contain students from multiple grades,<lb/> so there may be a wider spread of mathematics abilities. Of course there are also many other possible<lb/> reasons for differences in the magnitude of standard deviations between the two</p>

			<table>surveys. Since the content<lb/> 17<lb/> More specifically, the transformation used was:<lb/> PISA standard deviation in TIMSS unit = ( PISA standard deviation / 51.92) * 45.08,<lb/> where 51.92 was the standard deviation of the PISA country mean scores of the 22 countries participated in<lb/> both PISA and TIMSS, and 45.08 was the standard deviation of the TIMSS country mean scores.<lb/> 18<lb/> In fact, Indonesia has the highest standard deviation in TIMSS and lowest standard deviation in PISA<lb/> among the 22 countries. We have sought explanations for this, and it appears that the sampling base (types<lb/> of schools) may not be the same in the two surveys in Indonesia. A thorough investigation of this is outside<lb/> the scope of this report. But there are some indications that the PISA and TIMSS samples do not reflect the<lb/> same population groups in Indonesia, over and above the difference in grade-based and age-based sampling<lb/> in TIMSS and PISA.<lb/> EDU/WKP(2010)5<lb/> balance and the items are different in the two surveys, one test may spread students out more than the<lb/> other. However, as PISA has a slightly lower reported reliability than TIMSS&apos; reported reliability, it<lb/> appears unlikely that PISA test should spread students out more than TIMSS test.<lb/> Table 4.11 PISA and TIMSS standard deviations for the 22 countries<lb/> TIMSS<lb/> standard<lb/> deviation<lb/> PISA standard deviation<lb/> as reported<lb/> PISA standard deviation<lb/> transformed to TIMSS<lb/> unit<lb/> Quebec, Canada<lb/> 58<lb/> 93<lb/> 81<lb/> Tunisia<lb/> 60<lb/> 82<lb/> 71<lb/> Basque country, Spain<lb/> 64<lb/> 82<lb/> 72<lb/> Ontario, Canada<lb/> 66<lb/> 83<lb/> 72<lb/> The Netherlands<lb/> 69<lb/> 93<lb/> 80<lb/> Norway<lb/> 71<lb/> 92<lb/> 80<lb/> Sweden<lb/> 71<lb/> 95<lb/> 82<lb/> Hong Kong -China<lb/> 72<lb/> 100<lb/> 87<lb/> Belgium Flemish<lb/> 73<lb/> 105<lb/> 91<lb/> Latvia<lb/> 73<lb/> 88<lb/> 76<lb/> Scotland<lb/> 75<lb/> 84<lb/> 73<lb/> England<lb/> 77<lb/> 93<lb/> 81<lb/> Italy<lb/> 77<lb/> 96<lb/> 83<lb/> Russian Federation<lb/> 77<lb/> 92<lb/> 80<lb/> New Zealand<lb/> 78<lb/> 98<lb/> 85<lb/> Hungary<lb/> 80<lb/> 94<lb/> 81<lb/> Japan<lb/> 80<lb/> 101<lb/> 87<lb/> United States<lb/> 80<lb/> 95<lb/> 83<lb/> Australia<lb/> 82<lb/> 95<lb/> 83<lb/> Slovak Republic<lb/> 82<lb/> 93<lb/> 81<lb/> Korea<lb/> 84<lb/> 92<lb/> 80<lb/> Indonesia<lb/> 89<lb/> 81<lb/> 70<lb/> 90.00<lb/> 85.00<lb/> 80.00<lb/> 75.00<lb/> 70.00<lb/></table>

			<figure>PISAStdevTU<lb/> 90.00<lb/> 85.00<lb/> 80.00<lb/> 75.00<lb/> 70.00<lb/> 65.00<lb/> 60.00<lb/> 55.00<lb/> TIMSSStdev<lb/> QUE<lb/> ONT<lb/> BSQ<lb/> USA<lb/> TUN<lb/> SWE<lb/> SVK<lb/> SCO<lb/> RUS<lb/> NZL<lb/> NOR<lb/> NLD<lb/> LVA<lb/> KOR<lb/> JPN<lb/> ITA<lb/> IDN<lb/> HUN<lb/> HKG<lb/> ENG<lb/> BFL<lb/> AUS<lb/> Figure 4.7 TIMSS standard deviation versus PISA standard deviation (in TIMSS unit)<lb/> Percentiles<lb/> 209.<lb/> To examine the distributions more closely, the 5 th and 95 th percentile points are shown in <ref type="table">Table<lb/> 4</ref>.12. <ref type="figure">Figure 4</ref>.8 shows a country-by-country comparison of PISA and TIMSS percentile points. For each<lb/> country, two bars are shown. The first bar shows the PISA results, and the second bar shows TIMSS<lb/> results. The top end of each bar shows the 95 th percentile point, and the bottom end of each bar shows the<lb/> 5 th percentile point. The circle in the middle shows country mean score. For example, the pair of bars for<lb/> Australia shows that the spread of the achievement distribution of Australian students in PISA is similar to<lb/> the spread of TIMSS achievement distribution, but the whole PISA distribution is shifted upwards as<lb/> compared to TIMSS distribution. In contrast, for Hong Kong, the spread of the PISA distribution is much<lb/> larger than the spread of the TIMSS distribution, and the whole PISA ability distribution is shifted<lb/> downwards as compare to the TIMSS distribution.<lb/></p>

			<figure>EDU/WKP(2010)5<lb/></figure>

			<table>Table 4.12 PISA and TIMSS 95<lb/> th and 5<lb/> th percentiles for countries participating in both Surveys in 2003<lb/> TIMSS 5 th<lb/> percentile<lb/> TIMSS<lb/> 95 th<lb/> percentile<lb/> PISA 5 th<lb/> percentile<lb/> PISA 95 th<lb/> percentile<lb/> PISA 5 th<lb/> percentile<lb/> (in TIMSS<lb/> unit)<lb/> PISA 95 th<lb/> percentile<lb/> (in TIMSS<lb/> unit)<lb/> EDU/WKP(2010)5<lb/> 76<lb/> QUE<lb/> ONT<lb/> BSQ<lb/> USA<lb/> TUN<lb/> SWE<lb/> SVK<lb/> SCO<lb/> RUS<lb/> NZL<lb/> NOR<lb/> NLD<lb/> LVA<lb/> KOR<lb/> JPN<lb/> ITA<lb/> IDN<lb/> HUN<lb/> HKG<lb/> ENG<lb/> BFL<lb/> AUS<lb/></table>

			<head>CountryC<lb/></head>

			<figure>800<lb/> 700<lb/> 600<lb/> 500<lb/> 400<lb/> 300<lb/> 200<lb/> TIMSSMaths<lb/> PISAMathsTU<lb/> TIMSS95thPC<lb/> TIMSS5thPC<lb/> PISA95thTU<lb/> PISA5thTU<lb/> Figure 4.8 Comparison of PISA and TIMSS 95<lb/> th and 5<lb/> th percentile points<lb/></figure>

			<p>210.<lb/> Factors that have an impact on the differential spread of ability distributions in PISA and TIMSS<lb/> will be difficult to identify, as there are likely between-country differences in terms of characteristics of<lb/> samples and advantages/disadvantages in content balance. One can only make some hypotheses. For<lb/> example, in Hong Kong and Tunisia, the spread of the achievement distribution is considerably larger in<lb/> PISA than in TIMSS. One possible reason for this is that, in both Hong Kong and Tunisia, the PISA<lb/> sample contains many students from grades lower than the typical grade of 15 year-olds.</p>

			<figure>Figure 4.9 shows<lb/> the histograms of grade distribution for Hong Kong and Tunisia.<lb/> Hong Kong<lb/> Tunisia<lb/> 12<lb/> 11<lb/> 10<lb/> 9<lb/> 8<lb/> 7<lb/> 6<lb/> Grade Q1a<lb/> 50000.0000<lb/> 40000.0000<lb/> 30000.0000<lb/> 20000.0000<lb/> 10000.0000<lb/> 0.0000<lb/> Frequency<lb/> Mean =9.38<lb/> Std. Dev. =0.871<lb/> N =72,484<lb/> Adjudicated sub-region: Hong Kong SAR<lb/> Cases weighted by Student final weight<lb/> 12<lb/> 11<lb/> 10<lb/> 9<lb/> 8<lb/> 7<lb/> 6<lb/> Grade Q1a<lb/> 60000.0000<lb/> 50000.0000<lb/> 40000.0000<lb/> 30000.0000<lb/> 20000.0000<lb/> 10000.0000<lb/> 0.0000<lb/> Frequency<lb/> Mean =8.88<lb/> Std. Dev. =1.133<lb/> N =150,875<lb/></figure>

			<p>Adjudicated sub-region: Tunisia<lb/></p>

			<p>Cases weighted by Student final weight<lb/></p>

			<head>Figure 4.9 Grade distributions of Hong Kong and Tunisia PISA samples<lb/></head>

			<p>211.<lb/> The results in this section show that the spread of ability distributions need to be interpreted with<lb/> regard to the population definition. If one makes a statement that students in country X are more similar in<lb/> ability than students in country Y, the underlying population definition must be referred to. The<lb/> comparisons between TIMSS and PISA show that, within a country, the variation in student abilities for a<lb/> grade level may be quite different from the variation in abilities for an age group. Further, in general,<lb/> variations of mathematics abilities for an age group tend to be larger than variations for a grade level.<lb/></p>

			<head>The impact of calculator availability on differences in performance in TIMSS and PISA<lb/></head>

			<p>212.<lb/> In Chapter 2, it was found that the percentages of students with access to a calculator differ<lb/> across countries, and differ between the two surveys. These differences do not have an impact on the<lb/> differential performance of countries in TIMSS and PISA. The correlation between these two variables is<lb/> 0.106 (p=0.676). That is, the different policies on calculator use in TIMSS and PISA do not explain the<lb/> differences in country rankings in TIMSS and PISA.<lb/></p>

			<head>Summary<lb/></head>

			<p>213.<lb/> This chapter explores the factors that may have an impact on the observed differences between<lb/> country mean scores in PISA and TIMSS. It is found that the age at testing in TIMSS has a positive<lb/> relationship with the differential performance in TIMSS and PISA. That is, countries with an older cohort<lb/> of students tend to perform relatively better in TIMSS than in PISA. However, this does not mean that<lb/> better performance in TIMSS is necessarily due to older students in the TIMSS sample. In fact, there is a<lb/> strong negative correlation between age at time of testing in TIMSS and years of schooling at time of<lb/> testing in PISA. The older the students are in the TIMSS sample for a country, the fewer number of years<lb/> of schooling the students have at time of testing in PISA. Therefore, a relatively better performance in<lb/> PISA could be due to more years of schooling at time of testing in PISA.<lb/> 214.<lb/> Further, a Content Advantage Index was constructed based on each country&apos;s performance in the<lb/> five content areas of mathematics in TIMSS, and the relative proportions of items in the content areas in<lb/> each survey. Given that the content balance of PISA is quite different from that of TIMSS, some countries<lb/> have more advantage in PISA and some have more advantage in TIMSS, depending on their relative<lb/> strengths and weaknesses in the content areas. It was shown that the differential performance of countries<lb/> in PISA and TIMSS was closely related to the content balance of each survey.<lb/> 215.<lb/> Using three variables (TIMSS mathematics country mean score, Age at time of TIMSS testing,<lb/> Content Advantage Index) as predictors for PISA mathematics country mean scores, it was found that 93%<lb/> of the variance of PISA mathematics scores could be explained by these three predictors. Further, as there<lb/> is more reading demand in the PISA mathematics tests, if PISA reading score is used as an additional<lb/> explanatory variable, then 97% of the variance of the PISA mathematics scores can be explained. With<lb/> these four predictor variables, 11 out of the 22 countries have a predicted PISA mathematics score within<lb/> the confidence interval of the reported PISA score.<lb/> 216.<lb/> The standard deviations of achievement distributions in PISA are generally larger than the<lb/> standard deviations in TIMSS. This is likely the result of different population definitions where PISA<lb/> samples contain students from multiple grades while TIMSS controls for the grade level.<lb/></p>

			<head>CHAPTER 5 -GENDER DIFFERENCE AND ATTITUDES<lb/></head>

			<p>Introduction<lb/> 217.<lb/> It will be of interest to compare the results drawn from PISA and TIMSS in terms of the<lb/> performance of subgroups of students, such as girls and boys. In addition, both PISA and TIMSS<lb/> collected data on student home background information and attitudes towards mathematics. A comparison<lb/> of the similarities or differences in the findings will be interesting. If the same results are found, then each<lb/> study provides some evidence of validity for the other study. If different results are found, it will be<lb/> interesting to investigate why there are differences, and, in doing so, one hopes to gain a better<lb/> understanding of the results of each study.<lb/></p>

			<head>Gender differences<lb/> Gender differences for overall mathematics scale<lb/></head>

			<p>218.<lb/> Do PISA and TIMSS arrive at the same conclusions about the performance of girls and boys? On<lb/> the surface, the answer seems to be &quot; No &quot; .<lb/></p>

			<head>219.<lb/> TIMSS found that<lb/></head>

			<p>On average, across all countries, there was essentially no difference in achievement between<lb/> boys and girls at either the eighth or fourth grade, although the situation varied from country to<lb/> country. <ref type="biblio">(IEA, 2003, p.47)<lb/></ref> 220.<lb/> In contrast, in PISA, apart from two countries (Iceland and Thailand) where girls&apos; mean<lb/> mathematics score is higher than that for boys, in all other 38 countries, boys&apos; mean score is higher,<lb/> although not all significant statistically. <ref type="figure">Figure 5</ref>.1 shows pictorially the differences between the<lb/> performance of girls and boys, for TIMSS and PISA, where darkened bars indicate statistical significance<lb/> (extracted from TIMSS report, p48, (<ref type="biblio">IEA, 2003</ref>), and PISA report, p97, (OECD, 2004)).<lb/> 221.<lb/> On first impression, there appears to be a discrepancy between TIMSS and PISA results of<lb/> gender differences across countries. In TIMSS, there are about equal numbers of countries where girls<lb/> performed better and where boys performed better. On the other hand, in PISA, boys performed better in<lb/> the majority of countries. If one uses TIMSS results, one might conclude that, at Grade eight, there is no<lb/> clear gender difference in mathematics performance. If one uses PISA results, one might conclude that<lb/> 15 year-old girls lag behind 15 year-old boys in mathematics performance among OECD countries 19 .<lb/> 19.<lb/> In 26 out of 40 countries in PISA, boys&apos; performance is statistically significantly better than girls&apos;<lb/> performance. However, collectively for the OECD population, 38 out of 40 countries showed that the mean<lb/> score for boys is higher than the mean score for girls. This is extremely significant statistically; (Consider<lb/> tossing a coin 40 times and obtain a head 38 times. There is little doubt that the coin is biased.)<lb/></p>

			<figure>Figure 5.1 Gender Difference in TIMSS and PISA for overall mathematics scale (Extract from<lb/> TIMSS and PISA reports)<lb/> TIMSS<lb/> PISA<lb/></figure>

			<p>222.<lb/> On closer examination, all the countries where girls performed better in TIMSS have not<lb/> participated in PISA. These countries are Serbia, Macedonia, Armenia, Moldova, Singapore, Philippines,<lb/> Cyprus, Jordan and Bahrain. None of these countries is an OECD member. On the other hand, in TIMSS,<lb/> in nine countries boys performed better including the OECD member countries the Flemish Community of<lb/> Belgium, Hungary, Italy and the United States (the other countries were Chile, Ghana, the Lebanon,<lb/> Morocco and Tunisia). So it appears that gender difference varies between countries, as noted in both the<lb/> TIMSS and PISA reports, and that different compositions of participating countries could provide a<lb/> different picture of overall gender difference in a study. For valid comparisons, one needs to look at the<lb/> same group of countries.<lb/> 223.<lb/> <ref type="figure">Figure 5</ref>.2 shows gender differences for the 22 countries that participated in both PISA and<lb/> TIMSS.<lb/></p>

			<figure>Figure 5.2 Comparisons of gender differences in PISA and TIMSS<lb/> -10<lb/> 0<lb/> 10<lb/> 20<lb/> 30<lb/></figure>

			<table>LVA<lb/> HKG<lb/> IDN<lb/> AUS<lb/> NLD<lb/> NOR<lb/> SWE<lb/> USA<lb/> ENG<lb/> SCO<lb/> QUE<lb/> HUN<lb/> JPN<lb/> BSQ<lb/> RUS<lb/> TUN<lb/> ONT<lb/> BFL<lb/> NZL<lb/> ITA<lb/> SVK<lb/> KOR<lb/> Male mean score -Female mean score (in TIMSS score<lb/> unit)<lb/> PISA gender<lb/> difference<lb/> TIMSS gender<lb/> difference<lb/> 224.<lb/> In <ref type="figure">Figure 5</ref>.2, the countries are arranged in descending order of the magnitude of gender<lb/> difference in PISA (in TIMSS score unit). For each country, the first bar shows gender difference (in mean<lb/> scores) in PISA, and the second bar immediately below the first bar shows gender difference (in mean<lb/> scores) in TIMSS for that country. If there is a high correlation between PISA and TIMSS gender<lb/> differences, one would expect the second bar for each country to be longest (and positive) at the top of<lb/> the graph, and decreases in length or becoming negative, as for the bars for PISA. However, it can be<lb/> seen that TIMSS gender differences do not diminish as one scans down the graph, as PISA gender<lb/> differences diminish. The correlation between PISA and TIMSS gender differences is 0.23, indicating<lb/> that there is a weak relationship between the two variables. Consequently, one cannot conclude that there is<lb/> a strong agreement between PISA and TIMSS results in terms of gender difference for the overall<lb/> mathematics scale.<lb/> 225.<lb/> It should also be noted that <ref type="figure">Figure 5</ref>.2 shows that, on average, gender difference is larger in PISA<lb/> than in TIMSS, from a visual comparison of the size and direction of the two bars for each country. That is,<lb/> on average, boys outperformed girls by a greater amount in PISA than in TIMSS. On average, gender<lb/> differences in TIMSS are not only smaller in magnitude, but girls outperformed boys in a number of<lb/> countries.<lb/> EDU/WKP(2010)5<lb/></p>

			<head>Gender differences by mathematics content areas<lb/></head>

			<p>226.<lb/> A comparison of the composition of PISA and TIMSS tests in terms of the balance of content<lb/> domains was presented in Chapter 3. It was shown that there was a significant difference in terms of the<lb/> proportion of items from each traditional content area. How do gender differences vary across mathematics<lb/> content areas? Both TIMSS and PISA found that gender difference is not uniform across mathematics<lb/> content areas. In TIMSS, girls performed better in Algebra, while boys performed better in Measurement.<lb/> In Algebra, girls performed significantly better than boys in 23 countries/regions, while boys<lb/> performed better in only 3 countries/regions. In measurement, boys performed significantly better than<lb/> girls in 15 countries/regions, while girls performed better in only 2 countries/regions. For Number,<lb/> Geometry and Data, the difference in performance between gender groups is not as great as for Algebra<lb/> and for Measurement. However, there is some suggestion that boys performed a little better than girls<lb/> did, on average, in these three content areas. <ref type="table">Table 5</ref>.1 provides a tally of the number of<lb/> countries/regions in terms of differential performance between boys and girls.<lb/></p>

			<table>Table 5.1 Comparison of performance of boys and girls in TIMSS by content area<lb/> Mathematics content area<lb/> Number of<lb/> countries/regions in which<lb/> BOYS PERFORMED<lb/> BETTER<lb/> Number of<lb/> countries/regions in which<lb/> GIRLS PERFORMED<lb/> BETTER<lb/> Number of<lb/> countries/regions in which<lb/> there is NO DIFFERENCE<lb/> between performance of<lb/> boys and girls<lb/> Number<lb/> 14<lb/> 10<lb/> 26<lb/> Algebra<lb/> 3<lb/> 23<lb/> 24<lb/> Measurement<lb/> 15<lb/> 2<lb/> 33<lb/> Geometry<lb/> 13<lb/> 8<lb/> 29<lb/> Data<lb/> 9<lb/> 8<lb/> 33<lb/></table>

			<p>227.<lb/> For PISA, gender differences are examined by Overarching Ideas. <ref type="table">Table 5</ref>.2 shows a summary of<lb/> the results.<lb/></p>

			<table>Table 5.2 Comparison of performance of Boys and Girls in PISA by Overarching Ideas<lb/> Mathematics content area<lb/> Number of<lb/> countries/regions in which<lb/> BOYS PERFORMED<lb/> BETTER<lb/> Number of<lb/> countries/regions in which<lb/> GIRLS PERFORMED<lb/> BETTER<lb/> Number of<lb/> countries/regions in which<lb/> there is NO DIFFERENCE<lb/> between performance of<lb/> boys and girls<lb/> Space and shape<lb/> 32<lb/> 1<lb/> 7<lb/> Change and relationship<lb/> 21<lb/> 1<lb/> 18<lb/> Quantity<lb/> 16<lb/> 1<lb/> 23<lb/> Uncertainty<lb/> 29<lb/> 2<lb/> 9<lb/></table>

			<p>228.<lb/> Although PISA and TIMSS have different proportions of countries in which boys outperformed<lb/> girls, there is still some consistency in the results from the two studies. If PISA&apos;s overarching idea Space<lb/> and Shape can be mapped onto TIMSS&apos; Measurement and Geometry domains (see <ref type="table">Table 3</ref>.6), then both<lb/> studies found that boys outperformed girls by the greatest amount in this content area.<lb/> 229.<lb/> In PISA, differential performance between boys and girls is least in the area of the overarching<lb/> idea Quantity (although boys still performed significantly better). As PISA&apos;s overarching idea Quantity<lb/> can be regarded as a subset of TIMSS&apos; Number content domain (see the Quantity section in Chapter 3), it<lb/> is observed that the gender difference for the content domain Number in TIMSS is also less than the<lb/> difference in <ref type="biblio">Measurement.<lb/></ref> 230.<lb/> In TIMSS, at the international level, the only content area in which girls outperformed boys is<lb/> Algebra. In PISA, only 7 items are classified as Algebra. So one might conjecture that most PISA items<lb/> &quot; favour &quot; boys, since there are not many Algebra items in PISA. This could explain, at least in part, the<lb/> difference in the findings from the two studies for the overall mathematics scale. That is, in TIMSS, there<lb/> was little gender difference in overall mathematics performance internationally, while a large difference in<lb/> PISA was found.<lb/> 231.<lb/> In PISA, there is a large gender difference for the Overarching Idea Uncertainty (as compared to<lb/> the other Overarching Ideas), but in TIMSS, the content area Data showed less gender difference than, say,<lb/> for Number. Since Uncertainty could be regarded as part of the Data content area, there seems to be some<lb/> inconsistencies in the findings here. However, Data, in general, covers topics in statistics, while<lb/> Uncertainty deals more with Chance (as in Chance and Data). So it seems that Uncertainty in PISA is a<lb/> narrower domain than Data in TIMSS. Some aspects of Data in TIMSS may be classified as Change and<lb/> Relationships (e.g., graphing data) in PISA (see <ref type="table">Table 3</ref>.6). So one might conclude that, for topics dealing<lb/> with Chance, or, more formally, Probability, boys tend to do a great deal better than girls, as found in<lb/> PISA, but not for Data.<lb/> 232.<lb/> It should be noted that, while there are considerable variations across countries in gender<lb/> differences in both PISA and TIMSS, there is a consistent pattern of gender differences in the content<lb/> areas of mathematics. That is, in general, if one content area shows large gender difference<lb/> internationally, it is usually reflected in the gender difference within a country, where that content area<lb/> shows the largest gender difference relative to the other content areas within a country. This suggests that<lb/> there may be some underlying factors, whether biological or social, that differentiates between boys and<lb/> girls, so that boys may be naturally better than girls at certain tasks, and vice versa. For example, boys tend<lb/> to perform better in certain spatial tasks (Voyer, <ref type="biblio">Voyer and Bryden, 1995</ref>). Gallagher, Levin and Cahalan<lb/> (2002) also found that males performed relatively better on items requiring the use of spatial<lb/> representations, but gender difference is small on items involving the use of mathematics language and<lb/> recall of knowledge.<lb/> 233.<lb/> On the other hand, the vast variation in gender differences across countries indicates that gender<lb/> difference could be addressed, and there are many countries where gender difference in mathematics<lb/> performance is negligible. Further studies looking into how some countries address gender equity may be<lb/> useful. It should be noted that, while, in PISA, significant gender differences were found in mathematics,<lb/> the magnitude of the difference is much smaller than the gender difference found for Reading (OECD,<lb/> 2005, p.98). For Reading, girls outperformed boys by a great deal more. From this point of view, it is<lb/> somewhat surprising that more gender difference was observed in PISA than in TIMSS, since PISA<lb/> mathematics items require more reading than TIMSS mathematics items, as many TIMSS items are short<lb/> and context free items. If reading &quot; gets in the way &quot; of successfully responding to a mathematics item, then<lb/> one would expect the gender difference in PISA mathematics to be less than that observed in TIMSS.<lb/></p>

			<head>Possible explanations for observed differences between PISA and TIMSS in gender gap<lb/></head>

			<p>234.<lb/> In summary, there could be four reasons why the observed gender difference in PISA is larger<lb/> than in TIMSS:<lb/> 235.<lb/> First, in TIMSS and PISA reports, conclusions were drawn from an overall picture of gender<lb/> difference across countries. Since there are different countries participating in each study, the overall<lb/> pictures are somewhat different. An observation is that countries where girls&apos; performance is higher than<lb/> boys&apos; in TIMSS are not OECD member countries. It is possible that OECD countries, being more<lb/> developed than non-OECD countries in general, may have systematic similarities between themselves<lb/> (and differences from non-OECD countries) in education systems that exacerbate the gender difference in<lb/> mathematics achievement.<lb/> 236.<lb/> Second, it has been shown that gender differences are not uniform across mathematics content<lb/> areas. Since PISA and TIMSS have different content compositions for the tests, gender differences for the<lb/> overall test are likely to be different for PISA and TIMSS, depending on whether there are more items<lb/> giving advantage to boys or to girls in a test. From this point of view, gender differences should be<lb/> interpreted with regard to the composition of a test, and not just for mathematics as a whole.<lb/> 237.<lb/> There may also be subtle influences from the choice of particular contexts for individual items<lb/> which have not been detected by normal vetting procedures.<lb/> 238.<lb/> Third, PISA items are more application oriented, while TIMSS items are more curriculum<lb/> oriented. This difference in the emphasis of the type of the items may have different effects on the<lb/> performance of girls and boys. <ref type="biblio">Friedman (1989)</ref> found that girls are better at curriculum-based items, while<lb/> boys are better at problem-solving items. That is, girls are not as good as boys in applying mathematical<lb/> knowledge for functional use in everyday life. In PISA, the correlation between the problem solving<lb/> domain and the mathematics domain is a little higher for boys than for girls, indicating that there is a<lb/> slightly more consistent behaviour for boys than for girls in successfully (or unsuccessfully) answering<lb/> problem solving items and mathematics items in PISA.<lb/> 239.<lb/> Fourth, the PISA report described growing gender gap in mathematics achievement as students<lb/> get older (<ref type="biblio">Box 2.3, p96, OECD, 2004</ref>). Since PISA cohort has slightly older students than TIMSS&apos; cohort,<lb/> it is likely that gender differences for 15 to 16 years are greater than for 14-year-olds, as mathematics<lb/> becomes a more demanding subject. <ref type="biblio">Friedman (1989)</ref> found this result in the meta-analysis and also<lb/> commented on supporting findings from previous literature. Most commonly with young children (e,g. up<lb/> to age 10) no differences are found, or if found they favour girls. In the junior high school years, Friedman<lb/> reported that a mixed pattern develops, with either no differences or differences favouring boys, with the<lb/> exception that very gifted mathematics students are more likely to be boys. Differences favouring males<lb/> increase in older age groups. For example, the Australian report of the TIMSS Population 3 advanced<lb/> mathematics group (<ref type="biblio">Lokan and Greenwood, 2001</ref>) noted that in almost all countries and on all topics, there<lb/> were differences favouring males, often statistically significant, and these differences were much stronger<lb/> than in the TIMSS studies with younger students.<lb/> 240.<lb/> All of the above four points suggest that explanations are worth further investigation to try to<lb/> understand gender differences in mathematics achievement, and to put in place measures to reduce gender<lb/> differences within those countries where gender differences are large. <ref type="biblio">Friedman (1989)</ref> pointed to a social,<lb/> rather than a biological, explanation when commenting on the meta-analysis result that gender differences<lb/> had closed rapidly over two decades in the USA. This means that gender differences can be rectified with<lb/> appropriate intervention.<lb/></p>

			<head>Gender difference in the spread of achievement distributions<lb/></head>

			<table>241.<lb/> PISA noted that<lb/> Gender differences tend to be larger at the top end of the performance distribution. (OECD,<lb/> 2005, p.98)<lb/></table>

			<p>242.<lb/> That is, on average, there are more boys than girls at the top end of the scale. This can be readily<lb/> seen from <ref type="figure">Figure 5</ref>.3 where the percentages of boys and girls in Level 6 of the mathematics scale are<lb/> shown. Apart from Iceland where there are more girls than boys in Level 6, in every country where the<lb/> percentage of students in Level 6 is not zero, there are more boys than girls in Level 6. This finding is<lb/> consistent with that of Friedman (1989) who found that often gifted mathematics students are more likely<lb/> to be boys. In addition, in only 3 of 16 countries in the TIMSS study of advanced mathematics did more<lb/> females than males do advanced mathematics.<lb/> 243.<lb/> For TIMSS, the percentages of students in benchmark levels are not reported separately for boys<lb/> and girls. However, the standard deviations of performance distributions for girls and boys are reported.<lb/> These are shown in <ref type="figure">Figure 5</ref>.4.<lb/></p>

			<figure>Figure 5.3 Percentages of boys and girls in Level 6 of PISA mathematics scale<lb/> 0.0<lb/> 2.0<lb/> 4.0<lb/> 6.0<lb/> 8.0<lb/> 10.0<lb/> 12.0<lb/> 14.0<lb/> Figure 5.4 TIMSS standard deviations of performance distribution in mathematics for boys and girls<lb/> 0<lb/> 20<lb/> 40<lb/> 60<lb/> 80<lb/> 100<lb/> 120<lb/></figure>

			<table>Attitudinal scales<lb/> 245.<lb/> Both PISA and TIMSS collect information on students&apos; attitudes towards mathematics. In<lb/> particular, indices on &quot; self-confidence &quot; and &quot; interest and motivation &quot; have been constructed in both PISA<lb/> and TIMSS.<lb/></table>

			<head>Self-confidence index<lb/></head>

			<p>246.<lb/> <ref type="figure">Figure 5</ref>.5 shows a comparison of the self-confidence index in TIMSS and the self-concept index<lb/> in PISA. Note that the vertical axis shows the percentage of students who were assigned to the high level<lb/> of the TIMSS index. Box 6.1 details how each index was constructed.<lb/> 247.<lb/> The correlation between PISA and TIMSS self-confidence indices is 0.73. Indonesia appears an<lb/> outlier in <ref type="figure">Figure 5</ref>.5. Without Indonesia, the correlation is 0.83. So there is a close relationship between the<lb/> two indices. Further, in both PISA and TIMSS, it was found that the South East Asian countries (Hong<lb/> Kong-China, Japan and Korea) had a low self-confidence index, despite the fact that student achievement<lb/> was high in these countries. On the other hand, English speaking countries tend to have a high self-<lb/>confidence index in both PISA and TIMSS. Both PISA and TIMSS found that, within each country,<lb/> students with higher self-confidence index performed better than students with lower self-confidence<lb/> index.<lb/></p>

			<figure>Box 6.1 Measuring students&apos; confidence in mathematics in PISA and TIMSS<lb/> TIMSS: Index of students&apos; self-confidence in learning mathematics (SCM)<lb/></figure>

			<p>This index is based on students&apos; responses to four statements about their mathematics ability:<lb/> • I usually do well in mathematics;<lb/></p>

			<p>• Mathematics is more difficult for me than for many of my classmates * ;<lb/></p>

			<p>• Mathematics is not one of my strengths * ;<lb/> • I learn quickly in mathematics.<lb/></p>

			<figure>Figure 5.5 presents results for the students with a high level of self-confidence on this index. These students<lb/> agreed a little or agreed a lot with all four of the above statements on average.<lb/> PISA: Index of students&apos; self-concept in mathematics<lb/> This index is based on students&apos; level of agreement with the following statements about their mathematics ability:<lb/> • I am just not good at mathematics * ;<lb/> • I get good marks in mathematics;<lb/> • I learn mathematics quickly;<lb/> • I have always believed that mathematics is one of my best subjects;<lb/> • In my mathematics class, I understand even the most difficult work.<lb/> EDU/WKP(2010)5<lb/></figure>

			<p>Students could answer that they strongly agreed, agreed, disagreed or strongly disagreed with the above<lb/> statements. Positive values on this index indicate a positive self-concept in mathematics.<lb/> * The response categories for this statement were reversed in constructing the index.<lb/></p>

			<figure>Figure 5.5 Comparison of PISA and TIMSS self-confidence indices<lb/> 0.25<lb/> 0.00<lb/> -0.25<lb/> -0.50<lb/> PISA_SelfConcept<lb/> 80.00<lb/> 60.00<lb/> 40.00<lb/> 20.00<lb/> 0.00<lb/> TIMSS_SelfConf<lb/> QUE<lb/> ONT<lb/> USA<lb/> TUN<lb/> SWE<lb/> SVK<lb/> SCO<lb/> RUS<lb/> NZL<lb/> NOR<lb/> NLD<lb/> LVA<lb/> KOR<lb/> JPN<lb/> ITA<lb/> IDN<lb/> HUN<lb/> HKG<lb/> ENG<lb/> BFL<lb/> AUS<lb/></figure>

			<head>Interest and motivation indices<lb/></head>

			<p>248.<lb/> In TIMSS, an index of &quot; Students&apos; Valuing Mathematics &quot; was constructed from questions about<lb/> students&apos; interest, enjoyment, and motivation about doing mathematics. In PISA, two separate indices were<lb/> constructed. The first is &quot; Students&apos; interest in and enjoyment of mathematics &quot; . The second is &quot; Students&apos;<lb/> instrumental motivation in mathematics &quot; . For the comparison shown here, PISA&apos;s second index is used.<lb/> Box 6.2 details the components of each index. As can be seen, with the exception of the first two<lb/> statements in the TIMSS index of valuing mathematics, the two measures are very similar in the way they<lb/> capture students&apos; motivation to learn mathematics for some external motivating factor whether it be<lb/> helping them in their current studies, future studies or indeed future work. The first two statements in the<lb/> TIMSS index of valuing mathematics are similar to components of PISA&apos;s index of interest and enjoyment<lb/> of mathematics, that is, they capture students&apos; enthusiasm for learning the subject itself.<lb/></p>

			<figure>Box 6.2 Measuring students motivation to learn mathematics in PISA and TIMSS<lb/> TIMSS: Index of students valuing mathematics<lb/> This index is based on students&apos; responses to seven statements about mathematics:<lb/> • I would like to take more mathematics in school;<lb/> • I enjoy learning mathematics;<lb/> • I think learning mathematics will help me in my daily life;<lb/> • I need mathematics to learn other school subjects;<lb/> • I need to do well in mathematics to get into the university of my choice;<lb/> • I would like a job that involved using mathematics;<lb/> • I need to do well in mathematics to get the job I want.<lb/> Figure 5.6 presents results for the students with a high level on the valuing mathematics index. These students<lb/> agreed a lot with all seven of the above statements on average.<lb/> PISA: Index of students&apos; instrumental motivation in mathematics<lb/></figure>

			<p>This index is based on students&apos; level of agreement with the following statements about mathematics:<lb/></p>

			<p>• Making an effort in mathematics is worth it because it will help me in the work that I want to do later on;<lb/></p>

			<p>• Learning mathematics is worthwhile for me because it will improve my career prospects;<lb/> • Mathematics is an important subject for me because I need it for what I want to study later on ;<lb/> • I will learn many things in mathematics that will help me get a job.<lb/></p>

			<p>Students could answer that they strongly agreed, agreed, disagreed or strongly disagreed with the above<lb/> statements. Positive values on this index indicate higher levels of instrumental motivation to learn mathematics.<lb/></p>

			<figure>Figure 5.6 Comparison of indices of valuing/motivation in mathematics in TIMSS and PISA<lb/> 0.50<lb/> 0.25<lb/> 0.00<lb/> -0.25<lb/> -0.50<lb/> -0.75<lb/> PISA_InstrMotiv<lb/> 80.00<lb/> 70.00<lb/> 60.00<lb/> 50.00<lb/> 40.00<lb/> 30.00<lb/> 20.00<lb/> 10.00<lb/> TIMSS_ValueMaths<lb/> QUE<lb/> ONT<lb/> USA<lb/> TUN<lb/> SWE<lb/> SVK<lb/> SCO<lb/> RUS<lb/> NZL<lb/> NOR<lb/> NLD<lb/> LVA<lb/> KOR<lb/> JPN<lb/> ITA<lb/> IDN<lb/> HUN<lb/> HKG<lb/> ENG<lb/> BFL<lb/> AUS<lb/> 249.<lb/> <ref type="figure">Figure 5</ref>.6 shows a comparison of PISA and TIMSS indices on valuing/motivation in<lb/> mathematics. The correlation between TIMSS &quot; Valuing Mathematics index &quot; and PISA &quot; Instrumental<lb/> Motivation index &quot; is 0.87, showing a close relationship between these two variables.<lb/> 250.<lb/> The similarities in the findings of PISA and TIMSS on student motivation and self-confidence in<lb/> doing mathematics provide some reassurance of the validity of the results. It also suggests that, regardless<lb/> whether the sample is age based or grade based, and whether the sample consists of 14 or 15-year-olds,<lb/> profiles of students&apos; attitudes towards mathematics are largely stable.<lb/></p>

			<head>Gender differences in attitudes towards mathematics<lb/></head>

			<p>251.<lb/> One of the important findings in PISA is that, despite only moderate differences in mathematics<lb/> performance between girls and boys, where girls lag a little behind boys on average, there is a large<lb/> difference between attitudes of girls and boys towards mathematics.<lb/></p>

			<head>A</head>

			<table>first striking finding is that while gender differences in student performance tend to be modest,<lb/> there are marked differences between males and females in their interest in and enjoyment of<lb/> mathematics as well as in their self-related beliefs, emotions and learning strategies related to<lb/> mathematics. (p.151, OECD, 2004)<lb/> 252.<lb/> PISA made the comparisons between achievement differences and attitude differences based on<lb/> &quot; effect size &quot; 20 (p.152 – 153, OECD, <ref type="biblio">2004</ref>). In TIMSS, gender differences in attitude are not reported. In<lb/> this report, the average of TIMSS index of self-confidence was computed separately for girls and<lb/> boys. These indices are compared to PISA&apos;s scale of self-confidence. The results are shown in <ref type="figure">Figure<lb/> 5</ref>.7 21 . Since the metrics of attitude indices are different in the two studies, the indices have been scaled<lb/> to have the same variance. So the actual relative positions of the index between PISA and TIMSS is<lb/> not important (consequently, no vertical scale unit is shown), but the relative size of the gender<lb/> difference within countries, and the trends across countries, can be compared.<lb/></p>

			<figure>Figure 5.7 Self-confidence in mathematics, by gender<lb/></figure>

			<table>AUS<lb/> BFL<lb/> ENG<lb/> HKG<lb/> HUN<lb/> IDN<lb/> ITA<lb/> JPN<lb/> KOR<lb/> LVA<lb/> NLD<lb/> NOR<lb/> NZL<lb/> RUS<lb/> SCO<lb/> SVK<lb/> SWE<lb/> TUN<lb/> USA<lb/> TIMSS GIRLS<lb/> TIMSS BOYS<lb/> PISA GIRLS<lb/> PISA BOYS`253 BOYS`<lb/> BOYS`253.<lb/></table>

			<p>A number of observations can be made from the results shown in <ref type="figure">Figure 5</ref>.7. First, the rise and<lb/> fall of the lines across countries is similar between PISA and TIMSS. This means that countries where<lb/> students expressed high (or low) self-confidence are consistent in PISA and TIMSS. For example, in Japan<lb/> and Korea, students rated themselves low on the self-confidence scale in both PISA and TIMSS. This has<lb/> been discussed in the section on self-confidence index where it is shown that the correlation between the<lb/> self-confidence index between PISA and TIMSS is high.<lb/> 254.<lb/> Second, in both PISA and TIMSS, boys expressed higher self-confidence in mathematics in<lb/> every country, except for Russia in TIMSS. In PISA, all 19 countries 22 had a higher mean achievement<lb/> score for boys 23 , so it is expected that boys would have higher self-confidence score than girls. However,<lb/> in TIMSS, girls had a higher mean achievement score 24 in eight of the 19 countries, but girls had lower<lb/> self-confidence than boys in all countries except for the Russian Federation. This supports the finding in<lb/> PISA that the gender gap in attitudes towards mathematics is larger than the gender gap in</p>

			<table>achievement<lb/> scores.<lb/> 20.<lb/> Effect size is a measure of the magnitude of the difference in relation to the variance of all the scores (see<lb/> Box 3.3, p117, OECD, 2004).<lb/> 21.<lb/> Note that data for Spain-Basque country, Canada-Ontario and Canada-Quebec are not available for this<lb/> graph<lb/> 22.<lb/> Data for Canada-Ontario, Canada-Quebec and Spain-Basque country are not available.<lb/> 23.<lb/> Although not always statistically significant.<lb/> 24.<lb/> Although not always statistically significant.<lb/></table>

			<p>255.<lb/> Third, the magnitude of the gender difference in self-confidence for each country is similar<lb/> between PISA and TIMSS results. For example, in TIMSS, the gender gap in self-confidence is small for<lb/> Indonesia, Latvia, the Russian Federation and the Slovak Republic. In PISA, these countries also have<lb/> relatively smaller gender differences. On the other hand, the Netherlands and Sweden have larger gender<lb/> differences in self-confidence in both PISA and TIMSS. In general, in both studies, Eastern European<lb/> countries tend to have smaller gender gap in self-confidence, while Western countries and Asian<lb/> countries have larger gender gap. This is an interesting observation in itself and further investigation will<lb/> be worthwhile to study how Eastern European countries reduce gender gap in attitudes towards<lb/> mathematics, or, how Western and Asian countries create gender gap.<lb/> 256.<lb/> Fourth, the correlation of the magnitude of gender gap in self-confidence between PISA and<lb/> TIMSS is moderately high (0.72), as compared to the correlation of the gender gap in mathematics<lb/> achievement between PISA and TIMSS (0.25). This suggests that attitude scales are more invariant than<lb/> achievement scales. Achievement scales are more sensitive to variations such as years of schooling and<lb/> the content balance of the assessments. Importantly this moderately high correlation between the surveys<lb/> in magnitude of gender gap in self-confidence validates the findings from each survey and suggests that<lb/> this gender gap is quite established across the ages of 14 to 16 years.<lb/> 257.<lb/> Fifth, the correlation between gender gap in achievement and gender gap in self-confidence<lb/> across countries is relatively weak in both PISA and TIMSS. That is, in countries where boys outperform<lb/> girls by a great deal, the difference in self-confidence is not necessarily large. However, in TIMSS, the<lb/> relationship is slightly stronger than the relationship in PISA, with a correlation of 0.25 for TIMSS, and -<lb/>0.31 for PISA! This could be due to differences in the nature of the two assessments. While, in TIMSS,<lb/> the test is more curriculum-based, so performance on the test has a closer relationship with teaching and<lb/> learning in schools, which in turn relates to students&apos; self-confidence as they work with mathematics in<lb/> schools, while, in PISA, the test is not so curricular focused. The extent to which students can solve PISA<lb/> items may not be as closely related to school-based instructions as in TIMSS.<lb/> 258.<lb/> In this section, only the self-confidence index is examined. It is expected, however, that other<lb/> attitudinal indices will provide similar findings.<lb/></p>

			<head>Socio-economic Background of Students<lb/></head>

			<p>259.<lb/> Both PISA and TIMSS collected information on a number of student background variables. PISA<lb/> reported students&apos; performance in relation to parental occupation, parental education, possessions, single<lb/> parent family, immigrant status and language spoken at home. TIMSS reported student performance in<lb/> relation to parental education, language spoken at home, number of books in the home, availability of<lb/> study desk/table in the home, and the use of computers at home and at other places.<lb/> 260.<lb/> In relation to parental education, TIMSS found that, on average, for the Grade 8 cohort, students<lb/> with university-educated parents 25 scored more than 90 points higher than students whose parents had<lb/> primary or lower education (IEA, 2003, p.127). In PISA, on average across OECD countries, students<lb/> whose fathers completed tertiary education scored around 90 points higher than students whose fathers<lb/> completed primary or lower secondary education (OECD, 2004,</p>

			<table>Table 4.2c). While the score units are not<lb/> exactly the same in PISA and TIMSS, and the composition of the countries are different in the two<lb/> surveys, it is still evident that both surveys found a significant impact of parental education on student<lb/> performance in mathematics, and the magnitude of the impact is also of similar order of magnitude.<lb/> 25.<lb/> This is defined as the highest education level of either parent<lb/> 261.<lb/> In relation to home possessions, TIMSS reported student performance against each type of home<lb/> possessions (the number of books at home, the provision of a desk/table, and the availability of a computer<lb/> at home) separately. In contrast, PISA constructed an index of cultural possessions from the number of<lb/> classical literature, books of poetry and works of art in students&apos; homes (OECD, 2004, p.166; OECD,<lb/> 2004, Table 4.2d; OECD, 2005, p.283). A direct comparison of results is difficult because different<lb/> variables were used in PISA and TIMSS. However, both PISA and TIMSS found a strong relationship<lb/> between home possession and student performance.<lb/> 262.<lb/> The PISA report (OECD<ref type="biblio">, 2004</ref>) provided a more detailed analysis on the impact of socio-<lb/>economic status on students&apos; performance, both at individual student level and at the school level.<lb/> 263.<lb/> Despite differences in the collection of student background variables, the message is clear from<lb/> both PISA and TIMSS that student background has a significant impact on achievement level.<lb/></p>

			<head>Summary<lb/></head>

			<table>264.<lb/> This chapter explores similarities and differences in the findings of PISA and TIMSS in relation<lb/> to student backgrounds and attitudes towards mathematics.<lb/> 265.<lb/> On the surface, it appears the PISA found that boys performed better than girls, while TIMSS<lb/> found little gender differences for</table>

			<p>the grade 8 cohort. On a closer examination of the results, however, the<lb/> different findings in PISA and TIMSS can be attributed to different cohort of countries and different<lb/> content balance in the respective tests. In particular, girls performed better in algebra in TIMSS, but there<lb/> were very few algebra items in PISA. The pattern of gender differences is consistent between PISA and<lb/> TIMSS findings. For example, both TIMSS and PISA found that boys outperformed girls by the greatest<lb/> amount in the content areas of measurement and geometry (or space and shape as labeled in PISA). In<lb/> contrast, both TIMSS and PISA found the smallest gender difference for the number content area (or<lb/> quantity as labeled in PISA).<lb/> 266.<lb/> Further, in both TIMSS and PISA, the spread of mathematics achievement distribution is larger<lb/> for boys than for girls.<lb/> 267.<lb/> Both PISA and TIMSS constructed indices of students&apos; self-confidence. There is a strong<lb/> agreement in the measures of this index at the country level between PISA and TIMSS. Similarly, TIMSS&apos;<lb/> Valuing index and PISA&apos;s Motivation index have a correlation of 0.89 between country means, showing a<lb/> close relationship between these two variables. In short, there is a good agreement between PISA and<lb/> TIMSS on the measures of students&apos; attitudes towards mathematics, despite different age groups in the two<lb/> surveys.<lb/> 268.<lb/> Information on students&apos; socio-economic background was also collected in both PISA and<lb/> TIMSS. Both surveys found that socio-economic background had a significant impact on students&apos;<lb/> performance in mathematics.<lb/></p>

			<figure>CHAPTER 6 -CONCLUSIONS<lb/> Overview<lb/> 269.<lb/> The concurrent testing of TIMSS and PISA in 2003 provided education researchers with a<lb/> unique opportunity to gain an in-depth understanding of the results of international large-scale<lb/> assessments. Comparisons of the methodologies and the results between the two surveys led to a number of<lb/> important findings that not only would help us interpret the survey results, but also would help us plan<lb/> future surveys. This chapter discusses the findings and their implications for current and future PISA and<lb/> TIMSS surveys.<lb/></figure>

			<p>270.<lb/> The most important finding in this report is that differential performances of countries in TIMSS<lb/> and PISA can be accounted for by a number of factors. In particular, test content balance is the most<lb/> significant factor. Other factors include years of schooling and reading load in items. The identification of<lb/> factors not only enables us to draw valid conclusions from country scores and rankings, it also provides us<lb/> with evidence that both PISA and TIMSS conducted the surveys with rigorous procedures, since the results<lb/> can be cross verified and validated across the two surveys. Without two concurrent surveys, it would not be<lb/> possible to examine reliability and validity to this extent. Some specific findings and their implications are<lb/> discussed below.<lb/></p>

			<head>The impact of content balance on achievement results<lb/></head>

			<p>271.<lb/> A re-classification of PISA items according to TIMSS content domains (number, algebra,<lb/> measurement, geometry and data) shows that PISA and TIMSS have quite different test content balance.<lb/> PISA has more number and data items, and fewer algebra items. The differences in test content balance<lb/> between the two surveys account for 66% of the variation of country mean score differences between PISA<lb/> and TIMSS. This finding suggests that mathematics curriculum coverage has a significant impact on<lb/> student performance in PISA and TIMSS, and it is consistent with TIMSS&apos; finding that different countries<lb/> have different strengths and weaknesses in the five mathematics content areas (<ref type="biblio">Exhibit 3.1, IEA, 2003</ref>).<lb/> 272.<lb/> A link between students&apos; performance and a country&apos;s curriculum is a reasonable conjecture, but<lb/> it is not readily established, as it involves an extensive survey of the curriculum of each country. TIMSS<lb/> conducted a survey of the mathematics curriculum in each country. However, the results of the curriculum<lb/> survey were not examined closely in relation to the relative performance of each country in each<lb/> mathematics topic. There is only one note in the TIMSS 2003 International Mathematics Report (IEA,<lb/> 2003) about the link between curriculum coverage and achievement results:<lb/></p>

			<p>Although the relationship between inclusion in the intended curriculum and student achievement was<lb/> not perfect, it was notable that several of the higher-performing countries reported high levels of<lb/> emphasis on the mathematics topics in their intended curricula and that those with the lowest levels of<lb/> curricular coverage came from the lower half of the achievement distribution. <ref type="biblio">(p.182, IEA, 2003)<lb/></ref> 273.<lb/> Below we take a look at the relationship between the implemented curriculum (as represented by<lb/> instructional time) and achievement scores.<lb/> 274.<lb/> A survey of the percentage of instructional time in mathematics class devoted to TIMSS content<lb/> areas shows some variations across countries (<ref type="biblio">Exhibit 7.4, IEA, 2003</ref>). For example, in the Russian<lb/> Federation, only 3% of time in mathematics class is devoted to the data strand at Grade 8 level, while in<lb/> Australia, 14% of time is devoted to the data strand. One might hypothesise that such differences in<lb/> curriculum emphases of different mathematics content areas will lead to differential student performances<lb/> in the content areas. To test this hypothesis, the correlation between Percentage time in mathematics<lb/> class devoted to a content strand and Deviation of achievement score in TIMSS content strand from<lb/> TIMSS mean score is computed and presented in <ref type="table">Table 6</ref>			<figure>.1.<lb/> Table 6.1 Correlation between percentage of instructional time and achievement score in TIMSS 2003<lb/> expressed as deviation from the TIMSS mean score over 20<lb/> 26 countries<lb/> Correlation<lb/> p<lb/> Number<lb/> -0.043<lb/> 0.87<lb/> Algebra<lb/> 0.540<lb/> 0.045<lb/> Measurement<lb/> 0.223<lb/> 0.389<lb/> Geometry<lb/> 0.708<lb/> 0.001<lb/> 275.<lb/> The figures in <ref type="table">Table 6</ref>.1 show that, in geometry, instructional time has a high correlation with the<lb/> TIMSS achievement score. In algebra and data, the relationship between instructional time and<lb/> achievement score is moderate. In measurement, the relationship is low. In number, there appears to be no<lb/> relationship between the amount of instructional time and achievement score! These results may be<lb/> consistent with the view that number, measurement and data are topics which one encounters frequently in<lb/> everyday life, but algebra and geometry are topics that need to be studied formally to understand the<lb/> concepts and the use of special terminology, symbols and formulae. In PISA, there are few algebra and<lb/> geometry items. Consequently, it could be said that the PISA test contains items that require less direct<lb/> instruction on the specific mathematics content. From this point of view, the PISA achievement score<lb/> reflects everyday use of mathematics, which may or may not be learned at schools, while TIMSS<lb/> achievement score reflects more school mathematics. It should also be noted that instructional time in<lb/> <ref type="table">Table 6</ref>.1 is for Grade 8. Some countries teach topics such as number and measurement mostly in primary<lb/> schools, so that instructional time at Grade 8 level could be quite little. But this does not necessarily<lb/> indicate that the topics are not taught. Rather, they are taught in earlier grades.<lb/> 276.<lb/> Accordingly, a country with a high score in PISA shows that the students are good at &quot; everyday<lb/> mathematics &quot; , while a high score in TIMSS shows that the students are good at &quot; school mathematics &quot; .<lb/> For example, in terms of country mean scores, Australia performed significantly better than Hungary in<lb/> PISA, but Hungary performed significantly better than Australia in TIMSS. One might conclude that<lb/> students in Hungary are better at typical school mathematics than Australian students, but they are not as<lb/> good as Australian students in using mathematics for everyday life applications. The fact that there are<lb/> differences in country rankings between PISA and TIMSS results suggests that, at least in some countries,<lb/> school mathematics has not prepared students as well in the application of mathematics as in academic<lb/> mathematics. Conversely, there are countries that have not prepared students as well in specialist areas of<lb/> mathematics, such as algebra and geometry, as they have prepared students in solving mathematics<lb/> problems in everyday life. The question as to which approach is better or which curriculum balance is the<lb/> best will be for the education policy makers in each country to consider in their own context, and,<lb/> certainly, neither PISA nor TIMSS alone should set the directions for future mathematics curriculum<lb/> reform.<lb/> 277.<lb/> As content balance has such a significant impact on country performances in mathematics, to<lb/> establish reliable trend indicators from one survey cycle to another, each survey will need to maintain a<lb/> stable frame of reference in terms of content balance. This may be more difficult to achieve in PISA than<lb/> 26<lb/> Note that there are no data for the instructional time by content domain for England and Scotland.<lb/> in TIMSS, since PISA does not examine content balance in terms of traditional mathematics curriculum<lb/> strands. Given that there is not a one-to-one mapping between the Overarching Ideas and traditional<lb/> mathematics curriculum strands, the PISA content balance in terms of traditional mathematics curriculum<lb/> strands may vary across different cycles of PISA. This issue needs to be monitored carefully. On the other<lb/> hand, the content balance of an assessment cannot be static over many years, since the world will change<lb/> over time. What is relevant now may no longer be relevant in ten years&apos; time. So test content must change<lb/> accordingly. One can already see changes in TIMSS, such as the permission to use calculators in the<lb/> tests in 2003.<lb/> 278.<lb/> Even if the test contents can stay relatively stable across survey cycles, the curricula of countries<lb/> change continually (Leung, <ref type="biblio">Graf &amp; Lopez-real, 2006</ref>). TIMSS (IEA, 2003, p.165) also reported that the<lb/> curricula in participating countries were being revised constantly. This in turn will affect trend estimates<lb/> for the countries. Therefore, as a note of caution, in assessing trends across survey cycles, factors such as<lb/> content changes in the tests or curriculum changes within countries must be taken into account.<lb/> 279.<lb/> The impact of content balance on achievement also casts some doubts about the reliable measure<lb/> of growth between two grade levels, particularly if the grade levels are far apart, since the alignment of<lb/> content between different grade levels will be difficult, as topics and content inevitably change across<lb/> school grades. For example, algebra is usually not taught until secondary schools. Arithmetic computations<lb/> are more the focus of primary schools but not secondary schools. In primary schools, students mostly<lb/> work with concrete representations while secondary school students often work with abstract<lb/> representations. These differences in content balance between primary and secondary school mathematics<lb/> will make any measure of growth difficult when students cannot easily make use of their knowledge and<lb/> skills across different content areas.<lb/> 280.<lb/> More generally, one message for developers of assessments of mathematics is that the issue of<lb/> content balance should receive careful consideration and discussion, as performance results may be<lb/> sensitive to the composition of items from different content areas. The assumption made in many<lb/> assessment programmes about the uni-dimensionality of mathematics items may need careful checking.<lb/> That is, there is some evidence that mathematics items in PISA and TIMSS measure multiple abilities than<lb/> a single ability.<lb/></p>

			<head>The impact of reading load on mathematics achievement results<lb/></head>

			<p>281.<lb/> Leaving aside the three South-East Asian countries (Japan, Korea and Hong Kong-China), PISA<lb/> reading country mean score is found to be a good predictor of the differences between TIMSS and PISA<lb/> scores. As there is a considerable amount of reading in PISA tests compared with TIMSS tests, it is<lb/> reasonable to assume that poor readers will not perform as well in PISA as in TIMSS. It is also possible<lb/> that, in countries where reading achievement is relatively higher, students may be exposed to an<lb/> environment which facilitates mathematics problem-solving skills in everyday life. For example, students<lb/> may read newspapers more often, and be familiar with presentations of charts and diagrams for summary<lb/> of information, or, students may have more opportunity to be intelligent consumers such as through<lb/> reading advertisements of mobile phone cost plans. Unfortunately, it is difficult to disentangle these factors<lb/> from the current datasets.<lb/></p>

			<head>Grade-based and age-based samples<lb/></head>

			<p>282.<lb/> It has been contentious whether a grade-based sample or an age-based sample provides the most<lb/> comparable results in international surveys. The proponents for an age-based sample (OECD, 2004, p.27)<lb/> argue that, as each country has different school systems with different number of years of pre-schools, it is<lb/> difficult to define a grade at which the number of years of schooling is aligned across countries. In<lb/> contrast, an age-based sample defines the population according to an age range, where age is always<lb/> clearly defined. However, in the case of an age-based sample, the number of years of schooling will clearly<lb/> be different across countries. Proponents for a grade-based sample (IEA, 2003, p18) argue that there is a<lb/> better chance to align the number of years of schooling with grade-based sample. Further, a grade-based<lb/> sample provides the opportunity to collect information about classroom instructions and implemented<lb/> curriculum for comparison across countries.<lb/> 283.<lb/> This report shows that the age at time of testing of TIMSS can be matched with the number of<lb/> years of schooling in PISA for most countries. This finding shows that both the TIMSS sample and the<lb/> PISA sample provide the same degree of comparability across countries, in that neither provides a more<lb/> stable frame of reference than the other, since both samples can be cross-checked in age and grade.<lb/> 284.<lb/> However, the number of years of schooling does have an impact on mathematics achievement.<lb/> One year of schooling could increase the average achievement by 20 to 40 score points. Therefore, in<lb/> comparing the differences between TIMSS and PISA results, the number of years of schooling in PISA<lb/> should be taken into account. A recommendation from this report is that both PISA and TIMSS should<lb/> endeavour to obtain a better measure of the number of years of schooling, as even a fraction of a year of<lb/> schooling will make some differences to the achievement scores. Since the rough measure of the number<lb/> of years of schooling constructed in this report already can provide some analytic power in explaining<lb/> achievement differences in TIMSS and PISA, any refinement of the rough estimates produced in this<lb/> report should be an improvement.<lb/></p>

			<head>Correlated factors<lb/></head>

			<p>285.<lb/> While we have identified a number of factors that have an impact on achievement scores, the<lb/> picture is not so clear as to the causal relationships between achievement levels and the identified factors.<lb/> It appears that many factors are correlated, sometimes for no clear and obvious reasons. For example, it<lb/> appears that students in Asian and Eastern European countries tend to start school later than Western<lb/> countries. These same countries also tend to have a curriculum that is oriented towards an emphasis on<lb/> formal mathematics. A check on the correlation between the variable TIMSS advantage index and TIMSS<lb/> age at time of testing shows a correlation significantly different from zero (correlation=0.487, p=0.035). It<lb/> is possible that cultural factors could have an impact on both the school systems (in terms of age of entry,<lb/> etc.) and the curriculum (in terms of formal mathematics versus mathematics for application, etc.), so<lb/> that groups of countries have similar profiles in a number of aspects of mathematics education.<lb/></p>

			<head>Gender and attitudinal differences<lb/></head>

			<p>286.<lb/> The comparisons of gender differences in PISA and TIMSS in this report (Chapter 5) highlight<lb/> at least two important points. First, gender differences are not uniform across mathematics content<lb/> domains. In particular, for spatial tasks and measurement tasks, boys outperform girls by the greatest<lb/> amount. This finding has implications for instructional measures to address gender differences. For<lb/> example, particular kinds of tasks may be designed to engage girls and boys in different ways. Second,<lb/> gender differences are not uniform across countries. Is there a hint of suggestion that boys outperform girls<lb/> in mathematics in more developed countries such as OECD member countries? If so, this would seem<lb/> contrary to expectations that more developed countries would generally have better addressed the equity<lb/> issue between girls and boys. Is there a stereo-typing promoted in developed countries? The contrast in<lb/> TIMSS and PISA results regarding gender differences prompts us to examine gender issues in further<lb/> depths.<lb/> 287.<lb/> Both PISA and TIMSS reported similar findings in students&apos; attitudes towards mathematics. The<lb/> most important one is that there is a larger gender gap in attitudes towards mathematics than in<lb/> mathematics achievement, with boys feeling more positive and confident towards mathematics than girls.<lb/> The combined results of TIMSS and PISA provide us with a more complete picture of students&apos; attitudes<lb/> towards mathematics. In particular, there is still much to learn by Western countries from Eastern<lb/> European countries about reducing the gender gap in students&apos; attitudes towards mathematics.<lb/></p>

			<head>And finally…<lb/></head>

			<p>288.<lb/> A number of important findings in this report could only be made when there are two<lb/> international surveys conducted at the same time. From this point of view, the two surveys contribute not<lb/> only to cross-national comparisons, but also to the validation of similarities and the identification of<lb/> differences in the results of the two surveys. The interpretations of the results from each survey are<lb/> enhanced by the cross validation by the other survey. It is comforting to all involved in the surveys to<lb/> know that the results between the two surveys can be quantitatively validated and crosschecked. It shows<lb/> that the surveys are conducted with rigour and sound theoretical underpinnings.<lb/> 289.<lb/> However, the fact that the results from the two surveys can be corroborated quantitatively<lb/> suggests that there is some duplication in the outcomes of the surveys, at least in the results in the<lb/> international reports. The findings in this report highlight the importance of making careful interpretations<lb/> of results for each country individually, since there are country specific factors that impact on student<lb/> performance. Consequently, international reports are just the first steps in presenting data collected in these<lb/> surveys. National reports should be viewed with greater importance for examining the data in more depth<lb/> in relation to cultural and policy factors specific to each country.<lb/> 290.<lb/> In making comparisons between PISA and TIMSS results at the international level, it should be<lb/> remembered that PISA results reflect relative performance of countries on a set of desirable mathematical<lb/> proficiencies for everyday life (as set through a consensus building process guided by the PISA<lb/> mathematics expert group). In contrast, TIMSS results reflect how well countries performed against<lb/> important areas of current mathematics curricula as agreed by participating countries. The fact that these<lb/> two approaches to building the mathematics frameworks do not produce the same assessment results is<lb/> food for thought for both curriculum developers and assessment practitioners.<lb/> 291.<lb/> Looking ahead, for future cycles of PISA and TIMSS, many issues raised in this report should be<lb/> considered in order to provide the best strategies in producing internationally useful and valid results with<lb/> maximum efficiency for the countries. This calls for collaboration between the two surveys so that the<lb/> results from the surveys can complement each other, rather than duplicate each other.<lb/> EDU/WKP(2010)5</p>


	</text>
</tei>
